% ===============================================
% MATH 373: Intro to Numerical Analysis           Fall 2021
% math_373_notes.tex
% August 4, 2021
% ===============================================
% Source material a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.


\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx}
\usepackage{xcolor}
\usepackage{courier}
\usepackage{hyperref}


%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%\renewcommand{\familydefault}{times}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setcounter{section}{0}
  \setcounter{subsection}{0}
  \setcounter{equation}{0}
  \setcounter{theorem}{0}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Introduction to Numerical Analysis
	\hfill Fall 2021} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Class Notes #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   {\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications.  They may not be distributed outside this class. Any use, or distribution, of these materials beyond the use for this class requires the formal permission of the instructor. The author claims copyright for all original work presented in these notes.}
   \vspace*{4mm}
}

    
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}
\def\ds{\displaystyle}



\newcommand\E{\mathbb{E}}

\begin{document}
\fontfamily{times}
%%-------------------------------------------------------------------
%%Start Here!

%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{1}{Introduction}{Kyle Riley}{Riley}

\section{Introduction}
This course is designed to utilize many sources that are free, or open source, which will make the materials more affordable, but it should be noted that free materials often do not have the benefit of the careful proofing and editing that is applied to  a commercially published product. These notes will strive to provide a detailed direction of the sources we are using for the class and reference specific resources. A tentative schedule will be kept on the schedule page (available on the D2L course page) and submission of collected work will be tracked on D2L.  \par

{\bf Note:} {\color{teal} It is expected that you read the listed resource {\bf  in advance} of the lecture and also compile notes from your reading, class notes, homework, and any other activities you engage in to learn the material. }
\medskip \par
The main reference for this course will be the {\it Tea Time Numerical Analysis} book by Leon Brin, which will be referenced in these notes by \cite{LB16}. This book is open source and free to download, it also listed as a custom print on Lulu.com. The book does not include everything needed for our course and so these notes will be used to provide supplementary materials to help provide additional background in content that we will need. The ultimate goal will be to develop a companion book that will provide the material needed. Another open resource that we will make use of in the class is the Numerical Methods with Applications book that  was developed by Kaw and Kalu, which we will reference in these notes as \cite{KK09}. This book is also open source and an abridge version is available for custom print from Lulu.com. 
\medskip \par 
This course will make heavy use of two software packages: Matlab and LaTeX. Matlab is commercial software and a copy can be placed on a Tablet PC from the school's Tablet PC program, using the school's license.You can get this installed at Tablet PC central  in the basement of the library, installation takes a very long time and a reliable connection to the campus server. If you do use the campus license then the license manager will require your computer to be connected to the server to issue you a session, which means your Tablet PC must be connected to the campus network while on campus, or you must VPN to the campus to obtain an active session. The campus has a license for only 50 or so sessions with several courses on campus using Matlab, thus it is possible you will be locked out of getting a session during peak usage times.  You can get a test copy from Mathworks that is good for 30 days, or you can purchase a student copy at \$49, or you can buy a more loaded version for \$99. Our class will not be using any of the special toolboxes or other tools and so the \$49 version should be sufficient. The book, \cite{LB16}, does mention and make use of Octave, this software is a clone of Matlab and is free for download. One option is for a student to develop programs in Octave, but please note {\bf all assignments gathered for grading will be processed in Matlab} and so any differences in syntax and use between Octave and Matlab would need to be resolved to work on Matlab (best advice for those that are not expert users in Matlab should stick with working in Matlab). \ \LaTeX  is a typesetting software popular in science and engineering. This software can make wonderful looking documents, but it is structured more as a way to program your documents rather than simply writing your document in Word. We will keep our use of \LaTeX \ simplified in this course, but this introduction should be sufficient for you to continue use in other courses if you wish to embrace the power of \LaTeX.  \medskip \par

\section{Goals of the Course}
The primary goal of the course is to provide an introduction on using numerical methods to solve mathematical problems. The course will cover the foundational concepts and use introductory topics to illustrate theory. The course will also include concepts from numerical analysis and the primary text, \cite{LB16}, does include proofs to a few theorems from  numerical analysis. If you are interested in learning more about this more formal approach to numerical analysis then classic resources would include: \cite{KA89}, \cite{BF11}, and \cite{KC02}. On the other hand, there are a wealth of resources that share a more applied approach to numerical methods with a few examples to include: \cite{AM96}, \cite{CC10}, \cite{MF04}, and \cite{JR13}. This course will task you with deriving information from a variety of resources and learning to develop solutions along with carefully evaluating the results. 


The course also includes the opportunity to develop programming skills in Matlab to solve mathematical problems and analyze the performance of numerical methods. Lastly, the course will provide opportunity to develop written communication skills along with strategies for documentation. 

\section{Getting started in Matlab}
It is expected that a student starting this course is already familiar with the basics of programming, which would include the different types of variables, control structures (if, else, while, and for loops), the use of subfunctions, handling input along with output, simple commenting of code, and the basics to debugging programs. It is likely a student beginning this course has been trained in a programming language other than MATLAB. 

Note: {\color{teal} if you have never used MATLAB before then it is {\bf strongly encouraged} that you complete the Matlab Onramp program \href{https://www.mathworks.com/learn/tutorials/matlab-onramp.html}{https://www.mathworks.com/learn/tutorials/matlab-onramp.html}. The onramp class is free and takes about two hours to complete. This free online class should provide you a solid understanding of Matlab to enable you to complete the requirements of MATH 373.}

In this section, we will highlight a few pieces of Matlab to get you oriented, but much of the syntax and management of Matlab will need to be learned by external sources. The university already has a license for Matlab and you can get the current version loaded onto your campus Tablet PC. Please stop by the Tablet Central in the basement of the library for assistance in getting Matlab loaded onto your machine. The initial installation takes a long time and a reliable connection to the network so please plan accordingly. If  you have any prior copies of Matlab on your computer then you should remove them prior to installing a new version. Once a copy is installed you should have access to Matlab via the campus network and you can access the software off campus, but you will need to have a VPN connection to the campus.  Please note that the university has only a few copies of Matlab that can run at the same time and so it is possible for you to be locked out if other users are logged in using Matlab at the same time. Students do have the alternative to purchase a student version of Matlab directly from the vendor, this copy should be sufficient for MATH 373 and the user can have access both on and off campus if they purchase the software directly from the vendor. Currently, a student version of MATLAB with out Simulink is 49 dollars and should easily handle the needs of MATH 373, the software can be purchased directly at \href{https://www.mathworks.com/}{https://www.mathworks.com/learn/tutorials/matlab-onramp.html}. The course program assignments and all other related programming assessments will involve Matlab and so a student should arrange for access to Matlab during the duration of the course.  The textbook, \cite{LB16}, uses sample code from Octave and Octave is designed to be very similar to Matlab.  Many of the exercises and code presented in \cite{LB16} should work in Matlab or could be easily adaptable to work in Matlab. For example, the experiment 1 on page 7 of the text will also work in Matlab: 

\begin{verbatim}
>> p0=pi;
>> p1=10*p0-31; p2=100*p1-41; p3=100*p2-59;
>> p4=100*p3-26; p5=100*p4-53; p6=100*p5-58;
>> p7=100*p6-97

p7 =

    0.9312
\end{verbatim}

Notice to get Matlab to display a result you {\it omit} ending the line with a semicolon, but if you want the calculations to run without producing output then ending each line with a semicolon will hide the calculations from display.  An abbreviated copy of example 3 from experiment 1 should provide further practice on putting calculations into Matlab code
\begin{verbatim}
>> 10*pi-31

ans =

    0.4159

>> p=100*ans-41;p1=100*p-59; p2 = 100*p1-26; p3=100*p2-53;100*p3-58

ans =

    0.9793

>> 100*ans-97

ans =

    0.9312
\end{verbatim}
However, there are many differences between Octave and Matlab with one simple example: the Euler's constant $e$, is a default variable defined in Octave, whereas $e$ is not a defined variable in Matlab, which means the way to reference $e$ in Matlab is through the defined exponential function, exp():
\begin{verbatim}
>> exp(1)

ans =

    2.7183

\end{verbatim}
Matlab also stores the commands you input into the command line, which means it is possible to go back and see commands you entered in previous sessions. The command: 
\begin{verbatim}
>> commandhistory
\end{verbatim}
will generate a popup window that will provide the commands in previous sessions. These sessions are saved on the local machine, which means separate machines will keep separate logs. Another thing that will be useful in this course is the ability to list calculations with additional significant digits and this can be accomplished with the using long format
\begin{verbatim}
>> format('long')
>> exp(1)

ans =

   2.718281828459046
\end{verbatim}
The long format is a change that lasts for the entire Matlab session and if you have a desire to go back to the short format then you will need to change the format back to short using the same command syntax, i.e., format('short'). You should practice all the homework problems that involve programming since this class will include collected work on programming in Matlab and Matlab code will also be included in the exams. One of the goals of this course is to build proficiency in Matlab. Mathworks, the company that produces Matlab, has a wealth of tutorials and other materials to help users learn how to use Matlab. 
The user community has a discussion page at \href{https://www.mathworks.com/matlabcentral/}{matlab central} a very simplified tutorial course is given at
\href{https://www.mathworks.com/learn/tutorials/matlab-onramp.html} {www.mathworks.com/learn/tutorials/matlab-onramp.htm} and a simple autograder constructed to help learning the absolute basic forms of Matlab can be found at \href{https://www.mathworks.com/matlabcentral/cody/}{Matlab Cody}.


\subsection{Script Files}

The first step to handling Matlab files, which are called m-files. The most basic m-file is a script file and this is basically a collection of Matlab commands stored in a file. To create a script file you can start a new file in the Matlab editor or you type the file name at the Matlab prompt. For example, if you want to create a script file that will calculate the average between two values then you can create the program named avg.m via
\begin{verbatim}
>> edit avg.m
\end{verbatim}

Inside the Matlab editor you can type to commands you would like to use to calculate the average
\begin{verbatim}
a = 4;
b = 12.6;
average = (a + b)/2
\end{verbatim}
To run the program, you can click the green play button in the editor, or you can type the name of the program in the Matlab prompt
\begin{verbatim}
>> avg.m
\end{verbatim}
The value of average will be displayed since the command lacks a semicolon (Matlab prints commands that do not have a semicolon). The important thing to remember with script files is that all variables remain in the workspace after the program executes. Script files are a good way to start a programming assignment since all operations stay on the workspace. 

\subsection{Function Files}

Function files are created similar to script files, but one main characteristic of a function file is the start of the program, which is given by the command {\color{blue}function}.  Function files have more structure and require management of input and output variables. All other variables within a function file are local and only exist during the execution of the function file, the local variables do not remain on the workspace once the function file is complete. A simple example of a function file would be a program the converts meters to feet.  A way to construction this function is an m-file that contains the following:
\begin{verbatim}
function feet = m_to_ft(meters)
conv = 3.280839895;
feet = conv * meters;
\end{verbatim}
The file name for this function should be ``m\_to\_ft.m''. The input variable for this function is the meters and the output is the number of feet. For example,
\begin{verbatim}
>> feet = m_to_ft(5)

feet =   16.4042
\end{verbatim}
Notice that the $conv$ variable does not appear in the workspace of Matlab when the program is finished running. For function files, it is important to determine what should be the output and what should be input. The order of the variables is important for a function file. Consider a function file with the first line:
\begin{verbatim}
function [altitude, velocity] = progv(position, rotation, acceleration)
\end{verbatim}
A function call of:  $>>$ [altitude, velocity] = progv(12, 5.6, 2.4), would imply position = 12, rotation = 5.6, and acceleration = 2.4. 

\subsection{A few handy things to know in Matlab at the start}

There are a wealth of things that can be valuable at the start of learning Matlab. A few hints include:
\begin{itemize}

\item Control c can generally stop a Matlab program that is stuck although that is not always the case. 
\item Taking the semicolon off a Matlab command in a function file is an easy way to see the value of that variable within the program. 
\item Matlab has several debugging tools and it is useful to learn how they work, which can be found with a simple Google search or with the use of the Mathworks website. 
\item You can comment a block of code using control r and uncomment a block of code using control t. 
\item Most command loops in Matlab require an end command. For example a while loop starts with the command ``while'' along with a condition and finishes with an ``end'' statement. The same is true for using the ``if'' command. 
\item The editor for Matlab does automatically indent code within loops (for, while, if), but the indentation is not required. The use of ``end'' is the way to signal to Matlab that you are at the end of a loop or an end statement. One suggestion includes adding a comment to an ``end'' to help someone reading the code to know the correspondence between the ``end `` and the relevant control structure. For exam, end `` \% end for if statement'' is a way to note the ``end'' goes with an if condition. 
\item Matlab doesn't usually require declaration at the start of the program, but is can be necessary when using arrays, vectors, or matrices. 
\item function files can have subfunctions defined within the function file and typically subfunctions are located {\it below} the main function. 
\item Some handy commands to look up: break, return, dbstop, 
\item A counter variable in Matlab requires the statement: k = k + 1. Matlab does not recognize the use of i++ or i**.
\item Matlab generally uses the traditional parenthesis, ``()'', in the traditional format for doing mathematical calculations. The use of \{ and \} is usually not recognized in Matlab and will not function in the same manner as what is done in C or C++. 
\item The boolean operators: \& = and, $\vert$ = or, == is used for boolean equal. In reality, Matlab does encourage the use of  \&  \& for AND along with $\vert \ \vert $ for OR, which in perfectly fine to use in the programs for Math 373. 
\item Matlab usually does not require loading of libraries in the same manner as what is needed in other programming languages, which means Matlab already has all the matrix operations accessible at the start along with all the plotting automatically loaded. The packages that can require special load instructions are the commands associated with some of the tool boxes. Math 373 will not be using any of the special tool boxes. 
\end{itemize}

\section{Getting started in \LaTeX }
\LaTeX \ is free software that is used to typeset mathematics and is a publishing standard in many areas of science and engineering. There are many layers to using \LaTeX \ and the goal of this course is to make you aware of the very basic fundamentals along with how to use this tool for technical writing. The course will utilize a specific format for the reports and documentation that will require the use of some default template files, but there are a variety of approaches for setting up an environment to process your tex files. There are two approaches that are recommended:
\subsection{Use MikTeX}
\begin{enumerate}
\item Download MikTeX from \href{https://miktex.org/download}{ miktex.org/download}
\begin{enumerate}
\item Be sure to pick the appropriate version for the computer in use (32 bit versus 64 bit)
\item Be sure to match up the correct operating system
\item You can download and setup MikTeX multiple times to any computer you have administrative control

\end{enumerate}
\item Install MikTeX
\begin{enumerate}
\item When the installation asks for default paper size, choose 'letter'
\item When the installation asks about packages, choose 'on the fly' (this allows MikTeX to install any packages you might need in the documents you are building)

\end{enumerate}
\item The installation should include TeXworks, which is the default editor. All demonstrations given in class will be using TeXworks, but you can use other editors if you already have other editors that you would like to use. Any tex files should be self-contained so any special commands should be defined internal to the document and the use of customized external style files will be discouraged. Please also note that all student work that will be processed for grading purposes will be implemented via TeXworks. 
\end{enumerate}

A few helpful tips in getting TeXworks setup is related to the defaults on the editor. In the main menu options on the top of the editor, please select the edit option, select preferences from that menu, and then select the editor tab. The editor tab will allow you to set many of the default settings such as: the type of font, font size, identifying line numbers on the editor (very helpful for debugging), automatic word wrap, and selecting the spell check language (otherwise language is not selected and documents are not spell checked). If you make any changes in the editor tab for preferences then you will need to exit TeXworks followed by reopening the document for the defaults to be active. 

\subsection{Using Overleaf}
Another option in processing your \LaTeX \  work  is making use of Overleaf, which is a free platform that can be used for a few projects. The advantage to Overleaf is that you will not have to worry about installation packages and you can work from any machine through the web browser. The detractors include the need for a strong connections to the internet since the processing is remote  and your files exist in a cloud account on a commercial server. If you have strong concerns regarding privacy and security then Overleaf would present risks since all your information is through their cloud based service. You can find more information at \href{https://www.overleaf.com/}{www.overleaf.com/} \medskip \par

The template tex files for homework and the projects will be distributed in class and the expectations is that you use these templates to complete your work. 

\subsection{A Few Basics in \LaTeX }
\LaTeX \ is a bit more like programming your paper rather than just typing it. The advantage with \LaTeX \ is that you have a great deal more control in typesetting in general and generating equations in particular. The challenge is learning all the default commands and notation that is used to accomplish all the typesetting options. \par
Basic tex files have two basic parts: preamble and the main body. The preamble starts with $\backslash$documentclass and the document class has several options with article being one type, thus our document starts with $\backslash$documentclass$\{$article$\}$. The preamble contains all the default settings for the document, for those starting out the best advice is to leave the preamble alone and edit only the main body. The main body starts with the command $\backslash$begin$\{$document$\}$ and it finishes with $\backslash$end$\{$document$\}$. A couple of other handy things to know is that comments in LaTeX are marked by the percent sign, $\%$ and for our notes the bibliography is handled internally, but there are more extensive tools used for handling bibliographies (do a search on BibTex since it is one of the more popular options). \par 
There are many free resources to help with learning \LaTeX, which a few include:
\begin{itemize}
\item https://www.ctan.org/starter
\item https://www.overleaf.com/learn/latex/Learn\_LaTeX\_in\_30\_minutes
\item https://tex.stackexchange.com/.
\end{itemize}

\subsubsection{Equations} The part that is best associated with LaTeX is the ability to typeset math equations. In a tex file, a math component is surrounded by dollar signs "\$". The other key feature of LaTeX is that commands are usually initiated by the backward slash "$\backslash$". For example, the command to insert an integral into the text is \$ $\backslash$int \$, which results in $\int$. The use of the dollar sign has two modes with the use of single dollar signs embed the equation inline with the text, which means \$ $\backslash$int x$\wedge$2 dx \$ would result in $\int x^2 \ dx$. On the other hand, the use of double dollar signs typesets an equation on a separate line, which means  \$\$ $\backslash$int $\backslash$sqrt$\{$x$\}$ dx \$\$ turns into:
$$\int \sqrt{x} \ dx.$$
Lastly, one can use the equation environment to insert an equation into a text, which an example: 
\begin{verbatim}
\begin{equation}
\sum_{k=1}^{\infty} \frac 1{k^3}    
\end{equation}
\end{verbatim}
produces the following
\begin{equation}
\sum_{k=1}^{\infty} \frac 1{k^3}.    
\end{equation}

\subsubsection{Spacing}
There a few handy tips on handling spacing within LaTeX. Within a mathematics equation the use of a single
$\backslash$ will produce a space in the equation. Thus, \$ $\int$ x$\wedge$2 $\backslash$ dx \$ will produce a space between $x^2$ and $dx$ in the integral $\int x^2 \ dx$. The use of a double backslash, $\backslash \backslash$ inserts a carriage return in the document. The command $\backslash$par will end a paragraph and the new line of the next paragraph will start indented. The command $\backslash$par $\backslash$noindent starts a new paragraph without an indent. A carriage return within the tex document acts much like an end of paragraph command, but additional returns do not produce additional spacing in the document produced by LaTeX. LaTeX also has a couple of default spaces that can be introduced between paragraphs: $\backslash$smallskip, $\backslash$medskip, and $\backslash$bigskip. It is possible to insert a specific vertical skip using the command of $\backslash$vspace$\{$1.3in$\}$. A similar spacing technique can be applied horizontally using the command $\backslash$hspace$\{$1.4in$\}$. 


\subsubsection{Citation and Bibliographies}
One joy of using LaTeX is with the strong ability it has for citations, which is not something we will spend much time on in Math 373. The citation method we will use is a simple author based citation that will be contained in these notes. The full BibTex feature will not be something we will use as part of class, but is certainly something a student can pursue outside of class. The references for these notes are given at the end of the document and are rather easy to interpret how to add sources as needed. If the author needs to refer to a source then the command $\backslash$cite$\{$KK09$\}$ will result in the citation \cite{KK09}. The learning curve for LaTeX has an initial steep slope, but once a user is familiar with the basics there is a great advantage to using LaTeX for technical papers. Welcome to the world of \LaTeX! 
%%end------------------------------------------------------

\section{A few things to define at the start} \label{s:error_section}
The textbook, \cite{LB16}, uses a few things without really defining them formally. Here we will list a few things that we plan to use in class and formally define here. In other cases, the concept is important enough that we define here at the start of the class and will continue to refer to the subject throughout the course. 

\definition The {\bf floor function} produces the greatest integer that is less than or equal to x. The notation for the floor function is given by $\lfloor x \rfloor$. It should be noted that Matlab has a predefined function, $>>$floor(x), that can perform the operation for the floor function. 

\definition The {\bf ceiling function} produces the smallest integer that is greater than or equal to x. The notation for the ceiling function is given by $\lceil x \rceil$. It should be noted that Matlab has a predefined function, $>>$ceil(x), that can perform the operation for the ceiling function. 

Error is constant concern for our class and we will consistently be discussing error along with the sources of error. Lecture 3 contains some discussion regarding sources of error, but we will present a few ways to measure error.  

\definition {\bf Error} (or absolute error) is defined by: $\vert \hat x - x_0 \vert$ where $\ds \hat x$ represents the exact value and $\ds x_0$ denotes the approximation. In class, we will often make reference in the form: $\ds \vert exact - approx \vert$. Notice that the order does not matter since the absolute value produces the same value regardless,  i.e. $\ds \vert \hat x - x_0 \vert = \vert x_0 - \hat x \vert $. Moreover, using the absolute error to measure error gives us one simple goal, to make error as small as possible.\label{d:error} {\color{teal} A note on error is for our class, error is always nonnegative. Reference to negative error is either using a different definition or is simply incorrect with the definition we will be working with and you should avoid ever listing error as negative.} 

An error calculation used frequently in science and engineering is {\bf percentage error}, which is defined by
$${\rm percentage \ error} = \frac {\vert exact - approx \vert}{\vert exact \vert} \times 100.$$
Our course will be more concerned with {\bf absolute relative error}, which is defined by
$${\rm absolute \ relative \ error} = \frac {\vert exact - approx\vert}{\vert exact \vert}.$$

{\bf Note}: {\color{teal} A few things to note
\begin{itemize}
\item Percentage error is not the same thing as absolute relative error.
\item Both percentage error and absolute relative error are not defined if the exact value is 0 and both can have difficulty in measurement when exact is close to zero.
\item The big advantage of using relative error is that it measures the error relative to the size of your exact value. Hence, it does not matter if the units involved are milometers, kilometers, or megameters, the absolute relative error provides a measure that is relative to the size of the exact. 
\item At the end of the course, we will move to multiple dimensions and vectors. The measurement of error has a natural extension in multiple dimensions : $\ds \Vert \vec {exact} - \vec {approx} \Vert$. To be clear the use of $\ds \Vert \vec x \Vert$ is the traditional vector norm (or vector magnitude) you have likely used in other courses with $\ds \Vert \vec x \Vert = \sqrt{x_1^2 + x_2^2 + x_3^2+...+x_n^2}$. 
\item Calculating error is easy when you know the exact value, but if you know the exact value then there is little reason to be using a numerical method. We will be concerned with tracking performance when we don't know the exact value. 
\end{itemize}}

We will refer to definition \ref{d:error} numerous times in this course, but a particular concern we will have is how fast does the error shrink as we changed the step size of a numerical method or as we increase the number of iterations. The order of convergence is a concept that has several interpretations and numerous variations in notation. Moreover, the perspective we use in Numerical Analysis will be different from what is used in Computer Science. In Computer Science, the concept of order is related to how {\it large} of a load it takes in computation to accomplish a calculation. If a calculation takes $\ds n^3 + 9n^2 - 45n+3$ operations for an array of size $n$ then the order conveyed for this calculation is as third order since for very large values of $n$ the value of $n^3$  will be much larger than the other terms in the expression (i.e. $\ds 9n^2, 45n, {\rm and} \ 3$). Thus, it is easy to identify a calculation that involves $\ds n^9-n^3+\sqrt{n}-5$ has an order of 9 and is of higher order than a third order calculation. The concept of order, for Computer Science, is a technique to measure performance for large values of $n$ and tracking the size of calculations involved. The order is a way to simplify the analysis and make it easy to compare performance as the computational load gets larger. \par

Numerical Analysis generally uses the concept of order to capture performance in the {\it convergence} of a numerical method as $h$ shrinks to zero. The main instrument that we will use often is {\bf big Oh}, which has a notation of $\cal O$. A formal definition of big Oh convergence comes from \cite{BF11}: let $\ds \beta_n$ be a sequence that converges to zero and $\ds a_n$ is a sequence that converges to $a$. The sequence $a_n$ converges to $a$ in the order of $\ds \beta_n$ if there exists $K>0$ and $N>0$ such that
\begin{equation}
\vert a_n-a \vert \le K \ \vert \beta_n\vert
\label{e:order_in_Burden}
\end{equation}
for all $n \ge N$. The notation for this result is that $a_n$ is ${\cal O}(\beta_n)$, which is also denoted by $\ds a_n \in {\cal O}(\beta_n)$. The implication is that $a_n$ converges faster to $a$ than $\beta_n$ converges to zero and so this provides a relative measure of convergence so we can compare the performance of methods. The big difference with this measure is the concern on converging to zero and so Numerical Analysis uses big Oh to measure convergence to {\it zero}, which is in contrast to the Computer Science focus on $n$ getting larger. An illustration that we will revisit later is how to approximate the first derivative of a function. We know the definition of the derivative from Calculus:
\begin{equation}
f'(x) = \lim_{h\rightarrow 0} \frac {f(x+h)-f(x)}h.
\label{e:der1}
\end{equation}
We know from Calculus, that as $h \rightarrow 0$ in (\ref{e:der1}) then the calculation of $\ds  \frac {f(x+h)-f(x)}h$ will approach the value of $\ds f'(x)$. We will find that there are other methods that produce more accurate results using the same values of $h$ and that these methods converge {\it faster} to the desired solution. 

The concept of order of convergence is first addressed in section 1.3 of \cite{LB16} and Brin gives the same definition on page 24 as we find in \cite{BF11}.  What is missing is the connection between the definition of big Oh convergence and the order of convergence introduced by Brin on page 19: The sequence $\ds p_n$ converges to $p$ with order $\alpha \ge 1$ if there exists a positive $\lambda$ such that
\begin{equation}
\lim_{n\rightarrow \infty} \frac {\vert p_{n+1}-p\vert}{\vert p_n-p\vert^{\alpha}}=\lambda .
\label{e:order_in_Brin}
\end{equation}
A sequence that converges such that (\ref{e:order_in_Brin}) is true has the relation:
$$  {\vert p_{n+1}-p\vert} \approx \lambda \ {\vert p_n-p\vert^{\alpha}}, $$
or more commonly the expression is used to express the telescoping bound:
$$  {\vert p_{n+1}-p\vert} \le \lambda \ {\vert p_n-p\vert^{\alpha}}.$$
The order of convergence of this sequence is said to be of order $\alpha$. Higher order methods are still considered faster in convergence since some small positive number raised to a higher power will converge must faster to zero. For example, the number $\ds \frac 16$ is much larger than $\ds \biggr (\frac 16\biggr )^3=\frac 1{216}$, again the focus here is the convergence to zero. We will revisit convergence several times over the semester, but this section has the desire to emphasize the formal definition of big Oh and also to connect this definition to the discussion presented in section 1.3 of \cite{LB16}.  
\medskip \par

\section{Numerically Estimating Convergence} In section 3 of \cite{LB16} the author has a focus in the homework of calculating the order of convergence analytically. We will be able to calculate order of convergence analytically in a few cases, but it can be useful to know how to estimate order of convergence numerically. The methods of estimating rate of convergence numerically are rooted in the definition given by (\ref{e:order_in_Brin}), which can be approximated by the relation:
$$  {\vert p_{n+1}-p\vert} = M \ {\vert p_n-p\vert^{\alpha}}, $$
where M is a constant multiplier to provide for equality. The operation of taking the logarithm of both sides reveals:
$$  \log ({\vert p_{n+1}-p\vert}) = \log (M {\vert p_n-p\vert^{\alpha}}), $$
with the logarithm rules producing
$$  \log ({\vert p_{n+1}-p\vert}) = \log (M) + \alpha \log( {\vert p_n-p\vert}). $$
If one considers the given form as a discrete version of plotting $\ds y = \alpha x + b$ then the estimate of the slope can be given by:
$$\alpha \approx \frac {\log (\vert p_{n+1}-p \vert) - \log( \vert p_n - p \vert)}{\log ( \vert p_n - p \vert)-\log ( \vert p_{n-1}-p\vert )}.$$
For a convergent method, the approximation of $\alpha$ should improve with larger values of $n$. However, this estimate does require knowledge of the exact value, $p$, which is not always available. It follows that the simpler estimate of:
\begin{equation}
\alpha \approx \frac {\log \vert \frac {p_{n+1} - p_n} {p_n - p_{n-1} } \vert}{\log \vert \frac {p_n - p_{n-1}}{p_{n-1} - p_{n-2}}\vert}
\label{e:numerical_est_convergence}
\end{equation}
can be used to numerically estimate the order of convergence for a method. The estimate in (\ref{e:numerical_est_convergence}) should be used with caution since the convergence of this estimate is highly variable. In some case the convergence might be swift, but in other cases the convergence might be slow. In addition, round off error can easily skew the results and this is particularly true for methods that converge quickly since the fast convergence might not be easily measured when the estimates quickly round to the same values within machine arithmetic. We will revisit convergence again in lecture 3.

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{2}{Motivating the Need for Numerical Methods}{Kyle Riley}{Riley}

\section{Solving Problems}
As stated earlier, this class will provide you opportunity to develop proficiency in using Matlab and also work to develop your ability to communicate technical material. However, there is the broader view of the advantages of taking a numerical analysis class in the first place since teaching Matlab and working on technical communications could be accomplished in a variety of classes. Sadly, there is dirty secret that has been kept from you for the vast majority of your MATH classes you have taken to date. A student that has completed Calculus 1 $\rightarrow$ Calculus 3 along with completing Differential Equations might believe that all math problems can be solved completely. For example, any Calculus student should be able to solve 
\begin{equation}
\int_0^1 x \ \sqrt{1+x^2} \ dx.
\label{e:analytical_integral}
\end{equation}
It is worth note to review how to solve such a problem since such problems will likely arise in homework and/or exams. The technique needed is u-substitution, which engenders $\ds u = 1+x^2$, and also $\ds du = 2x \ dx$. Hence, u-substitution on (\ref{e:analytical_integral}) reveals
$$\int_0^1 x \ \sqrt{1+x^2} \ dx = \int_1^2 \frac {\sqrt{u}}2 \ du = \frac 12\biggr ( \frac 23 \biggr ) u^{3/2} \biggr \vert_1^2 = \frac {2\sqrt{2}-1}3 .$$
U-substitution is an example of an {\bf analytical method} and analytical methods produce a formula that is the {\it exact} answer. Looking back on Calculus 1 $\rightarrow$ Calculus 3 and Differential Equations a student will realize the vast majority of all the techniques covered are analytical methods that result in formulas that produce the exact answer. In Math 373 we will quickly pivot to different problems, for example,
\begin{equation}
\int_0^1 \sqrt{1+x^3} \  dx .
\label{e:num_int_example}
\end{equation}
The integral in (\ref{e:num_int_example}) cannot be solved with a simple closed formula and the techniques of u-substitution, trigonometric substitution, and any other techniques covered in previous classes fail to produce a simple formula. There should be an answer to this problem since the graph of $\ds y = \sqrt{1+x^3}$ has a finite area above the x-axis over the region from $x=0$ to $x=1$. All is not lost, in Math 373 we will learn about {\bf numerical methods} that will help us solve problems like the one posed in (\ref{e:num_int_example}). Numerical methods will allow us to solve a much broader range of problems and also enable us to tackle problems that involve data and do not include formal equations. For example, the trapezoidal rule can be used and reveal
$$\int_0^1 \sqrt{1+x^3} \  dx  \approx 1.117, $$
but this ability to solve more problems does come from a trade off that we can no longer find exact answers and must settle for {\it approximations}. Thus, for numerical methods the question of accuracy will be of constant concern and it is generally the case the higher degree of accuracy desired in an answer will require more computational work. 

A large component of the course will focus on solving differential equations and so another example to consider is solving
\begin{equation}
\frac {dy}{dt} = 4y; \ {\rm with} \   y(0)=-3
\label{e:diff_example}
\end{equation}
An analytical method that can be used in this case is separation of variables, where
$$ \frac {dy}y = 4 \  dy  \rightarrow \int \frac 1y \ dy = \int 4 \  dt \rightarrow \ln \vert y \vert  = 4t+c \rightarrow y = Ae^{4t}.$$
Using the initial value results in an exact solution to (\ref{e:diff_example}) results in: $\ds y(t)=-3e^{4t}$. Many of the methods covered in Differential Equations involve linear differential equations or a system of linear differential equations. A simple example of a problem that is not easily solved with analytical methods:
\begin{equation}
\frac {dy}{dt} = y^2+t; \ {\rm with} \ \ y(0)=-1,
\label{e:diff_num_example}
\end{equation}
which being a nonlinear equation eliminates most of the methods covered in MATH 321. In MATH 373, we will learn about the Runge Kutta Method and be able to compute approximations of the solution. For example, if we are interested in y at t=1 then with Runge-Kutta we can produce an approximations of $y(1)\approx -0.121189$. 

{\bf Note:} {\color{teal} this class will be concerned with: the accuracy of approximations, if a solution is consistent with the given mathematical problem, and also the work to evaluate the final results. }

\section{Continuous versus Discrete}
Another perspective that motivates the use of numerical methods is the nature of the information we have to work with and the tools most appropriate to address the problem with the given function. For example, if you happen to know the distance a car travels from a fixed point is given as a function of time as: $\ds f(t)=45t$ then computing the velocity is a simple result of taking the derivative. In this case, $f(t)$ gives us continuous information about the position of the car and so it is possible to analytically calculate the derivative. On the other hand, if we only have position for distinct time values: \par

\begin{tabular}{|c|r|}
\hline
{\rm time} & {\rm distance} \\
\hline
0 & 0 \\
\hline
0.2 & 8.8 \\
\hline
0.3 & 28.36 \\
\hline
0.4 & 37.93 \\
\hline
0.5 & 46.09 \\
\hline
\end{tabular}\par
then an analytical formula for velocity is not possible. Data for distinct times is known as a {\bf discrete} problem and to estimate velocity in this case we must return to the definition of the derivative $\ds f'(x)=\lim_{h\rightarrow 0}\frac {f(x+h)-f(x)}h$, which turns out estimating the slope of the tangent line using the given data. One more illustration involves calculating the work done to move a satellite from 400 miles to an 800 mile orbit. The calculation can be accomplished using the inverse square law
$${\rm work}=\int_{400}^{800} \frac C{x^2} \ dx, $$ 
where x is the height of the orbit and C is the mass. However, to make this move the satellite would need to burn fuel and this means it would lose mass, which implies that C is not constant and actually changes with x, i.e. $C(x)$. We can use data to determine the loss of the mass of fuel as the satellite climbs in orbit and so the integral becomes 
$${\rm work}=\int_{400}^{800} \frac {C(x)}{x^2} \ dx, $$ 
but the values of $C(x)$ are discrete. Again, this can be addressed via numerical methods. 

The conclusion of this section should reveal that numerical methods allow for solving more problems than the traditional analytical methods. In addition, the world of data forces problems to be discrete problems and this requires numerical tools. 

%---------------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{3}{Taylor's Polynomial, Error, and  Convergence}{Kyle Riley}{Riley}

This is a large lecture, but that is because it involves several concepts that are all interrelated. Many of the topics covered in this lecture will come up repeatedly in the course and it is important to master these fundamental concepts in order to be successful with the material that comes later. 


\section{Taylor's Polynomial}
The Brin book, \cite{LB16} section 1.2, does an excellent job with the Taylor Polynomial Theorem. It is strongly recommended that students carefully review section1.2. In this section we will a dd a few highlights and examples. 

{\bf Taylor's Polynomial Theorem}
\theorem{ Suppose that f(x) has n+1 derivatives on (a,b) and $\ds x_0$ in $(a,b)$. Then for each $x$ in (a,b) there exists an $\ds \xi$ that lies strictly between $x$ and $\ds x_0$  such that
$$f(x) = f(x_0) + \sum_{j=1}^n \left ( \frac {f^{(j)}(x_0)}{j!}(x - x_0)^j \right ) + \frac {f^{(n+1)}(\xi)}{(n+1)!} (x - x_0)^{n+1}.$$}
This is a theorem and a concept that is covered in Calculus 2, but in numerical analysis this becomes the bedrock for much of the work we will explore in the course. It is expected that any student in numerical analysis will have this theorem memorized and can fully explain how this theorem is used in approximating functions. 

{\color{teal}
Note: \begin{itemize}
\item{} The function must have at least n+1 derivatives over the interval (a,b), functions that lack this condition will not satisfy this theorem.
\item{} The notation of $\ds f^{(k)}(x)$ is for the $\ds {\rm k}^{{\rm th}}$ derivative of f evaluated at x. 
\item{} The term: $\ds  \frac {f^{(n+1)}(\xi)}{(n+1)!} (x - x_0)^{n+1}$ is often called the remainder term (or the error term). 
\item{} The equation in this theorem states equality between $f(x)$ (the exact value of the function at x) compared to the Taylor's Polynomial $\ds  f(x_0) + \sum_{j=1}^n \left ( \frac {f^{(j)}(x_0)}{j!}(x - x_0)^j \right )$ plus the remainder term. Thus, the remainder term does represent the missing value between the exact value of $f(x)$ and the Taylor's Polynomial. 
\item{} In practice, we pick $\ds x_0$ as a value as close to $x$ as we can get where we can easily calculate $\ds f(x_0)$ and all the necessary derivatives at $\ds x_0$. 
\end{itemize}}

A major point to discuss is the driver hidden in this theorem, which is the value of $\ds \xi$. The theorem guarantees that there exists an $\ds \xi$ that lies between $x$ and $\ds x_0$ that delivers the remainder value we need between $f(x)$ and the Taylor's Polynomial, but determining the value of $\ds \xi $ is the real challenge. If you can find the value of $\ds \xi$ then it is possible to determine the exact value of $f(x)$. We will find that in many cases, we can come up with estimates and bounds for the remainder term, but will not be able to find the exact value. Alas, our goal will not be to find $\ds \xi$ and instead we will seek ways to systematically shrink error. 

\subsection{Example} Let $\ds f(x) = \sqrt{x}$. Find the second order Taylor's Polynomial and use it to estimate $f(4.1)$ using the center of $\ds x_0=4$. 

The second order Taylor's Polynomial has the form:
$$T_2(x) = f(x_0) + f^{\prime}(x_0)(x-x_0) + \frac {f^{\prime  \prime}(x_0)}2(x-x_0)^2,$$
which simplifies to
$$T_2(x) = 2 + \frac 14(x-4) - \frac 1{64} (x-4)^2, \ {\rm and\ it \ follows \ that} \ T_2(4.1) \approx 2.0248 .$$

\subsection{Example} - Consider the case of finding the third order Taylor's Polynomial for $\ds f(x) = \sqrt{x}$ with $\ds x_0 = 1$ with the goal of approximating $\ds f(1.3)$.

The first step should involve calculating the derivatives, which in this examples should be: $\ds f^{\prime}(x) = \frac 1{2\sqrt{x}}$, $\ds f^{\prime \prime}(x) = \frac {-1}{4x^{3/2}}$, and $\ds f^{\prime \prime \prime}(x) = \frac {3}{8x^{5/2}}$. The calculation of the third order Taylor's Polynomial with $\ds x_0=1$ results in:
$$T_3(x) = 1 + \frac 12 (x-1) - \frac {\frac 14}2 (x-1)^2 + \frac{\frac 38}{3!}(x-1)^3, \  {\rm with} \ T_3(1.3) \approx 1.1404375.$$
Using the remainder term to {\bf develop an error bound} is something that can be illustrated with this example. The remainder term for the third order Taylor's Polynomial is:
$$\frac {f^{(4)}(\xi ) }{4!}(x-x_0)^4.$$
Our example has $\ds f(x) = \sqrt{x}$, which results in $\ds f^{(4)}(x)=\frac {-15}{16x^{7/2}}$, and we know that $\xi$ exists between $x$ and $\ds x_0$ (where for our example, $x=1.3$ and $\ds x_0=1$). Thus, the error has the bound
$$\vert {\rm Error} \vert =\biggr \vert \frac {-15}{16\xi^{7/2}}\frac {(x-x_0)^4}{4!} \biggr \vert, $$
which it is given that $x = 1.3$, $\ds x_0=1$, and the error bound is maximized if $\ds \xi = 1$. Therefore, the error bound estimate comes from
$$\vert {\rm Error} \vert =\biggr \vert \frac {-15}{16\xi^{7/2}}\frac {(x-x_0)^4}{4!} \biggr \vert = \frac {15/16}{4!}(0.3)^4 \approx 3.1641 \cdot 10^{-4} $$

The remainder term, $\ds \biggr \vert \frac {f^{(n+1)}(\xi)}{(n+1)!} (x - x_0)^{n+1} \biggr \vert$ is made smaller when n is increased or if $x$ is picked closer to $\ds x_0$.

\subsection{example} Problem 4 from section 1.2 in \cite{LB16}, given that $f(2)=3$, $\ds f^{\prime}(2)=-1$, $\ds f^{\prime \prime}(2)=2$, and $\ds f^{\prime \prime \prime}(2)=-1$. 

a) Write down the third order Taylor's Polynomial with center $\ds x_0=2$ and the values for f and the derivatives are given result in $\ds T_3(x) = 3-(x-2)+(x-2)^2-\frac 16 (x-2)^3$, which can be simplified to  $\ds T_3(x) = \frac {31}3 - 7x +2x^2 - \frac 16 x^3$.

b) The calculation of $\ds T_3(4) = \frac {11}3$.

c) Calculate an estimate of an error bound given that $\ds -3 \le f^4(\xi) \le 5$.  Given that $\ds R_3(x) = \frac {f^{(4)}(\xi)(x-x_0)^4}{4!}$ with  $\ds -3 \le f^4(\xi) \le 5$ then
$$\vert R_3(x) \vert = \frac {5\cdot 2^4}{4!} = \frac {10}3.$$

\section{Machine Arithmetic} The textbook, \cite{LB16}, does mention accuracy and sources of error in section 1.1. The book does not really discuss the details regarding machine arithmetic so this section of notes is to help supplement the information needed on machine arithmetic. 

\subsection{IEEE format}
There are a few things to note about the discussion in \cite{LB16} with one being the supplement to Crumpet 1 in using machine arithmetic. There are a few details regarding the IEEE format that deserve mention. As you may know, machines work in binary code and so the format a computer uses to represent a number must be in a binary string, which an example of a 16 bit string is given by:

%make table to represent  a bit string
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1& 0& 1 &1 & 0 & 0\\
\hline

\end{tabular}. \par 

Each cell in the string is a single bit, which is represented by a 0 or 1  and a 16 bit string is just a string of 16 cells collected together. One of the simplest IEEE formats involves a 32 bit string, which requires 32 binary cells collected together. The 32 bit string is formatted into three distinct parts: the sign is used to determine if the machine number is positive or negative, the exponent is a way to build the size of the number, and the mantissa is the component that influences the amount of significant digits that can be held in the machine number. The order of these components is given by:
%make table to represent  a bit string
\begin{tabular}{|c|c|c|}
\hline
sign & exponent & mantissa\\
\hline
\end{tabular}, where the sign is a single bit, the exponent is 8 bits, and the mantissa is 23 bits. In these notes, we will refer to the different components in the following manner: $ s$=sign, $t$=exponent, and $m$ = mantissa. The method for generating the decimal number from the binary parts is given by:

\begin{equation} 
{\rm decimal \ number}=(-1)^s(1+m)2^{t-127}.
\label{e:decimal_number}
\end{equation}
The easiest component of this calculation is the sign component where $s$ is just  the value of the bit and this translates into $s=0$ generating a positive number and $s=1$ generating a negative number. The other components have specific formats that are used to generate the values for $t$ and $m$, which will describe how to perform these other calculations in the following sections. 

\subsubsection{The Exponent}
The exponent is generated from an 8 bit string, for example: 
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
1 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
\hline
\end{tabular}, we can identify this as an array of 8 numbers and we can put them in sequence right to left starting with an index of zero (this indexing plays a role in how we build t). Thus, our array takes the form of:\\
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
$a_7$ & $a_6$  & $a_5$  &  $a_4$  &  $a_3$  &  $a_2$  & $a_1$  &  $a_0$ \\
\hline
\end{tabular}.\\
The way t is built from the array is via the formula: $\ds \sum_{k=0}^7 a_k2^k$. If we return to our example: \\
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
1 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
\hline
\end{tabular}\\ \\
then it follows that $\ds t =  2^7 + 2^1 = 130$. This calculation should reveal why the formula in (\ref{e:decimal_number}) has the expression of $t-127$ in the power. The subtraction of 127 splits the relative size of the exponent since a bit string composed of all ones would generate the value of 255 and a bit string of all zeros would result in zero. Thus, the range of the exponent in (\ref{e:decimal_number}) is from 128 to -127. 

\subsubsection{The Mantissa}
The Mantissa is comprised of a 23 bit string, for example,

%make table to represent  a bit string
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 &  0 & 0& 0  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
\end{tabular}.\\ 

The format for building m is similar to what was done to compute the exponent value, but the ordering and indexing is different. In this case, the mantissa can be thought of as an index array with the index starting at 1 and increasing left to right, that is,
%make table to represent  a bit string
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
$b_1$  & $b_2$  & $b_3$  &  $b_4$  & ... & $b_{21}$ & $b_{22}$  &  $b_{23}$ \\
\hline
\end{tabular}. The formula for calculating $m$ is given by, $\ds m=\sum_{k=1}^{23}b_k2^{-k}$. If we return our example: 

%make table to represent  a bit string
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 &  0 & 0& 0  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
\end{tabular},

then $m=2^{-2} + 2^{-3} + 2^{-5} = 0.40625$. Hopefully, this example reveals that the mantissa contains the ability to store the significant digits of a calculation and so computational accuracy is largely driven by how much memory can be allocated towards the mantissa. The IEEE single precision format uses a 32 bit string while the {\it double precision} format uses a bit string of 64 bits. The double precision is meant to improve accuracy and the allocation for the different components are: 1 bit for sign, 11 bits for exponent, and 52 bits for the mantissa. The increased accuracy that comes from double precision does come at a price with more storage and also more computation to carry out each step. 
\subsubsection{Example of taking a binary string and converting it to a decimal}
We already have one example worked out from the previous work given a bit string where the first cell is 0 the next 8 cells are

\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
1 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
\hline
\end{tabular}

and the next 23 bits are

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 &  0 & 0& 0  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
\end{tabular}.

It has been found that $s=0$, $t=130$, and $m=0.40625$, which means that our number is given by: $\ds (-1)^s(1+m)2^{t-127}= 11.25$. Consider another example where s = 1 the 8 bits for the exponent are given by

\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
0 & 1 & 1 & 1 & 1 & 0 & 1 & 1\\
\hline
\end{tabular}

and the 23 bits for the mantissa are 

 \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 &  0 & 0& 0  & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
\end{tabular}.

It follows that $\ds t=2^6 + 2^5 +2^4 + 2^3 + 2+ 1 = 123$, $\ds m= 2^{-1} + 2^{-3} + 2^{-4}+2^{-5} = 0.71875$, which produces $\ds (-1)^s(1+m)2^{t-127}= -0.107421875$.

\subsection{Summary on Machine Arithmetic}
The discussion on machine arithmetic has a few major concepts that should be fully understood and the primary concept to acknowledge is that a machine can only represent a finite number of numbers. This concept deserves some emphasis if you consider that on the real number line between 0 and 1 there are an infinite number of rational numbers between any two distinct numbers on the number line. Thus, computing almost any calculation exactly is an effort that can be doomed from the start since the number that might be the exact value could be a number that is not machine representable. In numerical computations the goal is to find good approximations and it turns out we can develop accurate methods that are used for a variety of applications. The book, \cite{LB16}, has a nice discussion on looking at $\frac 17$ and how a computer is doomed at representing this number exactly. There are also a few terms that will be useful in working with  Matlab.

\definition {\bf Overflow} is a result when the number to be calculated is larger than any number the machine can represent. In the IEEE format, the largest number that can be stored is a bit string that starts with a zero and is followed by a 1 in all other cells. If a calculation generates a number larger than what the computer can store then some software will output a warning regarding overflow.  \par

\definition {\bf Underflow} is a result when the number to be calculated is positive, but smaller than any positive number the machine can represent.  If a calculation requires precision beyond what a computer can represent then a warning of underflow can be produced. \bigskip \par

A related item to underflow is {\bf Machine Epsilon}. The formal definition of machine epsilon will not be useful for our class, but a simplified reference for machine epsilon that is often used is the {\it smallest} positive machine representable number such that $1+\epsilon$ is greater than 1. 

\section{Sources of Error}
The Brin book, \cite{LB16}, defines two types of error: algorithmic error and floating point error. {\bf Algorithmic error} is the error that can be attributed to the actual algorithm, or numerical method, being used to approximate the solution to the mathematical problem. An example that might illustrate algorithmic error is the use of linearization to solve a nonlinear problem. For example, for small x values one can use $y=x$ to approximate $y = \sin (x)$, but these are not exactly equal so for any $x \neq 0$ the use of $y=x$ to approximate $y = \sin (x)$ would entail an algorithmic error. {\bf Floating point} error is the source of error from using machine arithmetic. The Brin book, in crumpet 1 of section 1.1, there is an excellent example regarding the binary representation of 1/7. It is not possible for a machine to exactly represent 1/7 in binary form since that would require an infinite expansion. Thus, errors as a result of the finite restrictions of machine arithmetic are labeled floating point error, which also has a label of round-off error. Another type of error that the book discusses is {\bf truncation error}, which is the result of using a truncated expression of an infinite series to approximate a solution. We will often use a Taylor series to generate solutions to different problems in the text, but it is not possible to completely calculate an infinite sum with a finite loop and so we often use a truncated series to approximate the infinite series.  \par
There are other sources of error the book does not formally discuss and a primary source that occurs in the real-world application is measurement error. In reality, the data you work with will have measurement error from the devices used to make a measurement, or the challenges to accurately record the measurements will introduce error. The bottom line is that the data generated in the modern world regularly involves measurement error. Another area to discuss involves the operations that go with machine arithmetic that can result in a loss of accuracy.  The reference, \cite{CC10}, lists a few handy rules of thumb of operations that can tend to decrease accuracy

\begin{itemize}
\item  Adding a large number to a small number. Suppose $a=0.0003216$ and $b=12.14$ then it is difficult to represent $a+b$ since the precision of accuracy for $b$ is not the same as is for $a$. If one number is sufficiently larger than the other number then the sum or difference will likely result in retaining just the larger number. The best way to address this issue is for a long series of sums is to group the different elements by size so that numbers that are closer to the same size are done first so that the calculations are not dominated by the larger numbers. Comparing a penny to a ten dollar bill makes it easy to ignore the penny, but 500 pennies compare much more favorably to ten dollars.

\item  Subtracting two numbers that are very close to the same size. The book, \cite{LB16} section 1.1, does discuss this issue. Subtracting two numbers that are very close to the same value results in losing all the significant digits that might be present in either number. Thus, a loss of precision results when subtracting two values that are very close to the same value. The remedy is to reorganize so that a calculation avoids this situation. This feature can also be known as {\it subtractive cancellation}. 

\item Magnifying error by multiplying by a large number. If there is any measurement error to $a$ then a calculation that involves $900*a$ would magnify the measurement error. The same is true if a calculation involves dividing by a very small number. The best advice to this problem is to reorganize a calculation to put off this type of multiplication until the very end of the calculation. 

\item Error accumulation, can also be known as error propagation. Any error present at the start of a calculation, or takes place early in a calculation, will pass along error to the later steps of the algorithm. This means error from step 1 will likely force more error in the calculations for step 2. The best advice is to reorganize calculations so the largest errors are introduced as late as possible to the algorithm.  
\end{itemize}
Our course will spend some attention to the accuracy and stability of numerical algorithms, but our discussion will be  limited. A book by Nicholas Higham, \cite{NH02}, is a wonderful authority on the subject and can provide more detail for the interested reader.

\section{Convergence} 
We introduced a few basic principles of error in section 1.5 of the lecture notes and now it is time to revisit the topic and discuss the order of convergence in more detail. When discussing numerical methods there are several terms that can be applied. 
\definition{A numerical method is {\bf consistent} when the method converges to a solution that is consistent with the associated mathematical problem.} For example, if a numerical method is to find the solution to $\ds f(x) = 0$ and the method is an iterative method given by $\ds p_n$ then a {\it consistent} method has the property that $\ds p_n\rightarrow p^*$ as $n \rightarrow \infty$ and $\ds f(p^*)=0$. 

\definition {A numerical method is {\bf convergent} if the iterations from the method converge to a unique point.} In general this means for iterations $p_n$ there exists a unique $p^*$ such that $\ds \lim_{n \rightarrow \infty}p_n =p^*$. {\color{teal} Note that a convergent method is not automatically consistent and so it is best practice to have checks in mind to make sure that your numerical method is converging to a solution to the associated mathematical problem.}

Another concept we will discuss is the stability of a numerical method, but we will find that there are different forms of stability and so the designation of a numerical method as being ``stable'' will depend on the type of stability we seeking to describe. Another couple of terms that can be confusing is accuracy and precision. 
\definition{ The {\bf accuracy} of a method is how close the result from the numerical method comes to the solution to the associated mathematical problem.} Thus, an accurate method will have a smaller error than a less accurate method. 
\definition{ A method is {\bf precise} when the subsequent iterations are close to each other and you get a tight cluster of approximations.} 

The goal we have is to utilize methods that are convergent {\it and} consistent. The same is true for us to use a method that is accurate {\it and} precise. The bottom line is that we will often reflect on the solutions that we generate are consistent with the posed mathematical problem and try to determine how accurate the results might be. 

Consider the sequence $\ds s_n = (1+\frac 1n)^n$ as $n$ increases without bound. We can write a simple code in Matlab to look at the first 20 terms of this sequence and when we get the following
\begin{verbatim}
   2.0000
   2.2500
   2.3704
   2.4414
   2.4883
   2.5216
   2.5465
   2.5658
   2.5812
   2.5937
   2.6042
   2.6130
   2.6206
   2.6272
   2.6329
   2.6379
   2.6424
   2.6464
   2.6500
   2.6533
    \end{verbatim}
Notice that this sequence does show convergence since $\ds \vert s_n-s_{n-1}\vert$ is getting smaller with each iteration and in theory a convergent method would have the property: $\ds \lim_{n \rightarrow \infty}  \vert s_n-s_{n-1}\vert = 0$. Moreover, notice that as the subsequent iterations get closer together the  digits start agreeing in more and more decimal points. The alert calculus student will recall that $\ds \lim_{n \rightarrow \infty} (1+\frac 1n)^n = e$. Thus, $\ds s_n$ has two things that we like to see in a numerical method: it is convergent and the convergence is consistent with natural base value of $e$. 

The $\ds s_n$ sequence also illuminates another characteristic we seek, which is the rate of convergence. The $e$ to the first few digits is given by: $\ds e \approx 2.7183$. The first 20 terms of the sequence of $\ds s_n$ produces an approximation of $\ds s_{20} \approx 2.6533.$ Further calculations will reveal that $\ds s_{30} \approx 2.6743$ and so even after a 30 iterations the absolute relative error ($\ds \frac {\vert exact - approx \vert}{\vert exact \vert} \approx 0.0162$). A vital concern in numerical analysis is the {\bf order of convergence}, which is something we already introduced in section 1.5 of lecture 1 and that \cite{LB16} covers in section 1.3. The order of convergence is a way of quantifying the {\it rate} of convergence and it is important to fully understand the implications of this concept as it relates to numerical methods. 

\subsection{Numerically Estimating Order of Convergence}
Recall the discussion from lecture 1 regarding estimating convergence numerically. The methods of estimating rate of convergence numerically are rooted in the definition given by (\ref{e:order_in_Brin}), which can be approximated by the relation:
$$  {\vert p_{n+1}-p\vert} = M \ {\vert p_n-p\vert^{\alpha}}, $$
where M is a constant multiplier to provide for equality. The operation of taking the logarithm of both sides reveals:
$$  \log ({\vert p_{n+1}-p\vert}) = \log (M {\vert p_n-p\vert^{\alpha}}), $$
with the logarithm rules producing
$$  \log ({\vert p_{n+1}-p\vert}) = \log (M) + \alpha \log( {\vert p_n-p\vert}). $$
If one considers the given form as a discrete version of plotting $\ds y = \alpha x + b$ then the estimate of the slope can be given by:
$$\alpha \approx \frac {\log (\vert p_{n+1}-p \vert) - \log( \vert p_n - p \vert)}{\log ( \vert p_n - p \vert)-\log ( \vert p_{n-1}-p\vert )}.$$
The program numorder.m that is supplied in the course materials does use this method to numerically estimate the order of convergence based on output from each term in a sequence. However, this estimate does require knowledge of the exact value, $p$, which is not always available. It follows that the simpler estimate of:
\begin{equation}
\alpha \approx \frac {\log \vert \frac {p_{n+1} - p_n} {p_n - p_{n-1} } \vert}{\log \vert \frac {p_n - p_{n-1}}{p_{n-1} - p_{n-2}}\vert}
\label{e:numerical_est_convergence_2}
\end{equation}
can be used to numerically estimate the order of convergence for a method. The estimate in (\ref{e:numerical_est_convergence_2}) should be used with caution since the convergence of this estimate is highly variable. In some case the convergence might be swift, but in other cases the convergence might be slow. In addition, round off error can easily skew the results and this is particularly true for methods that converge quickly since the fast convergence might not be easily measured when the estimates quickly round to the same values within machine arithmetic. 

\subsection{The Implications of the Order of Convergence}
The order of convergence is a way to quantify the rate of convergence. For many of our numerical methods we will be interested in the step size, which represents the mesh size in x, i.e.,  $\ds \vert x_k - x_{k-1} \vert$. The formal notation we will use for step size is $\ds \Delta x$, but a frequent label we will use in the notes and in the textbooks is $h$. Hence, for uniform step size we will have: $\ds \Delta x = h = \vert x_k - x_{k-1} \vert$. To describe the implications of the order of convergence the definition we introduce in (\ref{e:order_in_Burden})
$$\vert a_n-a \vert \le K \ \vert \beta_n\vert .$$
In regards to numerical methods involving step sizes the order of convergence can be described as the value of $\alpha$ where $\ds x_h$ is the approximation using the step size $h$, $\ds x^*$ is the exact value along with fact there exists a $K\ge 0 $ such that,
$$\vert  x_h - x^* \vert \le K h^{\alpha}.$$
The true power of the order of convergence comes from the fact that $\alpha$ is in the power of the expression. The simplest version of convergence is linear convergence where $\ds alpha = 1$, which gives us the relation of
$$\vert x_h - x^* \vert \le K h.$$
Therefore, if the step size is reduced by some factor then we expect the resulting error to be reduced by the same factor, i.e., given an old step size of $\ds h_0$ and a new step size of $\ds h_1 = \frac {h_0}5$ then we expect the new error to have the relationship $\ds {\rm new \ error} = \frac {{\rm error}}5$. 

We will encounter methods that produce a second order of convergence, which means that $\ds \alpha = 2$ and this type of convergence is often referenced as {\bf quadratic} convergence. This faster convergence is observed when we duplicate the move from a step size of $\ds h_0$ to a new step size of $\ds h_1 = \frac {h_0}5.$ The error bound for the old step size ($\ds h_0$) is given by 
$$\vert \hat x_{h_0} - x^* \vert \le K h_0^{2},$$
but the approximation using the new step size ($\ds h_1 = \frac {h_0}5$) results in:
$$\vert \hat x_{h_1} - x^* \vert \le K h_1^2 = K \left ( \frac {h_0}5 \right ) ^2 = \frac {Kh_0^2}{25}.$$
Notice the reduction in error is $\ds \left ( \frac 15 \right ) ^ 2$, which gives an exponential reduction in error with the reduction in step size. Methods where the order of convergence is even greater will generate even faster convergence as the step size is reduced. We can demonstrate this convergence by using some numerical methods we will see later in the course by introducing the analysis of convergence in this section. 

\subsection{An Example to Illustrate Order of Convergence}
From Calculus 1 we have the definition of the derivative: $\ds \lim_{h\rightarrow 0}\frac {f(x+h)-f(x)}h = f^{\prime}(x).$ This definition inspires our first approximation of the derivative, which we will call the {\it forward} difference approximation
$$f^{\prime}(x) \approx \frac {f(x+h)-f(x)}h.$$
Another approximation method for the derivative that we will be using is the {\it centered} difference approximation
$$f^{\prime}(x) \approx \frac {f(x+h)-f(x-h)}{2h}.$$
We will apply these two methods to one of our favorite functions, $\ds f(x)=e^x$, to observe order of convergence at work. 

Consider the case of approximating the derivative of $\ds f(x) = e^x$ when $x=1$, which analytically we know the exact answer is $\ds f^{\prime}(1) = e$. We can use the work in the MATLAB script file, converg.m, as a way to implement the calculations and generate the plot that will illustrate the features we would like to capture. The program starts with a step size of $h = 0.8$ and then goes through a progression of smaller step sizes where each subsequent step size is half of the one previous. 
\begin{figure}[!ht]
\centering
\includegraphics[height=70mm]{converg_error_plot.png}
 \caption{Plot of error for first derivative with blue=forward and red=centered}
 \label{f:converg_error_plot}
\end{figure}
All the answers are kept in vectors and then it is possible to construct the error and plot it relative to each step size. Figure \ref{f:converg_error_plot} shows a plot of error at each step for both the forward difference ({\color{blue}blue}) and the centered difference ({\color{red}red}). It is easy to observe that centered difference has smaller error and as the step size (h) is decreased the error shrinks for both methods. 
\begin{figure}[!ht]
\centering
\includegraphics[height=70mm]{converg_error_log_plot.png}
 \caption{Plot of error for first derivative with blue=forward and red=centered}
 \label{f:converg_error_log_plot}
\end{figure}
The {\it order} of the convergence is much easier to observe if we plot the $\ds \ln ({\rm step size})$ versus the $\ds \ln ({\rm error})$, where the order is visible as the slope of the line. In Figure \ref{f:converg_error_log_plot} it should be noted that slope of the {\color{red}red} line (centered difference approximation) is much steeper than the {\color{blue}blue} line (forward difference approximation). We will learn later that the forward difference is a first order method and that the centered difference is second order. The fact that the order of convergence is represented as slope in a log-log plot should be evident through the application of the log rules:
$$\ln (Kh^{\alpha}) = \ln (K) + \alpha \ln(h).$$
Thus, a way to graphically compare the convergence across different numerical methods is to compare the slopes of the errors in the log-log plot. 
\subsection{Stopping Conditions to use in a Matlab program}
There are a variety of stopping conditions we will use when coding a numerical method into Matlab. If we happen to have the exact answer, let us call the exact answer $\ds x^*$ then two natural stopping conditions are:
$${\rm error} = \vert x^* - x_k \vert < tol,  \ \ {\rm and} \ \ {\rm absolute \ relative \ error} = \frac {\vert  x^* - x_k \vert}{\vert x^* \vert}<  tol,$$
where $tol$ is the stopping tolerance. As noted earlier, the relative measure has the advantage of being relative to the size of the exact value and it also has the disadvantage that it can have problems when $\ds x^* \approx 0$ along with being undefined when $\ds x^* = 0$.  Both stopping conditions are only advisable when the exact value is known. When the exact value is not known then we have analogous measures to use for stopping conditions with the first being:
$$ \vert x_k - x_{k-1} \vert < tol;$$
and the second being:
$$ \frac {\vert x_k - x_{k-1} \vert }{\vert x_k \vert } < tol.$$
These two stopping conditions are {\it not} the same as using error and so using the same tolerance for these will generate different results. It should also be noted that the relative measure ($\ds \frac {\vert x_k - x_{k-1} \vert }{\vert x_k \vert } < tol$) will also suffer due to rounding error if $\ds x_k \approx 0$ and will not be defined when $\ds x_k=0$. 

%------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{4}{Linear Interpolation and Newton's Divided Difference}{Kyle Riley}{Riley}
\section{Linear Interpolation}
An item that has likely been covered in other classes is linear interpolation, which is a common numerical method used in a variety of applications. This technique provides a great opportunity to introduce some of the fundamental concepts of numerical analysis along with an opportunity to program, which will occur in homework 2. \par

The base situation is that we have two data points: $\ds (x_0,y_0)$ and $\ds (x_1,y_1)$, where these values are given from observation or some other evidence gathering. We are interested in a new point between the two given data points for a given value $x$ where the {\it interpolated} point, $(x,y)$, lies on the straight line between $\ds (x_0,y_0)$ and $\ds (x_1,y_1)$. The situation is illustrated in Figure \ref{f:LinearInterp}. 

\begin{figure}[!ht]
\centering
\includegraphics[height=70mm]{LinearInterp.png}
 \caption{Figure for Linear Interpolation}
 \label{f:LinearInterp}
\end{figure}
There are several ways to derive the mathematical method that computes the value of $y$ associated with $x$ on the straight line between $\ds (x_0,y_0)$ and $\ds (x_1,y_1)$ with one of the simple approaches coming from the fact that all three points lie on the same straight line. This means that the slope between $\ds (x_0,y_0)$ and $(x,y)$ must have the same slope as between $\ds (x_0,y_0)$ and $\ds (x_1,y_1)$. Thus, we generate a formula:
$$\frac {y_1-y_0}{x_1-x_0} = \frac {y-y_0}{x-x_0}, $$
which can be simplified to
$$y-y_0 = \frac {y_1-y_0}{x_1-x_0}(x-x_0), $$
and finally the result of
\begin{equation}
y = y_0 + \frac {y_1-y_0}{x_1-x_0}(x-x_0).    
  \label{e:LinearInterp}  
\end{equation}
Linear interpolation translates into a relatively simple process that if given the values of $\ds (x_0,y_0)$,  $\ds (x_1,y_1)$, and $x$ then the use of the formula in (\ref{e:LinearInterp}) results in a calculation of the point $(x,y)$ on the straight line between $\ds (x_0,y_0)$ and  $\ds (x_1,y_1)$. In homework assignment 2, one of the exercises will be to program a Matlab function to complete the calculation of linear interpolation. There are many questions that can be raised with this simple method: \begin{itemize}
    \item The picture in Figure \ref{f:LinearInterp} shows $\ds x_0$ as smaller than $\ds x_1$, is that required for the method to work properly?
    \item What can influence the accuracy of this method?
    \item We have discussed the use of flags in a Matlab program to check against faulty input and/or faulty output. Is there any scenario(s) where a flag is needed in applying this method?
\end{itemize}
One thing to note is that interpolation is a method or process that uses to formulate estimates that occur {\it between} data points and we will cover many of the basic interpolation methods in Chapter 3 of \cite{LB16}. Another general form of numerical methods is {\bf extrapolation} where estimates are generated for values {\it outside} the data set. It should be noted that extrapolation methods usually have very different assumptions in use and are much more sensitive to error than interpolation. 
\section{Newton's Divided Difference}\label{section_div_diff}
A good source on Newton's Divided Difference can be found in \cite{KK09} in Chapter 5, section 3. Newton's Divided difference is an elegant design to construct interpolation polynomials directly from source data. In this scenario, source data is given in the form of $\ds \{(x_i,y_i) \} _{i=0}^n$ with premise that the data is a sampling of an unknown function $\ds f(x_i)$, which means that the data can also be denoted by $\ds \{(x_i,f(x_i)) \} _{i=0}^n$. One crucial component of the Newton's Divided Difference method is the notation:
$$f[x_1,x_0] = \frac {f(x_1)-f(x_0)}{x_1-x_0}, $$
and the notation builds recursively 
$$f[x_2,x_1,x_0]=\frac {f[x_2,x_1]-f[x_1,x_0]}{x_2-x_0},$$
and so on
$$f[x_3,x_2,x_1,x_0]=\frac {f[x_3,x_2,x_1]-f[x_2,x_1,x_0]}{x_3-x_0}.$$
The Newton interpolating polynomials that come from divided difference are defined recursively with a few examples:
$${\rm linear \ form}= P_1(x) = f(x_0) + f[x_1,x_0](x-x_0)$$
$${\rm quadratic \ form}= P_2(x) = f(x_0) + f[x_1,x_0](x-x_0) + f[x_2,x_1,x_0](x-x_0)(x-x_1), {\rm and}$$
$${\rm cubic \ form} = P_3(x)$$
$${\rm where} \ P_3(x) = f(x_0) + f[x_1,x_0](x-x_0) + f[x_2,x_1,x_0](x-x_0)(x-x_1) +  f[x_3,x_2,x_1,x_0](x-x_0)(x-x_1)(x-x_2).$$
Notice that Newton's Divided Difference offers the following:
\begin{itemize}
    \item It is recursive with an easy way of building higher degree polynomials with additional data
    \item The data {\it does not} have to be evenly spaced
    \item Higher degree polynomials are increasingly oscillatory between data points
    \item This is an excellent method to practice your Matlab programming! (As you will see in the homework.)
\end{itemize}
Please note that we are seeing Newton's Divided Difference earlier than it is presented in the textbooks and the reason is that it provides a good opportunity to practice Matlab programming and it also presents an opportunity to foreshadow upcoming material. Newton's Divided Difference is an interpolation technique we will see in Chapter 3 of \cite{LB16}, but it is also true the component calculations of this method are techniques for calculating derivatives and that is a subject that is part of Chapter 4 in \cite{LB16}. A couple of spoilers: $\ds f[x_1,x_0]$ is a technique for calculating the first derivative, while $\ds f[x_2,x_1,x_0]$ is a technique for calculating the second derivative. We will see all of this again later in the course and cover it in more detail, but for now this is material we are going to use to strengthen the programming skills in Matlab.  

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{5}{Root Finding}{Kyle Riley}{Riley} \par
Chapter 2 of \cite{LB16} will cover the methods of interest for us in root finding methods, which can also be classified as solving a nonlinear equation. Mathematically, the problem we see is finding a value $\ds x^*$ for a given function, $f(x)$, such that $\ds f(x^*)=0$. If the goal is to solve a problem where the function is equal to some value, for example, solving $f(x)=3$ then this can easily be adapted by defining a new function: $\ds g(x) = f(x)-3$ and returning to the goal of finding where $g(x)=0$. \par

The problem of solving nonlinear equations show up in a variety of applications across Engineering and Science. A fun example comes from \cite{KK09} with the first example of a general engineering problem in Chapter 3. This example can be found on my homepage at \href{https://webpages.sdsmt.edu/~kriley/class/m373/KK09_ch3_example.pdf}{https://webpages.sdsmt.edu/~kriley/class/m373/KK09\_ch3\_example.pdf}. This example describes the problem of determining the level a ball will float in a tank of water with the ball having specific density of 0.6 and the ball having the radius of 5.5 cm. The complete derivation of the problem is presented in the text of \cite{KK09} and here we will provide a brief synopsis. \par

The ball floats when there is balance between the weight of the ball and the buoyancy of the ball on water. The weight of the ball is given by:
$$F_{weight} = \frac 43 \pi R^3 \rho _b g, $$
where R is the radius of the ball, $\ds \rho_b$ is the density of the ball, and $g$ is the acceleration due to gravity. The buoyancy force is given by
$$F_{buoyancy} = ({\rm volume \ of \ ball \ under \ water}) ({\rm density \ of \ water})g$$
Let $x$ be the depth of the ball in the water when the forces are balanced, then using the slicing techniques from Calculus can reveal the formula:
$$({\rm volume \ of \ ball \ under \ water}) = \pi x^2\biggr ( R-\frac x3 \biggr ). $$
Thus, the balancing equation is:
$$\frac 43 \pi R^3 \rho_b g = \pi x^2 \biggr ( R-\frac x3 \biggr ) \rho_w g,$$
where $\ds \rho_w$ is the density of water. A bit of simplification results in 
$$4 R^3 \frac {\rho_b}{\rho_w}-3x^2R+x^3 = 0,$$
and simple substitution of all the constants involved results in
\begin{equation}
4(0.055)^3(0.6)-3x^2(0.055)+x^3=0.    
    \label{e:nonlinear_sample_1}
\end{equation}
The main interest in Math 373 is not so much where (\ref{e:nonlinear_sample_1}) comes from, but how do we find solutions (if any) to this problem. A natural approach is to graph the function to see where it obtains the value zero, which some careful plotting in Matlab produces the plot in Figure \ref{f:nonlinear_sample_1}. 
\begin{figure}[!ht]
\centering
\includegraphics[height=70mm]{nonlinear_sample_1.png}
 \caption{Plot of y=f(x) for the buoyancy ball problem}
 \label{f:nonlinear_sample_1}
\end{figure}
The plot clearly reveals three different roots to the problem and it is safe to eliminate the root with a negative value since the density of the ball prevents it from hovering in air above the water. The high quality graphic tools do allow us the ability to zoom in on each root and achieve higher modes of accuracy, but we need a method that allows us to find each root to a defined accuracy. Chapter 2 of \cite{LB16} will provide a few basic tools that we will be using in the homework along with more elaborate methods built into Matlab. \par
The methods we will cover are: Bisection Method, Fixed Point Method (in some books fixed point can be called the method of successive substitution), Newton's Method (in some books this is often referenced as the Newton-Raphson Method), and Secant Method. We will also discuss the use of the fzero function from Matlab. 
\section{Quick Outline on Numerical Methods to solve Nonlinear Equations}
The books (\cite{LB16} and \cite{KK09}) provide a great deal of information and background about the numerical methods that can be used to solve a nonlinear equation. In this section, we will highlight a few bullet points for each method. 

\begin{itemize}
    \item Bisection Method (section 2.1 of \cite{LB16})
    \begin{itemize}
        \item This method is part of a class of methods called {\it bracketing methods}, since they require two points to bracket the solution. Many of the other methods in chapter 2 only require one initial guess to start the method (not a bracket).
        \item Convergence is relatively slow compared to the other methods in Chapter 2.
        \item The method is robust since it is not possible for the method to diverge. If there is a root in the bracketed area and the function is continuous then this method will eventually "find" the root. 
        \item There is also a video on Bisection Method on the YouTube playlist for this course. 
    \end{itemize}
    \item Fixed Point Method (section 2.2 of \cite{LB16})
    \begin{itemize}
        \item The major issue is deriving the fixed point problem that corresponds to the root problem that is to be solved. If the appropriate fixed point problem can be found and the method converges then this is a great method. However, it is not easy (in general) to find a fixed point problem that provides fast convergence for a given nonlinear equation. 
        \item Once the fixed point function is found then this method is easy to program.
        \item This program can easily diverge and not find the desired root. 
        \item Convergence is often dependent on the initial guess being "close enough". 
        \item The bottom line, when fixed point method works then it works very well, but it is not easy to find the appropriate fixed point function.
        \item There are two videos on Fixed Point on the YouTube playlist for this course.  
    \end{itemize}
    \item Newton's Method (aka Newton-Raphson Method) (section 2.4 of \cite{LB16})
    \begin{itemize}
        \item This method is based on linear approximation.
        \item Convergence can be fast (quadratic convergence), but it is common for the method to not converge.
        \item Convergence is often dependent on the initial guess being "close enough".
        \item This method requires the use of the derivative, which is not always available. 
        \item This method does require $\ds f'(x_k)\neq 0$ for each iteration. 
        \item If the derivative is zero at the root then this can slow convergence or cause the method to not converge. 
        \item Bottom line is that Newton's Method can offer the fastest convergence of the methods discussed, but it can be difficult to implement. 
        \item There are a couple of videos on Newton's Method on the YouTube playlist for this course, but a simple search on YouTube would reveal more sources since this is a common method that is used widely in a variety of disciplines. 
    \end{itemize}
    \item Secant Method (section 2.4 of \cite{LB16})
    \begin{itemize}
        \item This is really Newton's Method with an approximation replacing the derivative.
        \item Convergence is slower than quadratic, but often better than linear convergence. Hence, the classification of super linear convergence. 
        \item This method needs two points to start the method instead of just one point, it is possible to use the seeded approach where one point is used to derive the second point and then start the method. 
        \item This method does require each secant line to be non-horizontal.
        \item Convergence problems can be similar to Newton's Method since $\ds x_0$ has to be close enough, the slope of the secant line needs to be nonzero, and if the derivative at the root is zero then convergence can be slow or the method could not converge. 
    \end{itemize}
\end{itemize}
\section{fzero in Matlab}
Matlab does have a solver developed to solve nonlinear functions and the name of this solver is fzero.  The structure of this Matlab function is:
\begin{verbatim}
x = fzero(fun,x0)    
\end{verbatim}
where fun is an anonymous function or function from an m-file and x0 is an initial guess of the root. The actual algorithm is based on the Bisection method, which means that fzero uses x0 to search for a second point in the domain that has a functional value that differs in sign from f(x0). It is also possible to enter x0 as a vector that contains to values that bracket the root. 
%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{6}{Quick Note on Solving Nonlinear Systems}{Kyle Riley}{Riley} \par
The theory and practice of solving nonlinear systems is easily enough material for several books. This quick note is to show the simple connections between the methods covered in Math 373 and how they can be extended to solving nonlinear systems. \par 

Nonlinear systems involve multiple variables and multiple equations. A simple example:
\begin{align}
    x^2-2y+z+3 &= 0 \\
    \cos(x) + y^2 &= -9 \\
    x^2+y^2+z^2 & = 1.
\end{align}
Just as with equation of single variable, this problem can be posed as finding where the system produces zero for each equation, i.e.
\begin{align}
    x^2-2y+z+3 &= 0 \\
    \cos(x) + y^2 + 9 & = 0\\
    x^2+y^2+z^2 -1 & = 0.
\end{align}
A crucial part of this extension is representing it in terms of vectors. For those students that have a background that includes Calculus 3 then this extension will be written in a form that was covered in that course. Let $\ds \vec w = \begin{pmatrix} x \cr y \cr z \end{pmatrix}$ and we can transform the nonlinear system into a vector valued function:
$$\vec F(\vec w) = \begin{pmatrix} x^3 - 2y + 3 \cr \cos(x) + y^2 + 9 \cr x^2 + y^2 + z^2 -1  \end{pmatrix}.$$
Thus, the goal is to find a point, $\ds \vec w ^* = \begin{pmatrix} x^* \cr y^* \cr z^* \end{pmatrix}$, where $\ds \vec F(\vec w ^*) = \begin{pmatrix} 0 \cr 0 \cr 0 \end{pmatrix} = \vec 0$. In many cases the methods we covered for a single nonlinear equation are just extended into vector form of the same method. For example, finding the root to $f(x)=0$ is the same as a fixed point problem for $g(x)$ if $\ds f(x) = g(x)-x$. The extension is evident with $\vec F(\vec w) = \vec 0$ related to the fixed point problem for $\ds \vec G(\vec w)$ where $\ds \vec F(\vec w) = \vec G(\vec w)-\vec w$. \par 

Another example is Newton's Method, which in single variable has the form $\ds x_{new} = x_{old} - \frac {f(x_{old})}{f'(x_{old})}$. This also has an extension given by:
$$\vec w_{new} = \vec w_{old} - J^{-1}(\vec w_{old})\ \vec F(\vec w_{old}),$$
where J is the Jacobian matrix that is associated with $\ds \vec F(\vec w)$. You can learn more regarding methods for solving nonlinear systems in \cite{BF11} and \cite{KC02}. 

\section{Example of Implementing Newton's Method for Systems}

Let $\ds \vec F$ be a vector valued function.

For example,
\begin{equation} 
\vec F \begin{pmatrix} x \\ y \\ z  \end{pmatrix} = \begin{pmatrix} x^2 - yz \\ x^2y^2 \\ ze^x \end{pmatrix} 
\label{e:ex1}
\end{equation}
is a vector valued function. The functions: $\ds f_1(x,y,z)=x^2-yz$, $\ds f_2(x,y,z) = x^2y^2$, and $\ds f_3(x,y,z)=ze^x$ are the component functions for the vector valued function $\ds \vec F$. The Jacobian is a matrix that is used in the multivariate version of Newton's Method to solve nonlinear systems.

In general, a vector valued function is a mapping from one vector space to another, which is denoted by $\ds \vec F : R^m \rightarrow R^n$ where
$$\vec x \begin{pmatrix} x_1 \cr x_2 \cr \vdots \cr x_m  \end{pmatrix} \ \ {\rm and} \ \ \vec F(\vec x) = \begin{pmatrix} f_1(\vec x) \cr f_2(\vec x) \cr \vdots \cr f_n(\vec x) \end{pmatrix}.$$

The {\bf Jacobian matrix} that corresponds to $\ds \vec F(\vec x)$ has n rows and m columns:
\begin{equation}
J(\vec x) = \begin{pmatrix} \frac {\partial f_1}{\partial x_1} & \frac {\partial f_1}{\partial x_2} & \dots & \frac {\partial f_1}{\partial x_m}   \\  \\  \frac {\partial f_2}{\partial x_1} & \frac {\partial f_2}{\partial x_2} & \dots & \frac {\partial f_2}{\partial x_m} \\ \vdots & \vdots & \vdots & \vdots \\ \frac {\partial f_n}{\partial x_1} & \frac {\partial f_n}{\partial x_2} & \dots & \frac {\partial f_n}{\partial x_m}  \end{pmatrix}. \label{e:def}
\end{equation}

The Jacobian that corresponds to our example in (\ref{e:ex1}) is a simple 3 by 3 matrix
$$J\begin{pmatrix} x \cr y \cr z \end{pmatrix} = \begin{pmatrix} \frac {\partial f_1}{\partial x} & \frac {\partial f_1}{\partial y} &  \frac {\partial f_1}{\partial z}   \cr  \cr \frac {\partial f_2}{\partial x} & \frac {\partial f_2}{\partial y} &  \frac {\partial f_2}{\partial z} \cr \cr \frac {\partial f_3}{\partial x} & \frac {\partial f_3}{\partial y}  & \frac {\partial f_3}{\partial z}  \end{pmatrix} = \begin{pmatrix} 2x &  -z &  -y   \cr  2xy^2 &  2x^2y &  0 \cr  ze^x & 0 & e^x  \end{pmatrix}.$$
 
 Consider another vector valued function
$$ \vec G \begin{pmatrix} x \cr y \cr z \end{pmatrix}=\begin{pmatrix} \sin(xy) \cr z\ln(x) \cr x^2+y^2+z^2 \end{pmatrix}$$
then the corresponding Jacobian would be:

$$J\begin{pmatrix} x \cr y \cr z \end{pmatrix}=\begin{pmatrix} y\cos(xy) & x\cos(xy) &  0   \cr  z/x &  0 &  \ln(x) \cr  2x & 2y & 2z   \end{pmatrix}.$$
Please note that the Jacobian can be evaluated at a point. For example, to evaluate the Jacobian above at $x=1$, $y=0$, and $z=3$ then
$$J\begin{pmatrix} 0 \cr 1 \cr 3 \end{pmatrix}=\begin{pmatrix} 0 & 1 &  0   \cr  3 &  0 &  0 \cr  2 & 0 & 6  \end{pmatrix}.$$

Consider the nonlinear system
\begin{eqnarray}
x^3-y^2+y-z^4+z^2 = 0 \cr
xy + yz + xz = 0 \cr
\frac y{xz} = 0 \label{e:nonlinear}
\end{eqnarray}

Finding the solution to (\ref{e:nonlinear}) is the same as finding the roots to a nonlinear vector valued function:
$$\vec F\begin{pmatrix} x \cr y \cr z \end{pmatrix} = \begin{pmatrix}  x^3-y^2+y-z^4+z^2 \cr xy + yz + xz \cr \frac y{xz}  \end{pmatrix}.$$
Recall Newton's Method for finding roots to single variable functions,
$$x_{k+1} = x_k - \frac {f(x_k)}{f'(x_k)},$$
where this is an iterative method that starts with some initial guess $x_0$ for the root value, and under the proper conditions, this method will converge to the desired root value.

The Newton's Method for nonlinear systems is just a generalization of the original single variable formula
$$\vec x_{k+1} = \vec x_k - J^{-1}(\vec x_k)\vec F (x_k),$$
where $\ds J(\vec x_k)$ is the corresponding Jacobian matrix evaluated at the point $\ds \vec x_k$. It follows that each iteration of Newton's Method requires the solution of a linear system
$$J(\vec x_k) \vec y = \vec F(\vec x_k),$$
where the method of Gaussian Elimination can be used to solve the system. It is also true that the matrix inverse, $\ds J^{-1}$ can be used to calculate the solutions at each step. Given some desired level of tolerance for the proximity to the root to the vector valued function the following algorithm is a rough template of the method (give some initial guess).

$\vec x_0 = {\rm initial \ guess}$\\
$ \vec y = J^{-1}(\vec x_0)\vec F(\vec x_0)$ \\
$\vec x_1 = \vec x_0 - \vec y$ \\
$ k = 1$\\
 \\
$ {\rm While} \ \Vert \vec x_{k} - \vec x_{k-1} \Vert > {\rm tolerance}$ \\
$ \ \ \vec y = J^{-1}(\vec x_k)\vec F(\vec x_k)$ \\
$ \ \ \vec x_{k+1} = \vec x_k - \vec y$ \\
$ \ \ k = k+1$ \\
$ {\rm end}$

Consider the nonlinear system
\begin{eqnarray}
x^2+2\sin(y)+z = 0 \cr
\cos(y) - z = 2 \cr
x^2 + y^2 + z^2 = 2 \label{e:example2}.
\end{eqnarray}
If we define $\ds \vec F(\vec x)$ as
$$\vec F \begin{pmatrix} x \cr y \cr z \end{pmatrix} = \begin{pmatrix}  x^2+2\sin(y)+z \cr \cos(y) - z - 2 \cr x^2 + y^2 + z^2 - 2 \end{pmatrix},$$
then the corresponding Jacobian is:
$$J\begin{pmatrix} x \cr y \cr z \end{pmatrix}= \begin{pmatrix} 2x & 2 \cos(y) & 1 \cr 0 & -\sin(y) & -1 \cr 2x & 2y & 2z \end{pmatrix}.$$
To approximate the solution using Newton's Method requires the use of an initial guess, which will take the initial guess of $\ds x=1.1$, $y=0.1$, and $z=-0.9$. It follows that
$$ \vec F \begin{pmatrix} 1.1 \cr 0.1 \cr -0.9 \end{pmatrix} = \begin{pmatrix}  0.5097 \cr -0.1050 \cr 0.03  \end{pmatrix}, \ \ {\rm and} \ J\begin{pmatrix} 1.1 \cr 0.1 \cr -0.9 \end{pmatrix}= \begin{pmatrix} 2.2 & 1.99 & 1 \cr 0 & -0.998 & -1 \cr 2.2 & 0.2 & -1.8 \end{pmatrix}.$$
Thus, the first iteration is found by
$$\vec x_1 = \begin{pmatrix} 1.1 \cr 0.1 \cr -0.9 \end{pmatrix} - J^{-1}(\vec x_0)\vec F(\vec x_0) = \begin{pmatrix} 1.0217 \cr -0.0229 \cr -0.9927 \end{pmatrix}$$
After just three iterations the approximation yields
$$\vec x_3 = \begin{pmatrix} 1.000000714 \cr -0.000000544 \cr -0.999999795 \end{pmatrix},$$
which points to the solution of $x = 1$, $y=0$, and $z=-1$. 

{\bf Remarks on Newton's Method for solving systems}\medskip \par \noindent
1. The same properties in one dimension are true in higher dimension with rapid convergence of the method if the initial guess being sufficiently close to the actual root. \medskip \par \noindent
2. Convergence of the method can be hampered if $\ds J(\vec x_k)$ is singular or gets close to being singular, i.e. no inverse exists. \medskip \par \noindent
3. If the root, $\ds \vec x^*$, has the property of $\ds J(\vec x^*)$ is singular then the convergence may be impossible no matter how close the initial guess. \medskip \par \noindent
4. The region of convergence is even smaller in higher dimensions than in one dimension. For our example the guess of $\ds x=1.1$, $y=0.1$, and $z=-0.9$ generates rapid convergence within just a few iterations. However, the initial guess of $\ds x=1$, $y=1$, and $z=0$  does not produce the same convergence in 100 iterations. \medskip \par \noindent
5. To track convergence you can use the same indicator as the one dimensional case by setting a desired tolerance for $\ds \Vert \vec x_{k+1} - \vec x_k \Vert < {\rm tolerance}$ .

As can be seen, implementing the solution method of Newton's Method for nonlinear systems does rely heavily on solving a series of linear systems. Numerical methods for solving linear systems will come later in the course. 

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{7}{More on Interpolation}{Kyle Riley}{Riley} 
\section{Interpolation}
Chapter 3 of \cite{LB16} takes a unique approach in presenting interpolation and manages to get into a few details that will not be as important to our class. A more compatible approach that matches the focus of our course is the approach discussed in Chapter 5 (sections 2, 3, and 5) of \cite{KK09}, which it is encouraged that the interested student review this material. We will use these notes to help clarify the concept of interpolation along with presenting a couple of basic methods. Interpolation is a foundational concept in numerical analysis and virtually all other methods presented in the subject can be tied to an interpolation method in some way. However, in practice there are only a couple of basic methods that are used in application and so we will focus on these few methods and let the interested reader pursue the subject in more detail. There is a vast library of literature on interpolation, but you can find more detail regarding interpolation from our basic resources of: \cite{KA89,BF11}, and \cite{KC02}. \par

In basic terms, interpolation starts with a set of data, $\ds \{(x_i, y_i)\}_{i=0}^n$. The object is to devise a function such that the function matches the data on the data points, i.e., $\ds f(x_i)=y_i$ for all i. The function is known as the {\it interpolant}, or {\it interpolating function}. Once the interpolating function is established then it can be used to approximate the behavior of the data for any $x$ value that is inside the data set. One could also use $f(x)$ to approximate derivatives and be used for integration. We have already worked with a similar tool in the Taylor polynomial, but the big difference is that a Taylor polynomial uses the function and derivative values at a point in order to approximate a function elsewhere. Interpolation uses multiple data values and can also use information regarding the derivative at data points (if available) in order to build an interpolant. There are several methods that can be used for interpolation, but we will focus on a couple of the more common methods that are used in practice. 

\subsection{Direct Method}
The direct method is presented in section 2 of Chapter 5 of \cite{KK09} and the name of this method is an accurate label to attach to this approach. Given a data set of two points, $\ds (x_0,y_0)$ and $\ds (x_1, y_1)$, then goal to construct a line that interpolates this data set becomes the target of constructing $\ds f(x)=a_0 + a_1 x$, which is an equation with two unknown coefficients: $\ds a_0$ and $\ds a_1$. The definition of the interpolation function requires that $\ds f(x_0)=y_0$ and $\ds f(x_1)=y_1$, which generates two equations:
\begin{align*}
    a_0 + a_1 x_0 &= y_0\\
    a_0 + a_1 x_1 &= y_1.
\end{align*}
This situation results in two equations and two unknowns (unknowns are $\ds a_0$ and $\ds a_1$), which means you can directly solving this linear system. It might be beneficial to rewrite this system into matrix form:
$$\begin{pmatrix} 1 & x_0 \cr 1 & x_1 \end{pmatrix}\begin{pmatrix} a_0 \cr a_1 \end{pmatrix} = \begin{pmatrix} y_0 \cr y_1 \end{pmatrix}.$$
This method quickly extends to larger data sets along with higher degree polynomials. For example, given a data set $\ds \{(x_i,y_i)\}_{i=0}^3$ then we can fit a polynomial of degree 3, which takes the form of: $\ds f(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3$. It must be the case that $\ds f(x_i)=y_i$ for every $i$ and this translates into linear system:
$$\begin{pmatrix} 1 & x_0 & x_0^2 & x_0^3 \cr  \cr 1 & x_1 & x_1^2 & x_1^3 \cr \cr 1 & x_2 & x_2^2 & x_2^3 \cr \cr 1 & x_3 & x_3^2 & x_3^3 \end{pmatrix}\begin{pmatrix} a_0 \cr \cr a_1 \cr \cr a_2 \cr  \cr a_3 \end{pmatrix} = \begin{pmatrix} y_0 \cr \cr  y_1 \cr \cr y_2 \cr \cr y_3 \end{pmatrix}.$$
This reveals that a collection of $n+1$ distinct data points can be used to generate an interpolating polynomial of degree $n$, however we will later discuss the merit of avoiding interpolation with high degree polynomials. 

\subsection{Newton's Divided Difference}
We discussed Newton's Divided Difference in section \ref{section_div_diff} of these notes and they are also the topic of section 3 in Chapter 5 of \cite{KK09}. The work in homework 2 should provide a solid understanding on how to use Matlab to produce a Newton's Polynomial and use it to interpolate for a data set. 

\subsection{The benefits and danger of using high degree interpolating polynomials} 
The title of section 7 of Chapter 5 in \cite{KK09} is: "Higher Order Interpolation Is a Bad Idea". The title really speaks for itself and the presentation in the book gives a good explanation as to the problems with using high degree polynomials for interpolation. High degree interpolating polynomials are smooth and converge quickly around data values, but high degree polynomials are also highly oscillatory and vary greatly away from data values. Thus, high degree interpolating polynomials produce more oscillatory artifacts for values distant from other data values. Please refer to the discussion in section 7 of Chapter 5 in \cite{KK09} to see an example of this oscillatory behavior. 

\subsection{Cubic Splines}
The most popular technique that is used in practice for interpolation is cubic splines. Cubic splines really offers the best combination of interpolation that involves a smooth interpolation (both first and second order derivatives are continuous), but the interpolating function is also a low order polynomial that avoids the dangers we have discussed when using higher degree polynomials for interpolation.

The spline technique is really a general approach where the data set of $\ds \{(x_i,y_i)\}_{i=0}^n$ is split up into segments and an interpolation function is devised for each segment. The final result is a piecewise defined interpolation function 
$$f(x) = \begin{cases} S_1(x) & x_0\le x < x_1 \\ 
S_2(x) & x_1 \le x < x_2 \\
S_3(x) & x_2 \le x < x_3 \\
\vdots \\
S_n(x) & x_{n-1} \le x \le x_n 
\end{cases}$$
For the general spline technique, the component functions of $\ds S_k(x)$ can be any kind of function. However, for {\it cubic} splines the restriction is that every component function is a cubic polynomial. A classic formulation for cubic splines is that 
$$S_k(x) = a_k + b_k(x-x_k) + c_k(x-x_k)^2 + d_k(x-x_k)^3, $$
where $\ds a_k$, $\ds b_k$, $\ds c_k$, and $\ds d_k$ are unknown variables. For a given data set,  $\ds \{(x_i,y_i)\}_{i=0}^n$, there would be $n$ segments with a cubic polynomial for every segment (each with 4 coefficients). This generates a situation that entails $4n$ unknown variables, but to an interpolating function each cubic spline must match the data value at each endpoint for each segment. It follows that matching the splines to each endpoint generates $2n$ equations. In addition, we will require that 
$\ds S'_k(x_k) = S'_{k+1}(x_k)$ for all $k = 1 ... n-1$, which is another $n-1$ equations. Furthermore, we will require that $\ds S''_k(x_k) = S''_{k+1}(x_k)$ for all $k = 1 ... n-1$, which is another $n-1$ equations. Lastly, a set of boundary conditions are used for the endpoints and these boundary conditions can take a variety of forms with a few examples being:
\begin{itemize}
    \item $\ds S''_1(x_0) = 0 =  S''_{n}(x_n)$,
    \item  $\ds S'_1(x_0) = m_0$ and $\ds   S'_{n}(x_n)=m_n$ for fixed values of $\ds m_0$ and $\ds m_n$,
    \item  $\ds S'_1(x_0) = m_0$ and $\ds   S''_{n}(x_n)=0$ for fixed values of $\ds m_0$, or
    \item  $\ds S''_1(x_0) = 0$ and $\ds   S'_{n}(x_n)=m_n$ for fixed values of $\ds m_n$.
\end{itemize}
The first list item above is one of the most commonly used endpoint condition, but the others on the list are also regularly used along with other constructions that are more specific to a particular problem. Reviewing the list reveals that the conditions involved sponsor $2n + n-1 + n-1 + 2 = 4n$ equations to identify $4n$ variables. One can conclude that cubic splines is really just a large linear systems problem. 

Matlab has the spline command that will automatically calculate a cubic spline for a data set. The syntax for the command is rather simple with x being the x coordinates for a data set and y being the y coordinates for a data set along with xp being coordinates that you desire to be interpolated then
\begin{verbatim}
    >> yp = spline(x,y,xp);
\end{verbatim}
generates the interpolated y values. If you want to see the coefficients of each of component cubic polynomials then that procedure is a bit more complicated. 
\begin{verbatim}
    >> pp = spline(x,y);
\end{verbatim}
stores all the information in a struct in Matlab. If you just want the coefficients then that can be accessed via
\begin{verbatim}
   >> [~, coeffs] = unmkpp(pp);
\end{verbatim}
The coefficients for each of the component cubic splines can be found in the coeffs array, for example
\begin{verbatim}
    >> coeffs

coeffs =

    0.0038    0.2017   20.3090         0
    0.0038    0.3152   25.4774  227.0400
    0.0056    0.3720   28.9132  362.7800
    0.0081    0.4566   33.0559  517.3500
    0.0081    0.5174   35.4907  602.9700
\end{verbatim}
The coefficients are stored for each subinterval are stored in each row of the array and the coefficients are stored in decreasing order. Thus, the third row from the row above corresponds to the cubic spline:
$$S_3(x) = 0.0056(x-x_2)^3 + 0.3720(x-x_2)^2 + 28.9132(x-x_2)+362.78.$$
\section{videos}
\begin{itemize}
\item A video on linear interpolation using the direct method from the authors of \cite{KK09} \href{https://youtu.be/v7kapVuoWhY}{https://youtu.be/v7kapVuoWhY}
\item A video on quadratic interpolation using the direct method form the authors of \cite{KK09} \href{https://youtu.be/ifS8LL3qT2g}{https://youtu.be/ifS8LL3qT2g}
\item A video on cubic splines on YouTube \href{https://youtu.be/wMMjF7kXnWA}{https://youtu.be/wMMjF7kXnWA}
\end{itemize}
%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{8}{Numerical Differentiation}{Kyle Riley}{Riley} 
\section{Numerical Approximation of the Derivative}
There are several ways to derive the tools for numerically approximating the derivative based on discrete data. The Brin book approaches the problem based on the use of interpolation in Chapter 4 of \cite{LB16}. This approaches tightly aligns with the use of Lagrange interpolation polynomials, which is something we did not cover in class. A common approach is to use the definition of the derivative and then analyze convergence by making use of Taylor's Polynomial. This type of approach is introduced in \cite{KK09} in Chapter 2 as part of section 2. 
\subsection{Definition of the Derivative}
The definition of the derivative:
\begin{equation}
    f'(x) = \lim_{\Delta x\rightarrow 0}\frac {f(x+\Delta x)-f(x)}{\Delta x}
    \label{e:def_derivative}
\end{equation}
provides a natural way to estimate the derivative from a data set. Consider the data set in Table \ref{T:ice_CO2} that lists the concentration of CO2 in an ice core relative to depth. 
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c||c|c|}
\hline
depth (m) & 81.22 & 86.8 & 90.77 & 95.17 & 98.8 & 101.8 & 105.25 & 107.2 & 108.8\\
\hline
CO2 (ppmv) & 312.7 & 307.9 & 306.6 & 305.5 & 301.6 & 300.5 & 299.2  & 296.9 & 294.8 \\
\hline
\end{tabular}

    \caption{Concentration of CO2 in ice core relative to ice depth from (Carbon Dioxide Information Analysis Center, CDIAC}
    \end{center}
    \label{T:ice_CO2}
\end{table}
If there is an interest in approximating the rate of change of concentration relative to depth, for the depth of 98.8 meters then we can use (\ref{e:def_derivative}) to use the estimate of:
$$f'(98.8) \approx \frac {f(101.8)-f(98.8)}{101.8-98.8}=\frac{300.5-301.6}{101.8-98.8}\approx -0.3667. $$

The use of the approximation 
\begin{equation}
    f'(x) \approx \frac {f(x+\Delta x)-f(x)}{\Delta x}
    \label{e:def_forward_difference}
\end{equation}
is known as the {\bf forward difference} approximation for the first derivative. An alternative to the forward difference approximation is the {\bf backward difference}, which is given by
\begin{equation}
    f'(x) \approx \frac {f(x)-f(x-\Delta x)}{\Delta x}.
    \label{e:def_backward_difference}
\end{equation}
The use of the backward difference to our problem in Table \ref{T:ice_CO2} turns into:
$$f'(98.8) \approx \frac {f(98.8)-f(95.17)}{98.8-95.17}=\frac{301.6-305.5}{98.8-95.17}\approx -1.0744 . $$
Thus, the estimate of the derivative can directly use the data and in the case of forward, or backward, difference the estimate is the slope of the secant line that connects the data points. To derive the order of convergence of this method of estimation involves the use of the Taylor's Polynomial Theorem. 

Recall that Taylor's Polynomial Theorem has the general form:
$$f(x) = f(x_0) + \sum_{j=1}^n\biggr( \frac {f^{(j)}(x_0)}{j!}(x-x_0)^j\biggr) + \frac {f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1} .$$
If we adopt the convention that $\ds x = x_0+\Delta x$ then the second order Taylor's expansion will take the form:
$$f(x+\Delta x) = f(x_0) + f'(x_0)(\Delta x) + \frac {f''(\xi)}2(\Delta x)^2$$
The magic of algebra results in:
$$\frac {f(x+\Delta x)-f(x_0)}{\Delta x} - f'(x_0)  = \frac {f''(\xi)}2(\Delta x),$$
and careful consideration reveals that the difference between the forward difference approximation and the exact value of $f'(x_0)$ is equal to a constant value times $\Delta x$. Thus, the difference between the approximation and the exact value is bounded by $\ds \Delta x$ to the first power as $\ds \Delta x \rightarrow 0$, which means this approximation is first order. A similar argument can prove that the backward difference approximation has first order convergence. 

It follows that forward difference and backward difference has the same order of convergence. We will introduce methods with higher order of convergence a bit later, but a brief reflection on the approximations generated for $f'(98.8)$ from the data in Table \ref{T:ice_CO2}. Recall that the forward difference results in $f'(98.8)\approx -0.3667$ while the backward difference resulted in $f'(98.8)\approx -1.0744$. If the data points are equally spaced then we would consider the accuracy of forward difference versus backward difference to be the same, however the data spacing is not equal in this case. The $\Delta x$ for forward difference is 3 units, while the $\ds \Delta x$ for backward difference is 3.63 units. Since the definition of the derivative in (\ref{e:def_derivative}) is in regards to the limit as $\Delta x \rightarrow 0$ then this means the data points that are closer together would be considered more accurate since $\Delta x$ is smaller for that selection. It can be concluded that we have a preference for the forward difference approximation in the specific case of the data in Table \ref{T:ice_CO2} for the derivative approximation at a depth of 98.8 meters since the spacing is smaller for the forward difference approximation. 

\subsection{Higher Order Approximations of the First Derivative}
First order approximations are generally easier to compute, but also converge slowly. It is possible to use higher order methods that converge more quickly. A second order approximation of the first derivative is {\bf centered difference} approximation:
\begin{equation}
    f'(x) \approx \frac {f(x+\Delta x)-f(x-\Delta x)}{2 \Delta x} .
    \label{e:centered_diff}
\end{equation}
A proof of that centered difference is second order can be found in the following:
the third order Taylor Polynomial expansion can generate the following
$$f(x + \Delta x) = f(x) + f'(x) \Delta x + f''(x) \frac {\Delta x^2}2 + f'''(c_1) \frac {\Delta x^3}{3!}$$
and
$$f(x - \Delta x) = f(x) - f'(x) \Delta x + f''(x) \frac {\Delta x^2}2 - f'''(c_2) \frac {\Delta x^3}{3!}.$$
It follows that subtracting the two expressions results in:
\begin{align*}
 f(x + \Delta x)- f(x - \Delta x) &= 2f'(x)\Delta x + (f'''(c_1)+f'''(c_2)) \frac {\Delta x^3}{3!}  \cr
  f(x + \Delta x)- f(x - \Delta x) - 2f'(x)\Delta x &=   (f'''(c_1)+f'''(c_2)) \frac {\Delta x^3}{3!} \cr
\frac {f(x + \Delta x)- f(x - \Delta x)}{2 \Delta x} - f'(x) &=   (f'''(c_1)+f'''(c_2)) \frac {\Delta x^2}{12}
\end{align*}
Thus, if we let $M = \frac {f'''(c_1)+f'''(c_2)}{12}$ then we have the result that the error between $\ds \frac {f(x + \Delta x)- f(x - \Delta x)}{2 \Delta x}$ and $f'(x)$ is bounded by some constant times $\Delta x^2$ (making it second order convergent). We define the {\bf centered difference} approximation as:
\begin{equation}
f'(x) \approx \frac {f(x + \Delta x)- f(x - \Delta x)}{2 \Delta x}.    
\label{e:centered_diff_fp}
\end{equation}
It should be noted that (\ref{e:centered_diff_fp}) is only when the {\it spacing is uniform}. Similar work can be employed to develop methods of even higher order of convergence. The very end of section 4.3 in \cite{LB16} catalogs several formulas that can be used to estimate the derivative. A couple of second order methods for $\ds f'(x)$ include:

Second order Forward Difference of $\ds f'(x)$
$$f'(x) \approx \frac {-3f(x) + 4f(x + \Delta x) - f(x+2 \Delta x)}{2\Delta x}.$$

Second order Backward Difference approximation of $\ds f'(x)$
$$f'(x) \approx \frac {f(x-2\Delta x) - 4f(x - \Delta x) + 3 f(x)}{2\Delta x}.$$

Methods of even higher order convergence for estimating the first derivative exist and Table 4.2 at the end of Section 4.3  in \cite{LB16} includes fourth order methods. Our class will primarily focus on the first and second order methods. 

\section{Approximating Higher Order Derivatives}
Taylor's Polynomial Theorem can also inspire methods to approximate higher order derivatives. In this case, we use the fourth order Taylor Polynomial expansions that are similar to the ones we used in the last section: 
$$f(x + \Delta x) = f(x) + f'(x) \Delta x + f''(x) \frac {\Delta x^2}2 + f'''(x) \frac {\Delta x^3}{3!} + f^{(4)}(c_1) \frac {\Delta^4}{4!}$$
and
$$f(x - \Delta x) = f(x) - f'(x) \Delta x + f''(x) \frac {\Delta x^2}2 - f'''(x) \frac {\Delta x^3}{3!} + f^{(4)}(c_2) \frac {\Delta^4}{4!}.$$
It follows that adding the two expressions results in:
\begin{align*}
 f(x + \Delta x) + f(x - \Delta x) &= 2f(x) + f''(x) \Delta x^2 +  (f^{(4)}(c_1)+f^{(4)}(c_2)) \frac {\Delta x^4}{4!}\cr
  f(x + \Delta x) - 2f(x)+ f(x - \Delta x) - f''(x)\Delta x^2 &=  (f^{(4)}(c_1)+f^{(4)}(c_2)) \frac {\Delta x^4}{4!}  \cr
 \frac {f(x + \Delta x) - 2f(x)+ f(x - \Delta x)}{\Delta x^2} - f''(x) &=   (f^{(4)}(c_1)+f^{(4)}(c_2)) \frac {\Delta x^2}{4!}.
\end{align*}
Thus, it follows that $\ds \frac {f(x + \Delta x) - 2f(x)+ f(x - \Delta x)}{\Delta x^2} $ is a second order approximation for $\ds f''(x)$. 

One should note that $\ds \frac {f(x + \Delta x) - 2f(x)+ f(x - \Delta x)}{\Delta x^2} $ is the {\bf centered difference} approximation for $\ds f''(x)$ and it is also true that there are forward and backward difference approximations for the second derivative. 

The {\bf forward difference} approximation is given by
$\ds f''(x) \approx \frac {f(x) - 2f(x  + \Delta x)+ f(x + 2 \Delta x)}{\Delta x^2} $ 

The {\bf backward difference} approximation is given by
$\ds f''(x) \approx \frac {f(x) - 2f(x  - \Delta x)+ f(x - 2 \Delta x)}{\Delta x^2} $ 

Table 4.4 in \cite{LB16} provides approximations to the third order derivatives and there are a variety of sources to investigate finite difference approximations for higher order derivatives. In many cases, Taylor's Polynomial Theorem can be used to generate formulas for these finite difference approximations. 

\section{A note when spacing is uneven between points}
So far, all the techniques covered in this lecture relate to methods that specialize in evenly spaced data points (evenly spacing in this case means $\ds \Delta x = x_k-x_{k-1}$ is the same for all k in the data set. Evenly spaced nodes can happen when there is significant control on data collection, but this is usually not the case for field data and numerous other situations that lack strict controls on data generation. There are several other techniques that can be utilized to approximate the derivative for data points that have uneven spacing with Newton's Divide Difference being one that we mentioned in class. However, the most popular approaches are based on interpolating the data and then taking derivatives of the interpolating function. 

Using an interpolation function on a data set to approximate a derivative is conceptually easy to envision since interpolation functions are a fundamental principle to most of the numerical methods in use. However, there is the issue that high degree interpolation polynomials tend to be more oscillatory and more prone to have dramatic shifts in the underlying derivatives between the data values. It is prudent to employ interpolation functions that are relatively low in degree. Moreover, estimating higher order derivatives with an interpolation function would be even more problematic. This is another case where the use of a cubic spline might be a prudent selection, but this would only be relevant for the estimate of relatively low order derivatives. 

\section{videos}
A few videos to consider
\begin{itemize}

\item Video on forward difference part 1 from the author of KK09 \href{https://youtu.be/KNFjPIqJAJ4}{https://youtu.be/KNFjPIqJAJ4}
\item Video on forward difference part 2 from the author of KK09 \href{https://youtu.be/1EV0fKPiQMk}{https://youtu.be/1EV0fKPiQMk}
\item Video on approximating higher order derivatives from author of KK09 \href{https://youtu.be/6wWOv6pZm0k}{https://youtu.be/6wWOv6pZm0k}
\item Video that is more theoretical based on Taylor's Polynomial from the postcard professor on YouTube \href{https://youtu.be/zM2wim4JZd0}{https://youtu.be/zM2wim4JZd0}
\end{itemize}

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{9}{Numerical Integration}{Kyle Riley}{Riley}


An easy application that relates to integration can be found in the horizontal velocity of a drone given in Table  \ref{t:drone_data}. If velocity was constant then difference in time multiplied times the velocity would yield the distance traveled. For a velocity function that changes with time this calculation will require the tools of calculus. 
\begin{table} [h]
\begin{center}
\begin{tabular}{|l|c|}
\hline
t in secs & velocity (m/s) \\
\hline
0 & 0 \\
\hline
10 & 3.5 \\
\hline
20 & 5.8 \\
\hline
30 & 7.2 \\
\hline
40 & 8.5 \\
\hline
50 & 9.6 \\
\hline
60 & 10.2 \\
\hline
\end{tabular} 
\end{center}
\caption{Horizontal straight line velocity for a drone}
\label{t:drone_data} 
\end{table}
As we know from first semester calculus, the integral of velocity would provide distance traveled and it provides a simple example of the need to calculate the integral from functional data. 

The place to start with numerical integration is the definition of the integral. As you may recall, the definition of the integral has many more details than what is needed to introduce the derivative. A brief synopsis or this long narrative would include the fact that $\ds \int_a^b f(x) \ dx$ is the area under curve and above the y-axis. The derivation of the Riemann Integral involves dividing up the interval, $\ds [a,b]$ into smaller and smaller pieces and then using an approximation of the area for each slice with the ultimate goal of adding up all these slices into an approximation of the total area. Most equations would give you the definition that is given in (\ref{e:definition_integral}).
\begin{equation}
    \int_a^b f(x) \ dx = \lim_{\Vert \Delta x \Vert \rightarrow 0} \sum_{k=0}^n f(c_k) \Delta x_k
    \label{e:definition_integral}
\end{equation}
A few details to include is that $\ds \Vert \Delta x \Vert $ represents the largest subinterval (also known as the largest partition) and the goal is to slice the interval in a manner that reduces the size of each subinterval. The sum in (\ref{e:definition_integral}) represents adding up the area in each slice and all of this is driven by a limit. Our course we will not be concerned with deriving and recounting the exact definition of the integral, but it is expected that a good student of numerical analysis will recognize that the numerical methods used to approximate the integral connect to the calculus definition. 

\section{Left Hand and Right Hand Rule}
A simple approach to approximating the integral is the left hand rule, or he right hand rule. This section will describe the left hand rule with the acknowledgement that the right hand rule is analogous. We start with the given interval [a,b] and move to divide the interval into $n$ subintervals. By definition, the step size for each subinterval is $\ds \Delta x = \frac {b-a}n$ and we define $\ds x_k$ in the following manner: $\ds x_0 = a$, $\ds x_1 = x_0 + \Delta x$, $\ds x_2 = x_0 + 2 \Delta x$, ... $\ds x_k = x_0 + k \Delta x$, and $\ds x_n = b$. Over each subinterval a rectangle is made to approximate the area under the curve and with the Left Hand Rule it is the functional value on the left hand side of each subinterval that is selected to form the height of each rectangle, as demonstrated in Figure \ref{f:LeftHandRule}. Thus, the total area underneath the curve is approximated by the sum total of all the rectangles used to approximate the area for each subinterval, i.e. 
$$\int_a^b f(x) \ dx \approx \sum_{k=1}^n f(x_{k-1}) \ \Delta x.$$

\begin{figure}[ht]
\centering
\includegraphics[height=70mm]{LeftHandRule.png}
 \caption{Picture of the Left Hand Rule from http://tutorial.math.lamar.edu/Classes/CalcI/AreaProblem.aspx}
 \label{f:LeftHandRule}
\end{figure}
The approximation becomes more accurate as more subintervals are introduced and the step size of $\ds \Delta x$ shrinks to zero, which relates to the definition supplied in (\ref{e:definition_integral}). The Right Hand Rule is an analogous method where the functional value of the right hand end point is selected.
\subsection{Example of Left Hand Rule}
To illustrate the numerical work of Left Hand Rule, consider approximating the value of $\ds \int_0^1 e^x \ dx$ using a step size of $\Delta x = 1/4 = 0.25$. It follows that:
$$x_0 = 0, \ x_1 = 0.25, \ x_2 = 0.5, \ x_3=0.75, \ {\rm and} \ x_4=1.$$
The Left Hand Rule calculation comes from:
$${\rm Left \ Hand \ Rule}= f(0)\Delta x + f(0.25) \Delta x + f(0.5) \Delta x + f(0.75) \Delta x = \sum_{k=1}^4 f(x_{k-1})\Delta x.$$
It follows that the approximation of Left Hand Rule results in approximately 1.5124 and that approximation does not compare favorably with the more accurate value of 1.7183. It turns out the absolute relative error is approximately 0.1198. 

It follows that Left Hand Rule and Right Hand Rule are first order methods, which means the associated approximations converge relatively slowly and require a small step size to obtain good accuracy. The rest of our discussion will focus on methods that have a higher order or convergence. 

\section{Midpoint and Trapezoidal Rules (second order methods)}
There are some fairly simple methods that can be employed that translate into second order methods to approximate an integral. The {\bf midpoint method} is simply picking the midpoint of every interval in order to approximate the area under the curve. In this case, the formula for this method takes the form:
$$\int_a^b f(x) \ dx \approx \sum_{k=1}^n f(c_{k}) \ \Delta x,$$
where $\ds c_k = \frac {x_k+x_{k-1}}2$ for $k=1...n$, which means the height of the rectangle for each subinterval is given by the functional value at each midpoint. Another second order method is the {\bf trapezoidal method} which is covered in \cite{KK09} in Chapter 7 section 2. The trapezoidal method takes the form:
$$\int_a^b f(x) \ dx \approx \frac {\Delta x}2 \left (f(a) + 2\sum_{k=1}^n f(x_k) + f(b) \right),$$
the derivation of this formula is using the formula for the area of a trapezoid for each subinterval. Figure \ref{f:TrapRule} shows an image of trapezoidal rule over just one interval and this illustrates the connection to interpolation. 
\begin{figure}[ht]
\centering
\includegraphics[height=70mm]{trap_from_KK09.png}
 \caption{Picture of trapezoidal rule from Figure 2 in KK09 7.2}
 \label{f:TrapRule}
\end{figure}
Trapezoidal rule is often referred as linear interpolation since the area being calculated is from a linear connection between the interval endpoints to approximate the underlying function. Many of the other numerical methods used to approximate integration can be tied to various forms of interpolation. 

A summary of the books and their coverage related to integration:
\begin{itemize}
    \item \cite{KK09}
    \begin{itemize}
        \item Trapezoidal Rule, Chapter 7 section 2
        \item Simpson's 1/3 Rule, Chapter 7 section 3
        \item Gaussian Quadrature, Chapter 7 section 5
    \end{itemize}
    \item \cite{LB16}
    \begin{itemize}
        \item Introduction on integration in 4.1
        \item Composite and Adaptive integration 4.4
        \item Guassain Quadrature, last part of 4.3
        \item Table of formulas in Table 4.5 that includes Simpson's 1/3 rule and Simpson's 3/8 rule
    \end{itemize}
\end{itemize}
\section{Simpson's 1/3 Rule and Simpson's 3/8 Rule}
Higher order methods traditionally come from using higher order interpolations techniques. For example, Simpson's 1/3 Rule can be derived from using a quadratic polynomial to interpolate three data points that are evenly spaced and then integrate the polynomial. The {\bf Simpson's 1/3 Rule} has the form:
$$\int_a^b f(x) \ dx \approx \frac {\Delta x}3 \left (f(a) + 4f\left ( \frac {a+b}2\right ) + f(b) \right ) ; \ \ {\rm where} \ \Delta x = \frac {b-a}2.$$

Simpson's 3/8 rule is derived from taking a cubic polynomial to interpolate four evenly spaced data points and then integrating the polynomial to generate the given formula. 
The {\bf Simpson's 3/8 Rule} has $\ds \Delta x=\frac {b-a}3$, $\ds x_0 =a$, $\ds x_k=x_0 + k\Delta x$ for $k=1 ...n-1$, while $\ds x_n=b$. The formula for the 3/8 rule is given by:
$$ \int_a^b f(x) \ dx \approx \frac {3 \Delta x}8 \left (f(x_0) + 3 f(x_1) + 3 f(x_2) + f(x_3)  \right ) $$
{\bf Note:} {\color{teal} It is important to note that the 1/3 rule uses an 3 data points to form an approximation while the 3/8 rule uses 4 data points. This pattern will become an important distinction when we consider composite integration}

\section{Composite Integration}
At this point, the way to improve the accuracy of our approximations for integration would involve shrinking the size of the subintervals and increasing the number of subintervals. Applying an integration method over more and more subintervals and then adding up the results is identified as composite integration. 
\subsection{Application of Composite Integration}
The heat capacity under constant pressure is equivalent to the partial derivative of enthalpy with respect to temperature under fixed pressure. Consider the data for a gas with the following data: \par 
\begin{tabular}{|l|c|}
\hline
Temperature & Heat Capacity\\
$^oC$ & $\frac J{{\rm gmol}\ ^oC} $\\
\hline
100 & 31.13 \\
\hline
200 & 33.18 \\
\hline
300 & 41.16 \\
\hline
400 & 43.18 \\
\hline
500 & 45.06 \\
\hline
600 & 47.15 \\
\hline
700 & 49.03 \\
\hline
\end{tabular} \par \noindent
It follows that the integral of heat capacity, $C_p$ over the table is the total change in enthalpy over the entire range of temperature. There is not a given function for heat capacity with respect to temperature, which leaves the tools of numerical integration in order to address this problem. Therefore, we have the goal of approximating
$$\int_{100}^{700} C_p \ dT$$
The example of trapezoidal rule is where the formula of trapezoidal rule is used for every subinterval and the results totaled up
$$\int_{100}^{700} C_p \ dT = \frac {100}2 ( 31.13 + 49.03) + 100(33.18+41.16+43.18+45.06+47.15) \approx 24981 \frac J{{\rm gmol}}$$
Simpson's 1/3 Rule results in the rule being applied for every two consecutive subintervals, i.e.
$$\int_{100}^{700} C_p \ dT = \frac {100}3 ( 31.13  + 4(33.18)+41.16+41.16+4(43.18)+45.06+45.06 +4(47.15)+49.03), $$
which can be condensed by combining the overlapping nodes between adjacent subintervals, which results in 
$$\int_{100}^{700} C_p \ dT = \frac {100}3 ( 31.13  + 4(33.18)+2(41.16)+4(43.18)+2(45.06) +4(47.15)+49.03) \approx 24888 \frac J{{\rm gmol}}.$$
Lastly, Simpson's 3/8 rule can also be applied to this data set
$$\int_{100}^{700} C_p \ dT = \frac {3(100)}8 ( 31.13  + 3(33.18)+3(41.16)+2(43.18)+3(45.06) +3(47.15)+49.03) \approx 24981 \frac J{{\rm gmol}}.$$
It is evident that the number of data points can influence the possible fixed point methods that can be used. For example, a data set with four data points is not compatible with the formula used to compute the 1/3 rule, but fits the structure of the 3/8 rule perfectly. We will use the homework to analyze this pattern further. 
\section{Convergence for Integration Methods and Composite Integration Methods}
The error terms for integration methods can be generated by taking the error terms of the interpolation function and then integrating the expression to generate the error expression for approximating the error. Table 4.5 in \cite{LB16} shows the formula and the error term for each of the given integration methods we have discussed. Trapezoidal Rule 

\begin{equation}
\int_a^b f(x) \ dx = \frac {\Delta x}2 \left( f(x_0) + f(x_1) \right) + {\cal O}(\Delta x^3 f'''(\xi))
\label{e:trap_error}
\end{equation}

Simpson's 1/3 Rule
$$\int_a^b f(x) \ dx = \frac {\Delta x}3 \left( f(x_0) + 4 f(x_1) + f(x_2) \right) + {\cal O}(\Delta x^5 f^{(4)}(\xi))$$

and Simpson's 3/8 Rule
$$\int_a^b f(x) \ dx = \frac {3\Delta x}8 \left( f(x_0) + 3 f(x_1) + 3 f(x_2) +f(x_3) \right) + {\cal O}(\Delta x^5 f^{(4)}(\xi))$$

As one might recall, Trapezoidal Rule was identified as a second order method yet the error term in (\ref{e:trap_error}) would seem to indicate that the method is third order. The second order designation applies to the composite version of Trapezoidal Rule, where $\ds \Delta x = \frac {b-a}n$ the third order bound in (\ref{e:trap_error}) applies to each subinterval, but to compute the final approximation requires the sum across each of the n subintervals. Hence, the order of {\it composite} Trapezoidal that is applied $n$ times over n subintervals is $\ds n {\cal O}(\Delta x^3)$, which turns into 
$$\int_a^b f(x) \ dx \le n M\Delta x^3 = n M \biggr (\frac {b-a}n\biggr )^3 =  M \frac {(b-a)^3}{n^2} = (b-a)M \frac {(b-a)^2}{n^2} = {\cal O}(\Delta x^2).$$ 
The same translation occurs for the other methods with both Simpson methods being $\ds {\cal O}(\Delta x^4)$ when applied as a composite method. 

\section{Gaussian Quadrature}
The methods discussed in the prior sections all have the form
$$\int_a^b f(x) \ dx \approx \sum w_k f(x_k),$$
where $\ds w_k$ are some specified constant and the nodes $\ds x_k$ are evenly spaced over the interval. Gaussian Quadrature has a very different approach where both the weights and the nodes are specified via the method and the nodes are not evenly spaced. The big difference with Gaussian Quadrature is that it requires a way to make functional evaluations at specified values of $\ds x_k$, which means this method cannot be directly applied to functions corresponding to just a fixed set of data. 

The basic idea behind Gaussian Quadrature is to generate integration formulas that are exact for polynomials of a certain degree and also all polynomials of lower degree, which turn out can be generated by a particular process. The generated  weights ($\ds w_k$) and nodes ($\ds x_k$) are then used as an approximation for integration. An outline of the derivation of this method starts with finding a way to integrate all polynomials of degree n or lower exactly over the interval $[-1,1]$. 

A few simple examples of how these formulas are generated include the case of $n=1$ where the desire is to find a formula exact for all polynomials of degree 1 or less. Thus, we are looking for $\ds w_1$ and $x_1$ where
$$w_1 f(x_1) = \int_{-1}^1f(x) \ dx, $$
where $f(x)$ is any polynomial of degree 1 or less. Two examples of polynomials that fit the requirement are $f(x) =1$ and $f(x)=x$. The first polynomial generates
\begin{align*}
    w_1 &= \int_{-1}^1 1 \ dx=2 \\
    w_1 &= 2.
\end{align*}
The second function generates:
\begin{align*}
    w_1x_1 = \int_{-1}^1 x \ dx=0 \\
    x_1 &= 0.
\end{align*}
It follows that this method corresponds to the midpoint rule that was covered earlier in the lecture. For the case of $n=2$, the goal is to find weights $\ds w_1$ and $\ds w_2$ along with nodes $\ds x_1$ and $\ds x_2$ so that the formula of $w_1 f(x_1) + w_2 f(x_2)$ is exact for all polynomials of degree 3 or lower. To generate the associated values we consider the polynomials: $\ds f(x) = 1, x, x^2, {\rm and} \ x^3$. Following the same procedure generates a system of four equations and four unknowns:
\begin{align*}
w_1 + w_2 &= \int_{-1}^1 1 \ dx = 2 \\
w_1 x_1 + w_2 x_2 &= \int_{-1}^1 x \ dx = 0 \\
w_1 x_1^2 + w_2 x_2^2 &= \int_{-1}^1 x^2 \ dx = \frac 23 \\
w_1 x_1^3 + w_2 x_2^3 &= \int_{-1}^1 x^3 \ dx = 0 
\end{align*}
Solving the system results in $\ds w_1 = w_2 = 1$, $\ds x_1 = \frac {-\ \sqrt{3}}3$, and $\ds x_2 = \frac {\sqrt{3}}3$.

In practice, Gaussian Quadrature with precision n (exact for polynomial of degree lesss than $2n$) is given by 
$$\int_{-1}^1 f(x) \ dx = \sum_{k=1}^n w_k f(x_k),$$
where $\ds w_k$ are the weights found by solving the linear system described in this section and $\ds x_k$ are roots to the n$\ds ^{th}$ degree Legendre Polynomial. The Legendre Polynomials have a special property of being orthogonal over the interval $[-1,1]$ and this forces the location of their roots to be natural locations for Gaussian Quadrature. 

Lastly, Gaussian Quadrature can be extended to any interval via a coordinate transformation (u - substitution). If you let $\ds x = \frac {a+b}2 + \frac {(b-a)t}2$ then 
$$\int_a^b f(x) \ dx = \int_{-1}^1 f\left( \frac {a+b}2 + \frac 12(b-a)t \right) \frac {b-a}2 \ dt,$$
with $\ds -1\le t \le 1$. Gaussian Quadrature can provide good accuracy for a relatively low number of points, but it does require the ability to evaluate the function at the specified nodes and also the work to compute the corresponding weights. 

\subsection{Gaussian Quadrature Weights and Points}
A couple of additional weights and points for Gaussian Quadrature:

3 point: $\ds c_1 = 0.55555556$, $\ds x_1 = - \sqrt{3/5}$, $\ds c_2=8/9$, $\ds x_2=0$, $\ds c_3 = 0.5555556$, and $\ds x_3 = \sqrt{3/5}$

4 point: $\ds c_1 = \frac{18-\sqrt{30}}{36}$, $\ds x_1 = - \sqrt{\frac 37 + \frac 27 \sqrt{6/5}}$, $\ds c_2=\frac{18+\sqrt{30}}{36}$, $\ds x_2=  - \sqrt{\frac 37 - \frac 27 \sqrt{6/5}}$, $\ds c_3 = \frac{18+\sqrt{30}}{36} $, $\ds x_3 = \sqrt{\frac 37 - \frac 27 \sqrt{6/5}}$, $\ds c_4 = \frac{18-\sqrt{30}}{36}$, and $\ds x_4 = \sqrt{\frac 37 + \frac 27 \sqrt{6/5}}$.

\section{Adaptive Methods for Integration}
Adaptive integration is introduced in \cite{LB16} at the end of section 4.4. The major element to keep in mind is that adaptive methods are built for efficiency. Please see the last part of section 4.4 in \cite{LB16} for details. 

\section{videos}
There are plenty of videos to help you learn this material. 
\begin{itemize}
\item Kahn academy on trapezoidal rule \href{https://youtu.be/1p0NHR5w0Lc}{https://youtu.be/1p0NHR5w0Lc}
\item YouTube also has a ``Math Easy Solutions'' channel that does have a good overview of the left, right, and midpoint methods \href{https://youtu.be/zWzmaLvQU6w}{https://youtu.be/zWzmaLvQU6w}
\item The author of \cite{KK09} also has a video on an application of trapezoidal rule \href{https://youtu.be/Xo9RzN5OFBo}{https://youtu.be/Xo9RzN5OFBo}
\item How to apply trapezoidal and Simpson rules is demonstrated in this MIT open course lecture \href{https://youtu.be/l2SjUREZk0c}{https://youtu.be/l2SjUREZk0c}
\item If you want to learn how the formula for Simpson's Method is derived then an MIT open course lecture has this presentation (this will not be something you will be asked to generate in our class) \href{https://youtu.be/uc4xJsi99bk}{https://youtu.be/uc4xJsi99bk}
\end{itemize}
%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{10}{Review of Ordinary Differential Equations}{Kyle Riley}{Riley}

A prerequisite of this course is Differential Equations and so it is expected that every student has a strong mastery of the fundamentals regarding ordinary differential equations (ODES). The numerical methods we will discuss can be found in Chapter 6 of \cite{LB16} and Chapter 8 of \cite{KK09}. First, we should review a few things to help get oriented in the world that is ODES.

\section{Notation and Theory}
There are several conventions with notation that are used with ODES that should be mentioned. The first notation to confront is all the ways we signal a derivative in mathematical notation. For example, all of the following indicate taking the first derivative with respect to t:
$$\frac {dy}{dt} = y'(t) = \dot y(t).$$

Another vexing issue is the default independent variable. As you may recall the differential equation:
$$\frac {dy}{dt}=t^2+y^2,$$
has a {\it dependent} variable that is $y$ since y depends upon $t$ while the {\it independent} variable is $t$. Thus, in \cite{LB16} it is arranged that the default dependent variable is $y$ and the default independent variable is $t$. However, please note that \cite{KK09} defines things differently with the independent variable represented by $x$. Regardless of the variables in use, the understanding that any derivatives being taken are with respect to the independent variable. 

We will make frequent reference to our default initial value problem (IVP):
$$\frac {dy}{dt} = f(t,y); \ \ y(t_0)=y_0,$$
where $f$ is a function of t and y, $\ds t_0$ is the initial value of t, and $\ds y_0$ is the initial value of $y$.

\subsection{Existence and Uniqueness Theorem}
A common fixture to an ODE course is the existence and uniqueness theorem.
\theorem For a given initial value problem, $\ds \frac {dy}{dt} = f(t,y); \ \ y(t_0)=y_0,$ if $f$ and $\ds \frac {\partial f}{\partial y}$ are continuous in an open region that contains $\ds (t_0,y_0)$ then there is an interval that contains $\ds t_0$ such that there exists a unique solution to the initial value problem. \label{t:existence_uniqueness}

In using numerical methods, it is very reassuring to have existence and uniqueness since it is challenging to solve a problem that might not have a solution or to numerically build a solution for a problem that might have many solutions. Consider the problem: $\ds ty'=y; \ y(0)=0$, which has an infinite number of solutions. Another example to consider is $\ds ty'=2y; \ y(0)=1$, which does not have a solution.  

\section{Slope Fields}
It can be useful to have a working knowledge of slope fields and/or direction fields since these tools have close ties to Euler's Method and the Runge-Kutta Methods. The course will also expect high proficiency regarding the technique of separation of variables along with the analytical technique of integration factors. Lastly, it is useful to review the solutions to higher order linear systems and first order linear systems, but it is not expected that the student have strong mastery of the analytical techniques for solving these systems. 
\par \noindent
A valuable visualization tool that connects with much of the numerical methods covered in this class relates to the slope field. A slope field is a figure that represents the value of the function, $f(t,y)$ at each point and the little lines generated in the figure are tangent to the solutions of the underlying ODE. Figure \ref{f:slope_field} displays the slope field corresponding to $\ds \frac {dy}{dx} = x-y$ with initial value $y(-1)=0$. One can visualize that numerical methods utilize the slope information in a slope field to help build the approximations used in the numerical method. An easy visual check that can be made for any numerical approximation is to plot the solution on a slope field graph, it should be the case that the solution is tangent to each of the slopes in the slope field. Approximations that are clearly not tangent would likely not be good approximations to the corresponding solution to the IVP. 


\begin{figure}[!ht]
\centering
\includegraphics[height=80mm]{slope_field_geogebra.png}
 \caption{Figure of a slope field using an app from Geogebra \cite{GG19}}
 \label{f:slope_field}
\end{figure}

\section{Analytical Methods}

There are two analytical methods that you should review and master for this class. In this section, we will just work a couple of examples and a through review would involve reviewing the topic from your differential equations textbook. The first method is {\bf separation of variables}, which can be reviewed by solving the following
\begin{equation}
\frac {dy}{dt} = 2t(1+y^2), \ \  y(0)=1.
\label{e:sep_example}
\end{equation}
The solution to (\ref{e:sep_example}) involves separating the variables into the following:
$$\frac {dy}{1+y^2} = 2t \ dt$$
and following this manipulation by integrating both sides
$$\int \frac {dy}{1+y^2} = \int 2t \ dt.$$
The integration steps result in the following sequence 
\begin{eqnarray*}
\int \frac {dy}{1+y^2} &= \int 2t \ dt \\
\arctan (y) &= t^2 + C \\
\end{eqnarray*}
solving for y results in the general solution of : $\ds  y = \tan (t^2 + C)$. The solution to the initial value problem is produced when substituting the initial condition into the general solution and solving for the integration constant, C. Thus,  the initial condition of $y(0)=1$ for the general solution: $\ds  y = \tan (t^2 + C)$ has multiple solutions since $\ds C = \arctan(1)$ can result in a variety of answers due to the periodicity of tangent. The default value we would get for C is $\ds \frac {\pi}4$ and so the solution generated is $\ds y(t) = \tan (t^2+\frac {\pi}4)$. 

The second analytical technique that is important to have mastered is {\bf integration factors}, which is a technique used to solve first order linear ordinary differential equations. This technique can be derived and presented in several ways and in the end the only thing required is an ability to solve first order linear ordinary differential equations regardless of how the exact steps that are followed. The general form of a first order linear ordinary differential equation is:
\begin{equation}
\frac {dy}{dx} + p(x) \ y = q(x)
\label{e:firstorderlinearode}
\end{equation}
Any differential equation that can be written into the form of (\ref{e:firstorderlinearode}) is a first order linear ordinary differential equation. The integration factor technique is base on two steps. The first step is calculating the integration factor (we will label the factor $\mu$)
$$\mu = e^{\int p(x) \ dx},$$
and the second step is taking the integration factor and substituting it into the formula:
$$y = \frac 1{\mu} \int \mu q(x) \ dx.$$
An illustration of this technique can be provided with a  sample problem
\begin{equation}
\frac {dy}{dx} + 2y = 3e^{-x}, \ \ y(0) = 4.
\label{e:firstlinearode}
\end{equation}
Calculating the integration factor for (\ref{e:firstlinearode}) takes the form: $\ds \mu = e^{\int 2 \ dx} = e^{2x}$. The integration factor is then substituted into the formula for the general solution: $\ds y = \frac 1{\mu} \int \mu q(x) \ dx = \frac 1{e^{2x}} \int e^{2x}3e^{-x} \ dx$. It follows that:
\begin{eqnarray*}
y &= \frac 1 {e^{2x}} \int 3 e^x \ dx  \\
y &= \frac 1 {e^{2x}} (3 e^x +C) \\
y &= 3e^{-x} + C e^{-2x}.
 \end{eqnarray*}
 The initial value of $y(0)=4$ results in $\ds y(0) = 3+C = 4$ and that implies $C=1$. Therefore, the solution to (\ref{e:firstlinearode}) is found to be $\ds y(x) =  3e^{-x} + e^{-2x}$. Integration factors is a specific format for how to solve this type of problem and it is likely you may have been instructed in a different approach, which is completely acceptable. The main point is that you will be responsible for solving first order linear ordinary differential equations and if you have mastered another approach then you can use it, but if you do not recall the method you used in Math 321 then integration factors is an approach you can use. 
 
 \section{Modeling}
 It is also useful to remember the utility of differential equations. In section 6.1 of \cite{LB16} we can review the derivation of the model for the nonlinear pendulum model. The Brin book does a good job of reviewing from first principles what connects the formal study of differential equations with physical models. The pendulum model might have been something you covered in Differential Equations, or it might not have been covered in your class, but it is still valuable to have an understanding of how differential equations connects with physical models. 
 
 \section{videos}
 A few extra resource videos to consider include:
\begin{itemize}
\item Separable Differential Equations video on Kahn Academy \href{https://youtu.be/DL-ozRGDlkY}{https://youtu.be/DL-ozRGDlkY}
\item A more complicate version of separable differential equation from an older version of Kahn Academy \href{https://youtu.be/C5-lz0hcqsE}{https://youtu.be/C5-lz0hcqsE}
\item Integration Factors covered by the Organic Chemistry Tutor on YouTube \href{https://youtu.be/gd1FYn86P0c}{https://youtu.be/gd1FYn86P0c}
\end{itemize}
 
%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{11}{Numerically Solving Ordinary Differential Equations}{Kyle Riley}{Riley}

Most ordinary differential equations classes cover an introduction of numerical methods, but coverage can greatly vary. We will start this subject from the simple methods and work our way to the more advanced methods. We return to the generic initial value problem is 
$$\frac {dy}{dt} = f(t,y); \ \ y(t_0)=y_0.$$
Consider the first order Taylor's Polynomial expansion of the solution to the IVP:
\begin{equation}
y(t+\Delta t) = y(t) + y'(t) \ \Delta t + y''(c) \frac {\Delta t^2}2,
\label{e:firststepEuler}
\end{equation}
recall that $\ds y'=f(t,y)$ and this can be substituted into the Taylor's Polynomial expansion
$$y(t+\Delta t) = y(t) + \Delta t \ f(t,y) + y''(c) \frac {\Delta t^2}2.$$
This motivates the development of {\bf Euler's Method}
\begin{equation}
    y_{k+1} = y_k + \Delta t \ f(t_k,y_k). 
    \label{e:Eulers_Method}
\end{equation}
Consider the example:
$$\frac {dy}{dt} = t-y; \ \ y(0)=4,$$
this means that $t_0 = 0$ and $\ds y_0 = 4$. If we decide to use $\ds \Delta t= 0.1$ and the formula in (\ref{e:Eulers_Method}) then this results in the following approximations:
\begin{align*}
    y_1 &= y_0 + \Delta t \ f(t_0,y_0) \\
    y_1 &= 4 + 0.1 \ (t_0-y_0) = 4-0.1*4 = 3.6\\
    y_2 &= y_1 + \Delta t \ f(t_1,y_1) \\
    y_2 &= 3.6 + 0.1 \ (0.1-3.6) = 3.6-0.1*3.5 = 3.25\\
    y_3 &= y_2 + \Delta t \ f(t_2,y_2) \\
    y_3 &= 3.25 + 0.1 \ (0.2-3.25) = 2.945\\    
\end{align*}
this process continues for as many steps as is needed. The derivation of Euler's Method can be extended to higher order methods, which can be illustrated via a second order Taylor's Polynomial expansion for $y(t)$:
$$y(t+\Delta t) = y(t) + \Delta t \ f(t,y) + y''(t) \frac {\Delta t^2}2 + y'''(c) \frac {\Delta t^3}{3!}.$$
The expansion of $\ds y''(t)$ in the Taylor's Polynomial expansion becomes $\ds \frac {d}{dt}f(t,y)$, which requires the subtle use of the chain rule. Keep in mind that $y$ is really a function of $t$ and so the function $\ds f(t,y) = f(t,y(t))$ is also really a function of $t$. The proper application of the chain rule results in:
$$\frac {d}{dt}f(t,y) = f_t(t,y) \ \frac {dt}{dt} + f_y(t,y) \ \frac {dy}{dt}, $$
but we have the fact that $\ds \frac {dy}{dt} = f(t,y)$. Thus, the expansion of the term results in:
$$\frac {d}{dt}f(t,y) = f_t(t,y) \ \frac {dt}{dt} + f_y(t,y) \ \frac {dy}{dt}= f_t(t,y) + f_y(t,y) \ f(t,y). $$
Consider the example, $\ds \frac {dy}{dt} = t^2-y^2$, then this case has $\ds f(t,y) = t^2-y^2$ and the derivative of $f$ with respect to $t$ is equal to
$$\frac {d}{dt}f(t,y) =  f_t(t,y) + f_y(t,y) \ f(t,y) = 2t + (-2y)(t^2-y^2). $$
It follows that any higher order methods using Taylor's Polynomial expansion is possible with continued application of the chain rule to take the higher order derivatives. 

\section{Measuring Error and Order of Convergence of a Numerical Method for an Initial Value Problem}
There are two types of error that we are going to discuss in this section with the first being the most natural. {\bf Global Error} is the absolute error that results from the approximation compared to the exact answer. Thus, the generic initial value problem, $\ds \frac {dy}{dt} = f(t,y)$; $\ds y(t_0) = y_0$ will have an exact answer and suppose we are interested in the solution at some specified t value, which we will call $T$. The exact answer would be denoted as $\ds y(T)$, but using a numerical method the approximation will require multiple steps to go from the initial t value, $\ds t_0$, to reach $T$. If we denote, $n$, as the number of steps to go from $\ds t_0$ to $T$ then $\ds y_n$ can represent the approximation of the numerical method at the n$^{th}$ step. Hence, the {\it global error} is represented by
$${\rm global \ error} = \vert y(T)-y_n\vert .$$
It follows that the global error is the absolute value of the actual error between the numerical approximation and the exact solution. The global error is of primary interest when applying a numerical method to solve a problem, however there are several items that can influence the global error that can include: order of the method, machine round off, the range between $\ds t_0$ and $T$, the number of steps, and the size of each of the steps can all influence the size of the global error. Judging a numerical method on just the global error is a narrow focus and even a poor method can produce good results if the step size is small enough. The {\bf Local Truncation Error} (LTE) is a different approach in order to assess the performance of a numerical method. The local truncation error is defined as:
$${\rm LTE} = \frac {\vert y(t_1) - y_1\vert}{\Delta t},$$
where $\ds y(t_1)$ is the exact answer after the first step, $\ds y_1$ is the approximation after the first step, and $\ds \Delta t$ is the step size. The LTE has two major differences compared to global error with one being the error is measured only after the first step of the method and the LTE is a measure {\it relative} to the step size. Making the LTE relative to the step size builds in a way to compare methods with the influence of the step size being divided out of the expression. A poor method that requires a small step size to perform will have that measure reflected in their LTE since division by a small number will increase the LTE.  
\par \noindent
When we discuss the order of convergence for numerical methods to solve an IVP then we will be using the LTE as the measure. For example, Euler's Method is a first order method, which starts from the introduction we give in (\ref{e:firststepEuler}) and can be proven by the following:
\begin{align*}
    y(t+\Delta t) &= y(t) + \Delta t \ f(t,y) + y''(c) \frac {\Delta t^2}2 \cr
    y(t+\Delta t) -\biggr [ y(t) + \Delta t \ f(t,y) \biggr ]&= y''(c) \frac {\Delta t^2}2  \cr
    y(t+\Delta t) - y_1 &= y''(c) \frac {\Delta t^2}2 \cr
    \frac {y(t+\Delta t) - y_1}{\Delta t} &= \frac {y''(c)}2 \Delta t \cr
    \frac {\vert y(t_1) - y_1 \vert }{\Delta t} & \le M \Delta t.
\end{align*}

{\bf Note:} {\color{teal} All the orders stated in the notes and the text for numerical methods for solving an IVP will be in terms of LTE.}

\section{Higher Order Methods and Runge-Kutta}
Section 6.3 of \cite{LB16} communicates the fundamentals behind the Runge-Kutta Method. The backdrop of the Runge-Kutta method is grounded in the differential equation:
$$\frac {dy}{dt} = f(t,y).$$
If one were to simply integrate both sides then the following would emerge:
\begin{align}
    \frac {dy}{dt} &= f(t,y) \cr
    \int_{t_0}^{t_1} \frac {dy}{dt} \ dt&=  \int_{t_0}^{t_1}f(t,y) \ dt \cr
    y(t_1) - y(t_0) &=  \int_{t_0}^{t_1}f(t,y) \ dt \cr
    y(t_1) &=  y(t_0) + \int_{t_0}^{t_1}f(t,y) \ dt. 
    \label{e:RK_origins}
\end{align}
Thus, the crucial issue to address is calculating the integral $\ds \int_{t_0}^{t_1}f(t,y) \ dt$ and if you used the trapezoidal method to estimate this integral then we would get:
$$\int_{t_0}^{t_1}f(t,y) \ dt = \frac {\Delta t}2 (f(t_0,y_0) + f(t_1,y_0+\Delta t \ f(t_0,y_0)).$$
We will see that $\ds y_1 = y_0 + \frac {\Delta t}2 (f(t_0,y_0) + f(t_1,y_0+\Delta t \ f(t_0,y_0)) $ is the same as the Modified Euler's Method that is given in the next section. All the other Runge-Kutta methods are base on ways to estimate $\ds \int_{t_0}^{t_1}f(t,y) \ dt$.

\subsection{Modified Euler's Method (second order Runge-Kutta)}
Modified Euler's Method (also known as the Trapezoidal Method for solving an IVP) is second order and can be related to the second order Runge-Kutta method. Essentially, the modified Euler method is the inclusion of an average between the slope at the local point and the slope that can be found with the regular Euler's step. The average of these two slopes should include more information about the differential equation and should be more accurate.  
\begin{align*}
    y_{k+1} &= y_k + \frac {\Delta t}2(m_1+m_2) \cr
    m_1 &= f(t_k,y_k) \cr
    m_2 &= f(t_k + \Delta t,y_k+\Delta t\  m_1)
\end{align*}
It turns out that Modified Euler is second order (in terms of LTE) and so it has a faster rate of convergence than plain Euler's method. As you may notice, higher order always comes with a price and in the case of Runge-Kutta methods the higher price often includes more functional evaluations from the slope field. 


\subsection{Third Order Runge-Kutta}
The \cite{LB16} text does present, in section 6.3, the discussion that third order Runge-Kutta comes from using 1/3 Simpson's Rule in order to integrate the integral in \ref{e:RK_origins}. The formula for third order Runge-Kutta (RK-3): 
\begin{align*}
    y_{k+1} &= y_k + \frac {\Delta t}6(m_1+ 4m_2 +m_3) \cr
    m_1 &= f(t_k,y_k) \cr
    m_2 &= f(t_k + \frac {\Delta t}2,y_k+\frac {\Delta t}2  m_1) \cr
    m_3 &= f(t_k + \Delta t,y_k- {\Delta t}m_1 + 2m_2 \Delta t)
\end{align*}

\subsection{Fourth Order Runge-Kutta}
Close inspection reveals that fourth order Runge-Kutta (RK-4) is the result of a numerical integration approximation for the integral in (\ref{e:RK_origins}). The formula for RK-4 is given by:
\begin{align*}
    y_{k+1} &= y_k + \frac {\Delta t}6(m_1+ 2m_2 + 2m_3 +m_4) \cr
    m_1 &= f(t_k,y_k) \cr
    m_2 &= f(t_k + \frac {\Delta t}2,y_k+\frac {\Delta t}2  m_1) \cr
    m_3 &= f(t_k + \frac {\Delta t}2,y_k + \frac {\Delta t}2 m_2 ) \cr
    m_4 &= f(t_k + \Delta t,y_k +  m_3 \Delta t)
\end{align*}
\section{Adaptive Numerical Methods for an IVP }
Adaptive methods to solve an IVP have similar goals and similar structure as what was discussed in adaptive integration. The primary interest when using and adaptive method is efficiency with the goal of using the largest steps as possible and adaptively reducing the step size for only the portions where it is needed. The general structure of the numerical method for solving an IVP is using a predictor and corrector where a predictor method is a low order method and the corrector is a higher order method. The test to determine if a step needs to be reduced or not is to compare the estimate from the predictor with the estimate from the corrector. If the predictor and corrector agree then the estimates must be close to the exact answer and with this good fortune the method takes the corrector estimate and steps forward. If the predictor and corrector do not agree then this is viewed as being a poor approximation and the step size is reduced with the estimates being recalculated with the smaller step size and compared.  \par \noindent
%
A rough outline of how an adaptive method is structured starts with our generic initial value problem, $\ds \frac {dy}{dt} = f(t,y)$, with $\ds y(t_0) = y_0$. We are interested in solving the problem for terminal value of t, which we will reference as T. The method would start with setting a default tolerance to drive accuracy and a default step size along with a minimum step size. \par {\bf Outline of Adaptive solver for an IVP}
\begin{verbatim}
y = y0; %initial y value
t = t0; %initial t value
dt = default_t; %default step size
% tol = initial tolerance
% T = stopping value for t
% dtmin = minimum step size;

while t < T & dt >= dtmin % loop for t to build a solution
  yp = predictor(f,t,y,dt); %low order approximation
  yc = corrector(f,t,y,dt); %higher order approximation
  if (1/dt)*abs(yp-yc) < tol  % (1/dt)*|yp-yc| is a way to weight tolerance
    y = yc; % corrector should be more accurate
    t = t + dt; % step forward
    dt = default_t; % reset to default
  else
    dt = dt/2; % reduce step size and try again
    tol = tol/2; % reduce tolerance over smaller interval
  end % end if
  
end % end while
\end{verbatim}
Notes on adaptive methods
\begin{enumerate}
    \item The primary purpose to use an adaptive method is efficiency
    \item Just like adaptive integration, an adaptive method to solve an IVP reduces step size where it is needed and not everywhere else
    \item The algorithm does need to work to prevent infinite loops
    \begin{enumerate}
        \item Minimum step size prevents reduction of dt to zero
        \item Resetting to default step size is essential
        \item In general, the reduction of step size can be more dynamic
    \end{enumerate}
    \item Adaptive methods can stall
    \begin{enumerate}
        \item Problems with singularities
        \item Solutions might not be defined for the entire interval of $(t_0,T)$
    \end{enumerate}
    \item A key feature of these adaptive methods is that calculations involved between the predictor and corrector heavily overlap. In other words, the corrector is often just a small calculation added to the predictor. Without this overlap the calculations per iterations would be more costly and the efficiency of the method would not be realized. 
    \item The name of these methods often include a hint to the order of the methods used. For example, ode45 in Matlab is an adaptive method that uses a predictor method that is fourth order and a corrector method that is fifth order. Given that the predictor and correct must overlap it is almost always the case that the orders of each method are consecutive integers. 
  \item The extension of Runge-Kutta to an adaptive method does carry the new label of {\bf Runge-Kutta-Fehlberg}. The initial development of the Runge-Kutta methods were founded by the work of two Germans in the early 1900s: Carl Runge and Wilhelm Kutta. Carl Runge was a physicist that was working on atomic spectra. Wilhelm Kutta was an applied mathematician that was working on differential equations involving aerodynamics. Another German Mathematician, Erwin Fehlberg, was able to leverage Runge-Kutta methods of different orders in a predictor/corrector structure to construct an adaptive method. 
\end{enumerate}

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{12}{Systems of Ordinary Differential Equations}{Kyle Riley}{Riley}
\par \noindent 
The work we have completed to numerically solve a single initial value problem can be expanded to work for systems of differential equations. The big jump from the single equation is to consider the problems via vectors and doing all the calculations with vectors. This work is easier with the full experience and knowledge from Calculus 3, but it is still possible to process the work if one considers the work step by step. The first thing to establish is that any higher order differential system can be rewritten into a form that is a first order system of differential equations. Once this fact is established then the remaining issue is to define how to adapt the methods we have learned to work for a first order system of differential equations. 
\section{Higher Order Differential Equations}
Consider the example: 
$$\frac {d^2y}{dt^2} + \frac {dy}{dt} + 2y = t\cos(t); \ {\rm with}\ \ y(0)=1, \ {\rm and} \ y'(0)=-2.$$
Let $\ds x = y'$ then it follows that:
\begin{align*}
    y' &= x;  \hspace{1.0in} y(0)=1,\cr
    x' &= -x - 2y + t\cos(t); \ \ x(0)=-2.
\end{align*}
Thus, the result is a first order system that is equivalent to the original second order differential equation, including the initial values. Once mastered, this technique is rather easy to implement where the order of the equation translates into the size of the system. It follows that a second order equation will translate into a two dimensional system, a third order equation will translate into a three dimensional first order system, and the pattern continues to any finite order. Consider the nonlinear example:
$$\frac {d^2y}{dt^2} - 2 \frac {dy}{dt}y = e^t; \ {\rm with}\ \ y(0)=-2, \ {\rm and} \ y'(0)= 3.$$
Let $\ds x = y'$ then it follows that:
\begin{align*}
    y' &= x;  \hspace{.5in} y(0)=-2,\cr
    x' &= 2xy +e^t ; \ \ x(0)= 3.
\end{align*}
Higher order problems are addressed by simply increasing the number of variables used to define the first order system, which can be observed via the example:
$$y''' +y'' -2y' +3y = e^{-t}; \ y(0)=4, \ y'(0)=1, {\rm and} \ y''(0)-2.$$
Let $\ds x=y'$ and $\ds z = x'$ then it follows that:
\begin{align*}
    y' &= x;  \hspace{1.2in} y(0) = 4, \cr
    x' &= z; \hspace{1.2in} x(0) = 1, \cr
    z' &= -z + 2x -3y + e^{-t} ; \ \ z(0)=-2.
\end{align*}
This technique of rewriting higher order systems into a first order system explains the work done in differential equations. Any higher order linear system can be rewritten into a first order linear system and so the work of finding the eigenvalues for the associated matrix is the same as the characteristic equations that were used when solving the higher order systems. It is important to note that this technique of rewriting higher order equations into first order systems works no matter if the system linear or nonlinear. Thus, our ability to apply numerical methods can be employed on both linear and nonlinear systems. 
\section{Numerical Methods for First Order Systems}
The key to recognizing the connection between using a numerical method for just one first order equation and applying the same technique for a first order system is the ability to describe and implement the technique using vectors. Consider the following first order system:
\begin{align*}
    \frac {dx}{dt} &= x - y + t; \ \ \ x(0)=1,\cr
     \frac {dy}{dt} &= x - 2y - t^2; \  y(0)=-3
\end{align*}
Recall the generic IVP formula we have been using: $\ds \frac {dy}{dt} = f(t,y)$ with $\ds y(t_0)=y_0$. If we let $\ds \vec w = \begin{pmatrix} x \\ y \end{pmatrix}$ then the first order system can take the form:
$$\vec w ' = \vec F(t,\vec w), {\rm where} \ \ \vec F(t,\vec w) =  \begin{pmatrix} x - y + t \\ x - 2y - t^2 \end{pmatrix}, \ {\rm and} \ \vec w (0) = \begin{pmatrix} 1 \\ -3\end{pmatrix}.$$
{\bf Note: }{\color{teal} The amazing thing is that all the formulas for the numerical methods discussed for solving a single equation IVP are the same formulas for systems with the replacement of the vector valued system in place of the single equation.}

{\bf Euler's Method} for one dimension is $\ds y_{k+1} = y_k + \Delta t  \ f(t_k,y_k)$ and the transformation to a multiple dimension system is simply:
$$\vec w_{k+1} = \vec w_k + \Delta t \ \vec F(t_k,\vec w_k), $$
where $\ds \Delta t$ is the step size in t, $\ds \vec w_k = \begin{pmatrix} x_k \\ y_k \\ \vdots  \end{pmatrix}$, and $\ds \vec F(t_k,\vec w_k)$ is simply the values of $\ds t_k$ and $\ds \vec w_k$ substituted into the vector valued function, $\ds \vec F$. 
\par \noindent 
The adaptation to systems is similar for all the other methods discussed in the previous lecture.
 
 {\bf Modified Euler}
\begin{align*}
    \vec w_{k+1} &= \vec w_k + \frac {\Delta t}2(\vec m_1+\vec m_2) \cr
    \vec m_1 &= \vec F(t_k,\vec w_k) \cr
    \vec m_2 &= \vec F(t_k + \Delta t,\vec w_k+\Delta t\  \vec m_1)
\end{align*}
 


{\bf Third Order Runge-Kutta}
 
\begin{align*}
    \vec w_{k+1} &= \vec w_k + \frac {\Delta t}6(\vec m_1+ 4\vec m_2 + \vec m_3) \cr
    \vec m_1 &= \vec F(t_k,\vec w_k) \cr
    \vec m_2 &= \vec F(t_k + \frac {\Delta t}2,\vec w_k+\frac {\Delta t}2  \vec m_1) \cr
    \vec m_3 &= \vec F(t_k + \Delta t,\vec w_k- {\Delta t}\vec m_1 + 2\vec m_2 \Delta t)
\end{align*}

{\bf Fourth Order Runge-Kutta}
\begin{align*}
    \vec w_{k+1} &= \vec w_k + \frac {\Delta t}6(\vec m_1+ 2\vec m_2 + 2\vec m_3 + \vec m_4) \cr
    \vec m_1 &= \vec F(t_k,\vec w_k) \cr
    \vec m_2 &= \vec F(t_k + \frac {\Delta t}2,\vec w_k+\frac {\Delta t}2  \vec m_1) \cr
    \vec m_3 &= \vec F(t_k + \frac {\Delta t}2,\vec w_k + \frac {\Delta t}2 \vec m_2 ) \cr
    \vec m_4 &= \vec F(t_k + \Delta t,\vec w_k +  \vec m_3 \Delta t)
\end{align*}

This same approach in applying vectors is also present in the corresponding Matlab code. 

\subsection{How to use Matlab to solve first order systems}
Matlab is built to work with vectors and matrices, which means the vector approach is ideal for how Matlab would view solving a system of differential equations. An example to consider is
\begin{eqnarray}
\frac {dx}{dt} &= t-x, \ \ \ x(0)=\frac 12 \cr \cr 
\frac {dy}{dt} &= xy , \ \ \ y(0) = \frac {-1}2.
\label{e:sys_ode_ex_1}
\end{eqnarray}
The system in (\ref{e:sys_ode_ex_1}) is a first order nonlinear equation with coupling since the second equation involves both x and y. It is possible to solve this system analytically since the first equation can be solved directly and that solution can be applied in solving the second equation, but we proceed straight to the approach on how to solve this problem using Matlab. We introduce the vector $\ds \vec w = \begin{pmatrix} x \cr y \end{pmatrix}$ and in Matlab the order of arrangement matters since  $\ds \vec w = \begin{pmatrix} x \cr y \end{pmatrix}$  would have a different manner of coding than if we initiated the problem with a vector of the form  $\ds \begin{pmatrix} y \cr x \end{pmatrix}$ . 

The application of Modified Euler for a system is very similar and the only thing is remember everything is done via vectors. Thus,
\begin{verbatim}
% initialize
t = 0;  %initial t value
W = [0.5; -0.5];  %initial value
dt = 0.02;  % step size in t
Tn = 2;     % Terminal stopping value for t

%define f as a vector valued anonymous function 
f = @(t,W) [t - W(1); W(1)*W(2)]; 


% Loop with Modified Euler for systems
while t <  Tn - dt/2  %-dt/2 to make sure we stop at Tn and don't go over Tn
    M1 = f(t,W);
    M2 = f(t+dt,W+dt*M1);
    W = W + (dt/2) *(M1 + M2); %update for Modified Euler
    t = t + dt;  % update t
end  %end while
\end{verbatim}

It is also possible to use an adaptive method, like ode23 to solve our first order system, which can be accomplished with
\begin{verbatim}
>> f = @(t,W) [t - W(1); W(1)*W(2)];
>> [T, W] = ode23(f,[0 2], [0.5; -0.5]);
\end{verbatim}
It should be noted that the output from ode23 involves arrays T and W, which output in the followig form
\begin{verbatim}
>> [T,W]
         0    0.5000   -0.5000
    0.0800    0.4647   -0.5196
    0.2800    0.4136   -0.5669
    0.4800    0.4080   -0.6151
    0.6800    0.4397   -0.6691
    0.8800    0.5020   -0.7349
    1.0800    0.5892   -0.8193
    1.2800    0.6969   -0.9315
    1.4800    0.8213   -1.0839
    1.6800    0.9594   -1.2947
    1.8800    1.1087   -1.5918
    2.0000    1.2029   -1.8285
\end{verbatim}
This format has T has a column vector of the t values for each time step. The step sizes vary since this is an adaptive method. W is an array comprised of the first column corresponding to the x values for every time step and the second column being the y value for every time step. If the user is interested in plotting the solution of x then the Matlab command is: plot(T,W(:,1)), which will plot the data with T and the first column in W. If the user is interested in plotting the solution of y then the Matlab command is plot(T,W(:,2)), which will plot the t values and the values corresponding to the second column. 

One last thing to mention is how to arrange a solution in Matlab that uses a subfunction. This approach matches the work seen in more traditional code of Matlab and it also has the advantage of being a bit more explicit regarding the structure of the first order system. The code for using a subfunction to use ode23 to solve (\ref{e:sys_ode_ex_1}) can be represented in an m file solvesys1.m and the code is the following:
\begin{verbatim}
%  solvesys1.m 

[T, W] = ode23(@myfunsysdiff,[0 2], [0.5; -0.5]);

% sub function to the evaluation of F(t,W) for the IVP in system form
function dw =  myfunsysdiff(t,w)
% W' = F(t,W)  with W = [x; y]
% x' = t-x
% y' = xy

% initialize the derivative vector
dw = 0*w;

dw(1) = t + w(2); % x' = t - x
dw(2) = w(1)*w(2); %y' = xy
end % end function 
\end{verbatim}
The use of a subfunction is more lines of code, but it does have the advantage of arranging your system equation by equation and can be easier to develop and track than managing the same problem in a vector valued anonymous function. In all cases, the order matters in how you first define your vector w. If w = [x; y] then this means that x = w(1) since it is the first element of w, and y =w(2) since it is the second value of w. All of this structure does extend for systems that have more dependent variables. 

\section{Solving System ODEs with a Spreadsheet Approach}
While Matlab loves the use of vectors to solve systems of differential equations it is common for students to have difficulty with using the vector notation. However, presenting the technique in a form of a spreadsheet is often easier to understand. Consider the initial value problem:
\begin{eqnarray}
\frac {dx}{dt} &= x-y + t, \ \ \ x(0)=1 \cr \cr 
\frac {dy}{dt} &= xy - t, \ \ \ y(0) = 3
\label{e:euler_sys}
\end{eqnarray}
In this presentation, we will not jump to vectors and instead look at our two equations in this system as:
\begin{eqnarray*}
\frac {dx}{dt} &= f_1(t,x,y), \ {\rm with} \ \  x(t_0) = x_0 \cr \cr
\frac {dy}{dt} &= f_2(t,x,y), \ {\rm with} \ \  y(t_0) = y_0.
\end{eqnarray*}
The approach of using Euler's Method to solve the system can be viewed as using Euler's Method on each equation instead of attempting to apply it to a vector. Hence, given a step size $h$ in $t$ would result in:

\begin{eqnarray*}
x_1 &= x_0 + hf_1(t_0,x_0,y_0) \cr \cr
y_1 & = y_0 + hf_2(t_0,x_0,y_0).
\end{eqnarray*}
Repeated steps of Euler's Method turns into repeated applications to each equation and translates into the following extension:
\begin{eqnarray*}
x_2 &= x_1 + hf_1(t_1,x_1,y_1) \cr \cr
y_2 & = y_1 + hf_2(t_1,x_1,y_1). \cr \cr
x_3 &= x_2 + hf_1(t_2,x_2,y_2) \cr \cr
y_3 & = y_2 + hf_2(t_2,x_2,y_2). \cr \cr
\vdots & \vdots \hspace{0.5in} \vdots \cr
x_n &= x_{n-1} + hf_1(t_{n-1},x_{n-1},y_{n-1}) \cr \cr
y_n & = y_{n-1} + hf_2(t_{n-1},x_{n-1},y_{n-1}). \cr \cr
\end{eqnarray*}
This structure is rather easy to implement in a spreadsheet since each element can be categorized in a column in the spreadsheet and then each row would represent every time step. This approach actually mirrors what is done in Matlab using vectors and loops, but students often find this approach easier to implement by hand. If we apply this technique to our example in (\ref{e:euler_sys}) with a step size of $h=0.1$ then two steps would generate:
\begin{eqnarray*}
(t_0,x_0,y_0) &= (0,1,3) \cr 
(t_1,x_1,y_1) &= (0.1,0.8,3.3) \cr 
(t_2,x_2,y_2) &= (0.2,0.56,3.554) \cr 
\end{eqnarray*}
Note: {\color{teal} it should be noted that the calculations in this approach is {\bf exactly} the same amount of work as the approach using vectors and the big difference is avoiding vector notation compared to utilizing vectors. The vector notation does pay dividends when one considers large systems while spreadsheets can become more cumbersome with large systems. }

\section{Revisiting Error with Systems of Ordinary Differential Equations}
As pointed out earlier, the initial value problem with a first order system takes the form:
$$\vec w ^{\prime} = \vec F(t,\vec w); \ \ \vec w(t_0) = \vec w_0.$$
The big point is that we are operating with solutions that now take the form of vectors and so now is a good time to revisit our definition of absolute error and relative error. Recall from section \ref{s:error_section} the definitions we had for absolute error and absolute relative error. If we let $y$ represent the exact answer and $y_k$ represent the approximation then our definitions would follow with:
$${\rm absolute \ error} = \vert y - y_k\vert $$
$${\rm absolute \ relative \ error} = \frac {\vert y - y_k\vert}{\vert y\vert}.$$
The question, at this point, is how do we translate these definitions when the exact answer, $\ds \vec w$, and the approximation, $\ds \vec w_k$, are both vectors. The answer is rather simple in that we just take the magnitude of the vectors using the same definition. The notation for this calculation results in:
$${\rm absolute \ error} = \Vert \vec w - \vec w_k\Vert $$
$${\rm absolute \ relative \ error} = \frac {\Vert \vec w - \vec w_k\Vert}{\Vert \vec w\Vert}.$$
To be crystal clear on the notation, if we define $\ds \vec a = \begin{pmatrix} x_1 \cr \vdots \cr x_n \end{pmatrix}$ then it follows that
$$\Vert \vec a \Vert = \sqrt{\sum_{k=1}^n x_k^2}.$$ Thus, the absolute error we calculated in one dimension with the absolute value is replaced using vector magnitude instead. The absolute value, in the context of vectors, often represents the calculation with the vector magnitude in many books. However, in mathematics textbooks the use of the notation, $\ds \Vert \vec a \Vert$ is far more commonly employed and we will adhere to this notation in this class. 

Note:{ \color{teal} These definitions of absolute error and absolute relative error will also be employed for all the error discussions in subsequent lectures involving systems. }

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{13}{Boundary Value Problems}{Kyle Riley}{Riley}

Boundary value problems are common models that show up in a variety of applications. A few examples of applications that involve boundary value problems would include: one dimensional heat equations, loads on a beam, circuits, and electromagnetic potential. Boundary value problems are also a strong preliminary modeling tool that often extend to partial differential equations. It is possible you skipped the Boundary Value Problem (BVP) in your Differential Equations class, but it turns out much of the solution techniques you learned in Differential Equations can be applied to linear BVPs. An simple example of a BPV is:
\begin{equation}
y'' + y' - 6y = e^{t}, \ \ {\rm with} \ y(0) = 2, \ {\rm and} \ y(4)=10.
    \label{e:bvp_example}
\end{equation}
The big distinction with the equation in (\ref{e:bvp_example}) is that it is not an initial value problem and the two portions known about the solution $y(t)$ are two different time values: at t=0 and t=4. For linear systems, the solution techniques covered in differential equations apply and it is possible to solve linear boundary value problems exactly. You can find a simple example of solving a boundary value problem analytically by looking at the video that can be found at \href{https://youtu.be/OjtMeMKV1NI}{https://youtu.be/OjtMeMKV1NI}.  However, a nonlinear bvp presents a challenge that cannot be solved analytically with the techniques covered in differential equations.  

\section{Type of Boundary Value Conditions}
The boundary conditions have important implications for physical applications of BVPs. The boundary conditions that result in a fixed value for the function at two points, for example $y(0)=2$ and $y(5)=-2$ is also known as the {\bf Dirichlet boundary condition}. The Dirichlet name traces back to Johann Peter Gustav Lejeune Dirichlet (1805-1859). The fixed boundary conditions correspond to fixed values on the boundary and is common for physical systems that relate to vibrating systems or other physical systems where the conditions on the boundary can be fixed. Another common boundary condition is the {\bf Neumann boundary condition} and it is typically identified as conditions fixing the derivative at the boundary, i.e., $y'(0) = 1$ and $y'(5) = 3$. The Neumann name traces back to Carl Neumann (1832-1925) and this boundary condition is often deployed related to flux or other boundary conditions that can be interpreted in some kind of rate on the boundary. Of course, boundary conditions can come in a variety of designs and the mixed boundary conditions are when some of the boundary conditions might be fixed values of the function and other boundary conditions involve fixed values for a derivative. 


\section{Numerical Methods for Solving a BVP}
We will cover two techniques for solving a boundary value problem. The first technique will be the shooting method and the second will be finite differences. 

\subsection{Shooting Method}

The shooting method is a fun technique that comes from rigging a problem that we can't solve into a problem that we can solve. Consider the problem:
$$y'' + y'y = t-4 \ \ {\rm with} \ \ y(0)=1 \ {\rm and} \ y(3)=4.$$
This problem is nonlinear and so that eliminates much of the standard methods covered in differential equations. We can attempt to convert this problem to a first order system that is an initial value problem with one complication. If we let $x=y'$ the the first order system becomes:
\begin{align*}
    y' &= x \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y(0)=1 \cr
    x' &= -xy+t-4 \ \ x(0)=?
\end{align*}
The incredible idea behind the shooting method is to guess the value of the derivative for the initial value problem in order to use that guess to help solve the original BVP. For example, if we go with a guess of $y'(0)=14$ then the associated first order system is: 
\begin{align*}
    y' &= x \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y(0)=1 \cr
    x' &= -xy+t-4 \ \ x(0)=14,
\end{align*}
which can be solved by the numerical techniques covered earlier. If we use Matlab's ode23 to solve the generated IVP and then inquire on the value of the approximation at t=3 then we get $y(3)=3.8263$. We are interested in a solution that can generate the boundary value: $y(3)=4$. The guess of $y'(0)=14$ and the corresponding solution does lead to a revised guess of $y'(0)=15$. The revised guess is associated with the system:
\begin{align*}
    y' &= x \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y(0)=1 \cr
    x' &= -xy+t-4 \ \ x(0)=15,
\end{align*}
and a Matlab ode23 solution applied to this problem results in a solution that at t=3 generates, $y(3)=4.0741$. We can keep revising our guesses, but it turns out we can use our old friend interpolation to help us. The data we have generated so far is: $(14,3.8263)$ and $(15,4.0741)$, but to implement the interpolation in the same manner as we have used before then it is best to reconfigure the data. Hence, the change in our data format returns:  $(3.8263,14)$ and $(4.0741,15)$. If we apply linear interpolation to the given data and interpolate for $x=4$ then we get 14.7008. To investigate how well linear interpolation performed then we turn to the revised IVP system:
\begin{align*}
    y' &= x \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y(0)=1 \cr
    x' &= -xy+t-4 \ \ x(0)=14.7008.
\end{align*}
Using ode23 to approximate the solution to this last IVP does result a function that produces $y(3)=4.0015$. One decision is to determine if 4.0015 is sufficiently close to 4, or if more work must be done. The current value being used is from linear integration and it is likely the underlying process is nonlinear, which would likely solicit a nonlinear interpolation technique. In addition, we used just two data points and things would likely improve with more data points. Lastly, the accuracy of ode45 is tied to a default tolerance within the adaptive solver and this problem might require a smaller tolerance. It is clear there are several ways to refine the shooting method and the components that might produce the best results will likely vary depending on the given BVP.


\subsection{Finite Differences}

Finite Differences is a very useful technique that can be used for linear differential equations. Finite differences is also a common method to be applied to partial differential equations. Consider the boundary value problem:
\begin{equation}
    \frac {d^2y}{dx^2} + 2 \frac {dy}{dx} - y = x^2, \ \ y(0)=1, \ {\rm and} \ y(1)=4
    \label{e:bvp4finite_diff}
\end{equation}
The main object of the finite difference method is to replace the differential equation in (\ref{e:bvp4finite_diff}) with finite difference approximations. Thus, we trade our continuous problem with derivatives to a discrete problem using finite differences:
\begin{equation*}
     \frac {d^2y}{dx^2} + 2 \frac {dy}{dx} - y = x^2 \rightarrow \frac {y(x_i+\Delta x) - 2y(x_i)+ y(x_i-\Delta x)}{\Delta x^2} + 2 \frac {y(x_i+\Delta x) -  y(x_i)}{\Delta x} - y(x_i) = x_i^2.
\end{equation*}
For this method, there are several changes we are going to make in notation to make things a bit easier to write down. In this section, we will let $h$ represent the step size, which means $h = \Delta x$. In addition, $\ds y_i = y(x_i)$, $\ds y(x_i+\Delta x) = y_{i+1}$, and $\ds y(x_i-\Delta x) = y_{i-1}$. Hence, the compact version of the difference equation will be:
\begin{equation}
 \frac {y_{i+1} - 2y_i+ y_{i-1}}{h^2} + 2 \frac {y_{i+1} - y_i}{h} - y_i = x_i^2.
 \label{e:bvp_finite_diff_h}
\end{equation}
The discretization of the space does result in x becoming a vector $[x_0, x_1, x_2, ..., x_n]$ with $x_0 = 0$ and $x_n = 1$ with the associated vector of: $[y_0, y_1, y_2, ..., y_n]$ with $y_0 = 1$ and $y_n = 4$. The next step in working with (\ref{e:bvp_finite_diff_h}) is to multiple both sides by $\ds h^2$ in order to group terms, i.e.,
\begin{align}
    \frac {y_{i+1} - 2y_i+ y_{i-1}}{h^2} + 2 \frac {y_{i+1} - y_i}{h} - y_i &= x_i^2 \cr
    y_{i+1} - 2y_i+ y_{i-1} + 2 h(y_{i+1} - y_i) - h^2y_i &= h^2x_i^2 \cr
   (1+2h)y_{i+1} - (h^2+2h+2)y_i + y_{i-1} &= h^2x_i^2
   \label{e:finite_diff_template}
\end{align}
The equation in (\ref{e:finite_diff_template}) is true for every $i = 1...n-1$, which means we have n-1 unknowns ($[y_0, y_1, y_2, ..., y_n]$) along with the n-1 equations. The boundary values influence just two equations ($i=1$ and $i=n-1$), which for $i=1$
$$(1+2h)y_{2} - (h^2+2h+2)y_1 + y_{0} = h^2x_1^2,$$
but $\ds y_0 = 1$, which results in:
$$(1+2h)y_{2} - (h^2+2h+2)y_1 = h^2x_1^2-1.$$
For $i=n-1$ the template generates
$$(1+2h)y_{n} - (h^2+2h+2)y_{n-1} + y_{n-2} = h^2x_{n-1}^2,$$
but $\ds y_n = 4$, which results in:
$$- (h^2+2h+2)y_{n-1} + y_{n-2} = h^2x_{n-1}^2-4(1+2h).$$
Combining the boundary values and the system of equations that correspond to (\ref{e:finite_diff_template}) will generate a tridiagonal matrix:
$$ \begin{pmatrix} -(h^2+2h+2) & (1+2h) &  & & & 0 \cr 1 & -(h^2+2h+2) & (1+2h) & & & \cr  \ \ \ \ \ \  \ddots & \ddots & \ \ \ \ \ \ \ddots &  & \cr  \cr & 1 & -(h^2+2h+2) &  (1+2h) \cr 0 &  & 1 & -(h^2+2h+2) \end{pmatrix} \begin{pmatrix} y_1 \cr y_2 \cr \vdots \cr \vdots \cr y_{n-2} \cr y_{n-1}\end{pmatrix} = \begin{pmatrix} h^2 x_1^2-1 \cr h^2x_2^2 \cr \vdots \cr \vdots \cr h^2 x_{n-2}^2 \cr h^2x_{n-1}^2-4(1+2h)\end{pmatrix}.$$
A linear differential equation results in a linear system and solution can be found via solving the linear system. The finite difference method has the extra benefit of a transparent way of tracking accuracy by shrinking the step size with the order of the methods used to approximate the derivatives as a way to track accuracy overall. Another benefit of the finite difference method is that it has the ability to address a variety of boundary conditions since any derivative boundary condition can be approximated by the appropriate finite difference method. The finite difference method does have the restriction that the method cannot be easily solved for nonlinear systems. 

The choice of the finite difference method to approximate each of the given derivatives does have considerable flexibility. The choices of using a forward, centered, or backward difference approximation become pivotal when working with partial differential equations, but in the ordinary differential equation setting the choice of finite difference approximation has a stronger influence on the related convergence where the centered difference approximation has a higher order of convergence than forward or backward difference. 

Lastly, the finite difference method does have the advantage of addressing a variety of boundary conditions. For example, the problem in (\ref{e:bvp4finite_diff}) involves fixed boundary conditions (also known as Dirichlet boundary conditions), which were incorporated into the linear system as additional components on the right hand side. Finite Differences can also apply for problems with derivative boundary condition (also known as Neumann boundary conditions). To save some time an effort, we can reconsider the same differential equation in (\ref{e:bvp4finite_diff}) with the same difference equations, but consider the problem with derivative boundary conditions:
\begin{equation}
    \frac {d^2y}{dx^2} + 2 \frac {dy}{dx} - y = x^2, \ \ y'(0)=2, \ {\rm and} \ y'(1)= -3.
    \label{e:bvp5finite_diff}
\end{equation}
Applying the same finite difference approximations as before results in the same finite difference template:  $\ds (1+2h)y_{i+1} - (h^2+2h+2)y_i + y_{i-1} = h^2x_i^2$. The boundary conditions are again involved in the first and last equations, but the results are different due to the fact these are derivative boundary conditions. For the boundary condition: $\ds y'(0)=2$ the forward finite difference approximation is used to generate $\ds \frac {y_1-y_0}h = 2$, which can be solved for $\ds y_0$ to reveal: $\ds y_0 = y_1-2h$. The expression of derivative boundary condition as  $\ds y_0 = y_1-2h$ can be used in the first equation ($i=1$):
\begin{align*}
    (1+2h)y_{2} - (h^2+2h+2)y_1 + y_{0} &= h^2x_1^2 \cr 
    (1+2h)y_{2} - (h^2+2h+2)y_1 + y_1-2h &= h^2x_1^2 \cr
    (1+2h)y_{2} - (h^2+2h+1)y_1  &= h^2x_1^2+2h.
\end{align*}
The same work applies for the right side boundary condition, $\ds y'(1)= -3$, which is addressed using the backward difference approximation: $\ds \frac {y_n-y_{n-1}}h = -3$. Solving for the right hand function value results in the expression: $\ds  y_n = y_{n-1} -3h$ and this can be applied in the last equation ($\ds i=n-1$):
\begin{align*}
(1+2h)y_{n} - (h^2+2h+2)y_{n-1} + y_{n-2} &= h^2x_{n-1}^2 \cr
(1+2h)(y_{n-1} -3h) - (h^2+2h+2)y_{n-1} + y_{n-2} &= h^2x_{n-1}^2 \cr
- (h^2+1)y_{n-1} + y_{n-2} &= h^2x_{n-1}^2 + 3h(1+2h).
\end{align*}
The combination of all this information results in the following linear system that corresponds to solving the problem posed in (\ref{e:bvp5finite_diff}):
$$ \begin{pmatrix} -(h^2+2h+1) & (1+2h) &  & & & 0 \cr 1 & -(h^2+2h+2) & (1+2h) & & & \cr  \ \ \ \ \ \  \ddots & \ddots & \ \ \ \ \ \ \ddots &  & \cr  \cr & 1 & -(h^2+2h+2) &  (1+2h) \cr 0 &  & 1 & -(h^2+1) \end{pmatrix} \begin{pmatrix} y_1 \cr y_2 \cr \vdots \cr \vdots \cr y_{n-2} \cr y_{n-1}\end{pmatrix} = \begin{pmatrix} h^2 x_1^2 + 2h \cr h^2x_2^2 \cr \vdots \cr \vdots \cr h^2 x_{n-2}^2 \cr h^2x_{n-1}^2+3h(1+2h)\end{pmatrix}.$$

\section{Applications of Boundary Value Problems}

Boundary value problems can arise in several applications in engineering and science. We will discuss some of the more common examples with the hope you can recognize others in context. 

\subsection{RLC Circuit }

Recall the classic RLC problem you likely discussed in differential equations. A reminder from differential equations is the model:
\begin{equation}
L \frac{d^2q}{dt^2} + R\frac {dq}{dt} + \frac 1C q = E(t),
\label{e:RLCbvp}
\end{equation}
where L is inductance (often measured in henrys), R is resistance (often measured in Ohms), C is capacitance (often measured in farads), and $E(t)$ is the voltage source of the circuit as a function in time and measured in volts. The $q(t)$ is the charge in the circuit as a function of time (often measured in Coulombs). Another fun fact that is often discussed in differential equations is that $\ds \frac {dq}{dt}$ is the current in a circuit. To make things simple, please consider the case where $L=2$, $R=16$, $C=0.02$, E is a constant function set equal to 100, and $q(0)=5$ along with $q(10)=3$. Since the information regarding charge is at {\it two different} time values, it follows that this is a boundary value problem. The good news for this example is that it can be completely solved with the techniques covered in differential equations since the coefficients are constant and the differential equation is linear. However, if it turns out resistance is not constant and we have $R(t)$ varies with time then this can be enough to negate the use of the analytical methods covered in differential equations and the person working with the model would then need to utilize some type of numerical method. 

\subsection{Steady State Heat Profile in a Rod}
Another example that was likely not covered in differential equations is modeling the heat profile in a thin rod that is between to plates. A crude illustration is given in Figure \ref{f:heat_rod_f1} where the goal is to model the steady state temperature profile of a thin rod between two plates (or walls) and the rod is surrounded by a fluid that is kept at a uniform temperature. This simple problem has several assumptions to make it easier to build a simple model for the given situation. 
\begin{itemize}
\item{} The rod is thin enough and constructed of a uniform conductor that enables heat to transfer in the radial direction
\item{} x measures the distance from the left wall and the length of the rod is L
\item{} the fluid the surrounds the rod is uniform material and of uniform temperature
\end{itemize}
A balance equation gives us:
\begin{equation}
0=q(x) A_c - q(x+\Delta x) A_c + h A_s(T_{\infty}-T(x)),
\label{e:heat_rod_balance}
\end{equation}

\begin{figure}[!ht]
\centering
\includegraphics[height=80mm]{Heat_rod_f1.png}
\caption{Figure of a rod between to end plates using an app from Geogebra \cite{GG19}}\label{f:heat_rod_f1}
\end{figure}
where $\ds q(x)$ be the flux of the heat moving into position x on the rod, $\ds A_c$ is the cross sectional area, $\ds A_s$ is the surface area, $h$ is the heat transfer coefficient, $\ds T_{\infty}$ is the ambient temperature of the fluid that surrounds the rod, and $T(x)$ is the temperature of the rod at position $x$. Some details on the components of (\ref{e:heat_rod_balance}) comes from looking at $\ds q(x)A_c$ as the heat conducting along the rod at position $x$ (i.e. the heat coming in), which $\ds q(x+\Delta x)A_c$ is the heat coming out, and the last term ($\ds h A_s(T_{\infty} - T(x))$) is the heat lost or gained with the surrounding fluid.

As with every other model using calculus, we divide both sides by $\ds \Delta x$ and we are interested in the case when $\ds \Delta x \rightarrow 0$, which results in
$$\frac {(q(x)-q(x+\Delta x)A_c}{\Delta x} + \frac {hA_s}{\Delta x} (T_{\infty} - T(x))=0.$$
With this being a thin rod we have $\ds A_c = \pi r^2$ and $\ds A_s = 2 \pi r \Delta x$, which the limit as $\ds \Delta x \rightarrow 0$ results in:
$$\frac {-dq}{dx} + \frac {2h}r (T_{\infty} - T(x))=0.$$
The application of Fourier's Law is using the relationship that $\ds q=-k\frac {dT}{dx}$ where $k$ is a constant related to a materials heat conductivity. Putting everything together results in:
\begin{equation}
\frac {d^2T}{dx^2} + H(T_{\infty}-T(x))=0,
\label{e:heat_rod}
\end{equation}
where $\ds H = \frac {2h}{kr}$. The boundary application arrives when the fixed temperatures are introduced, an example could be $\ds T(0) = 300$, $\ds T(L) = 450$, and $\ds T_{\infty} = 200$. This problem is also a linear, second order differential equation with constant coefficients and so this problem can also be solved with the analytical methods covered in a differential equations class. Adaptations that create the need for numerical solutions would include situations where $H$ varies with $x$, or if the effects of heat transfer involved components that were nonlinear. 
%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{14}{Solving Linear Systems}{Kyle Riley}{Riley}
\section{Introductory Definitions}
In general, a linear system can be expressed as a matrix system:
$$A \vec x = \vec b.$$
This result includes matrices that are not square, i.e., the number of rows do not match the number of columns. An {\bf overdetermined} system is where there are more equations than unknowns, which is the same as a matrix expression that has more rows than columns. It should be noted that overdetermined systems will generally not have a solution. A system is classified as {\bf underdetermined} if there are more unknowns than equations, which is a system expressed as a matrix that has more columns than rows. It should be noted that underdetermined systems will generally have an infinite number of solutions.  

The analysis we will focus on involves systems that are expressed with square matrices (number of columns equals number of rows). However, all square systems can be classified into two different categories: singular and nonsingular systems. A {\bf nonsingular} system is one that has one unique solution while singular systems can have either multiple solutions or no solution. Related to this distinction is the {\bf inverse matrix} ($\ds A^{-1}$), which is defined as the unique matrix such that
$$AA^{-1}=A^{-1}A=I,$$
where $I$ is the identity matrix. Nonsigular systems have the distinction of having an inverse matrix while singular systems do not have an inverse. A brief summary of singular systems and nonsingular systems would include:
\begin{itemize}
    \item Singular Systems
    \begin{itemize}
        \item matrix does not have an inverse
        \item has an eigenvalue that is equal to zero
        \item determinant of the matrix is zero
    \end{itemize}
    \item Nonsingular Systems
    \begin{itemize}
        \item matrix does have an inverse
        \item all eigenvalues are not equal to zero
        \item determinant of the matrix is nonzero
    \end{itemize}
\end{itemize}

\section{Solution Techniques}
There are a wealth of techniques to solve linear systems with chapter 4 from \cite{KK09} being one of our primary resources. However, one can find additional information in \cite{KA89,AM96,BF11,CC10,KC02,MF04} and a particularly good resource is \cite{GV96}.

A standard technique is Gaussian Elimination with back substitution, which is described in detail in Chapter 4 of \cite{KK09}. A related method is the Gauss-Jordan technique and LU factorization is another related technique. It is expected that students should be able to use Gauss-Jordan to solve nonsingular systems and also be able to use Gaussian Elimination to solve both singular and nonsingular systems. 

A relevant numerical consideration for Gaussian Elimination involves partial pivoting, which is a form of Gaussian Elimination that is used to reduce the error due to machine roundoff and improve the accuracy of Gaussian Elimination. 

It is assumed you have seen row reduction methods in a previous class, we cover row reduction (Gaussian Elimination) in Calculus 2. If you have not encountered the techniques of solving linear systems using matrices then you can find reference to this in Chapter 4, section 6 in \cite{KK09}. Some videos that might be helpful can be found at:

\begin{itemize}
\item Gaussian Elimination with back substitution \href{https://youtu.be/2j5Ic2V7wq4}{https://youtu.be/2j5Ic2V7wq4}
\item Gauss Jordan Method for solving a system \href{ https://youtu.be/lP1DGtZ8Wys}{ https://youtu.be/lP1DGtZ8Wys}
\item Example of working with a singular system \href{https://youtu.be/JVDrlTdzxiI}{https://youtu.be/JVDrlTdzxiI}
\item A lecture on Gaussian Elimination with partial pivoting \href{https://youtu.be/DAMKhFwSaw8}{https://youtu.be/DAMKhFwSaw8}
\end{itemize}

\subsection{Numerical Methods and Hints for using Matlab}
Linear systems are a common infrastructure element that is used for solving a wide variety of problems in modeling and simulation. The thing to keep in mind is that many of these applications involve large linear systems where you can have thousands of equations and thousands of unknowns. On these huge scales, the importance of efficiency and accuracy take a pivotal role and in this subsection we will communicate at least an impression of what to look for in the tools you decide to use to solve these types of problems. If one were to use Gauss-Jordan to calculate the full inverse matrix for a square matrix that is n by n in dimension then the number of operations is $\ds {\cal O}(n^3)$. In addition, Gauss-Jordan can be prone to rounding errors and that can accumulate errors due to machine arithmetic. Thus, solving a large linear system by calculating the full inverse is a costly and potentially error prone approach. Matlab can calculate the complete inverse  via the command: inv(A), but the approach we often suggest in Matlab is the use of the $\ds \backslash$ command. A user tasked with solving a linear system of the form: $\ds A\vec x = \vec b$ can be solved in Matlab using: $\ds >> x = A\backslash b;$. There is a great deal work and programming that is built into the $\backslash$ command in Matlab, but the highlights are that this approach utilizes techniques that seek to reduce the computational load and also provide good management of rounding error. The bottom line is that the $\backslash$ command is an advanced program built to give you the most accuracy to a large linear system that Matlab can provide at the lowest computational cost. 


\section{Condition Number of a Matrix and Ill-Conditioned Systems}
In theory, linear systems come in just two flavors: singular and nonsingular, which means (in theory) a linear system either has a unique solution, or it does not have a unique solution. Numerically, the situation is a bit more grey with the existence of matrices that theoretically have an inverse, but numerically it is difficult to solve these systems. For example, 
$$A = \begin{pmatrix} 1 & 0 & 1 \cr 1 & 1 & 1 \cr 2 & 1 & 1.99\end{pmatrix}$$
has an inverse that can be found in Matlab.
\begin{verbatim}
    >> inv(A)
ans =
  -99 -100  100
   -1    1    0
  100  100 -100.
\end{verbatim}
The inverse gets even more difficult to calculate with a slight revision to the new matrix:
$$B = \begin{pmatrix} 1 & 0 & 1 \cr 1 & 1 & 1 \cr 2 & 1 & 1.9999\end{pmatrix}$$
(in Matlab)
\begin{verbatim}
    >> inv(B)
ans =
   1.0e+04 *
   -0.9999   -1.0000    1.0000
   -0.0001    0.0001   -0.0000
    1.0000    1.0000   -1.0000.
\end{verbatim}
A careful review of the determinants for matrix A and matrix B reveals: det(A)= -0.0100 and det(B)= -1.0000e-04. One can conclude that a matrix that has a determinant that is nonzero is theoretically nonsingular, but if numerically the determinant rounds to zero then a computer would consider the matrix singular. Thus, there exists a class of matrices that are {\it nearly} singular. To aid with identifying matrices that might be nearly singular there is the introduction of a {\bf condition number} for a matrix. The formal definition of a condition number is a bit challenging to unravel, but a popular estimate for a square matrix is:
$${\rm cond}(A) \approx \frac {\vert \lambda_{\rm max}\vert }{\vert \lambda_{\rm min}\vert },$$
where $\ds \lambda_{\rm max}$ is the largest eigenvalue of the matrix in complex modulus while $\ds \lambda_{\rm min}$ is the smallest eigenvalue in complex modulus for the matrix A. Recall that a singular matrix has an eigenvalue that is equal to zero and so the condition number for a singular matrix is not defined, but a matrix near singularity would have a very large condition number and the closer a matrix is to singular the closer the corresponding condition would  approach infinite in value. On the other hand, the smallest value a condition number can be is 1. Therefore, the range of condition numbers for a nonsingular matrix is: $\ds 1 \le {\rm cond}(A) < \infty$. For our two sample matrices we have cond(A) = 899.0924 and cond(B) = 9.0188e+04. A matrix that has too large of a condition number is labeled {\bf ill-conditioned} and there are specialized numerical techniques for solving ill-conditioned systems. There is not a hard definition for what should be deemed as too large of value for a condition number, but condition numbers on the order of $10^9$ or larger can be problematic to solve. 

\section{Gauss-Seidel Method}
There are a myriad of iterative methods for solving a linear system. The advantage of an iterative method is that for a large system it might be easier to apply an iterative method multiple times to approximate an answer instead of the computationally intensive work of Gaussian Elimination. The key question is convergence of the iterative method to the solution to the linear system. The Gauss-Seidel Method is covered in \cite{KK09} in Chapter 4, section 8. Convergence of the Gauss-Seidel method is heavily dependent on the structure of the matrix. Gauss-Seidel will generally converge if the matrix is symmetric positive definite or if the matrix is strictly diagonally dominant. A symmetric positive definite matrix is a matrix where A equals the transpose of A along with each of the eigenvalues being positive. A strictly diagonally dominant matrix is where each diagonal element is larger in absolute value then the sum in absolute value for each of the rest of the column elements containing that diagonal element
$$\vert a_{i,i}\vert > \sum_{j\neq i}\vert a_{i,j} \vert , {\rm for \ all}\  i .$$

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{15}{Eigenvalues and Eigenvectors}{Kyle Riley}{Riley}

Many of the results in Linear Algebra are strongly linked to structure of the eigenvalues and eigenvectors for a matrix. This lecture will briefly cover the basic information of eigenvalues and eigenvectors and how it relates to numerical analysis, but it should be noted this is an area rich with connections to be made to other areas of mathematics. To keep things simple, {\it the discussion will focus on square matrices} and much of the results and definitions will be restricted to square systems with acknowledgment that many of the concepts and definitions can be extended to systems that are not square. 

\section{Eigenvalue}

The definition for an eigenvalue is easy to state, but will require a bit clarification with all the terms that are used. 
\definition The {\bf eigenvalue} of a square matrix is a value, $\lambda$,  such that ${\rm det}(A-\lambda I)=0$. 

The det($A$) is a reference to the determinant of a matrix, which is mentioned in Chapter 4 of \cite{KK09}. We can build on the process of calculating the derivative by starting with a 2 by 2 matrix and extending the general calculation to 4 by 4. For a 2 by 2 matrix, $\ds A = \begin{pmatrix} a & b \cr c & d\end{pmatrix},$
the determinant can be represented via notation as $\ds {\rm det}(A)$ or $\ds \vert A \vert$. The calculation of the determinant for a 2 by 2 is given by
$${\rm det}(A)=\vert A \vert = ad - bc.$$ 
For a 3 by 3 matrix, $\ds B = \begin{pmatrix} a & b & c \cr d & e & f \cr g & h & m\end{pmatrix}$ the determinant is calculated via:
$$\vert B \vert = a \biggr \vert \begin{matrix} e & f \cr h & m\end{matrix} \biggr \vert - b \biggr \vert \begin{matrix} d & f \cr g & m\end{matrix} \biggr \vert + c \biggr \vert \begin{matrix} d & e \cr g & h\end{matrix} \biggr \vert.$$
The extension to a 4 by 4 matrix should reveal the necessary pattern with the 4 by 4 matrix being,  $\ds C = \begin{pmatrix} a & b & c & d \cr  e & f & g & h \cr m & n & o & p \cr q & r & s & t \end{pmatrix}$ and the associated determinant being:

$$\vert C \vert = a \Biggr \vert \begin{matrix} f & g & h \cr n & o & p \cr r & s & t\end{matrix} \Biggr \vert - b \Biggr \vert \begin{matrix} e & g & h \cr m & o & p \cr q & s & t\end{matrix} \Biggr \vert + c \Biggr \vert \begin{matrix} e & f  & h \cr m & n & p  \cr q & r & t \end{matrix} \Biggr \vert - d \Biggr \vert \begin{matrix} e & f & g \cr m & n & o \cr q & r & s \end{matrix} \Biggr \vert .$$
There is a general formula for the determinant that involves the development of {\it cofactors}, but for our discussion the expansion pattern defined above should be sufficient to establish that det$\ds (A-\lambda I)$ is a n$^{th}$ degree polynomial for an n by n matrix. Thus, the eigenvalue problem is the same challenge as the root finding problems covered earlier in the semester with the restriction of finding roots to only polynomials. 
\section{Eigenvector}
Every eigenvalue has an associated eigenvector. 
\definition Given an eigenvalue, $\ds \lambda_1$ for a given matrix the associated {\bf eigenvector} is a nonzero vector such that 
$$(A-\lambda_1 I) \vec v_1 = \vec 0, \ {\rm which \ is \ the \ same \ as} \ \ A\vec v_1 = \lambda_1 \vec v_1.$$

Recall that det($A - \lambda_1 I)=0$ for an eigenvalue so that means an eigenvector is a solution to a singular system. For example, $\ds A = \begin{pmatrix} 2 & -2 & 2 \cr 0 & 1 & 1 \cr -4 & 8 & 3\end{pmatrix}$ has an eigenvalue of $\ds \lambda = 2$, which means the associated eigenvector is a solution to:
$$(A-\lambda I)\vec v = \begin{pmatrix}  0 & -2 & 2 \cr 0 & -1 & 1 \cr -4 & 8 & 1\end{pmatrix} \vec v = \vec 0.$$
Using Gaussian Elimination on the singular system does generate a solution of $\ds \vec v = \begin{pmatrix}9/4 \cr 1 \cr 1 \end{pmatrix}$. However, it is true that any nonzero multiple of a eigenvector is also an eigenvector and this fact would mean that $\ds 4 \vec v = \begin{pmatrix}9 \cr 4 \cr 4 \end{pmatrix}$ is also and eigenvector. Please recall that singular systems have an infinite number of solutions and so it is not a surprise that eigenvectors would have some flexibility in representation given they are solutions to singular systems. 

One detail from linear algebra is that a square n by n matrix with n distinct real eigenvalues is diagonalizable, which means that if $V$ is the matrix made up of the individual eigenvectors then 
$$V^{-1}AV = \Lambda ,$$
where $\Lambda$ is a diagonal matrix with the individual eigenvalues for A residing in the diagonal positions. Moreover, the set of eigenvectors form a basis for the linear space. This basis relationship along with being diagonalizable are the crucial foundations for many of the analytical solution techniques that are utilized in science and engineering. 

\section{The Power Method}
Analytically computing the eigenvalues and eigenvectors can be computationally intensive and the work can be prohibitive for very large systems. There are several ways to approximate eigenvalues, but one of the simplest methods is the Power Method. If the collection of eigenvectors are a basis of the linear space then any vector can be represented as a linear combination of the eigenvectors:
$$\vec w = c_1 \vec v_1 + c_2 \vec v_2 + ... + c_n \vec v_n,$$
where $\ds \vec w$ is a given vector and $\ds \{ \vec v_i\}_{i=1}^n$ is the collection of eigenvectors with coefficients $\ds \{ c_i\}_{i=1}^n$. It follows that,
\begin{align*}
    A\vec w &= A(c_1 \vec v_1 + c_2 \vec v_2 + ... + c_n \vec v_n) \cr
    &=c_1 A \vec v_1 + c_2 A \vec v_2 + ... + c_n A\vec v_n \cr
     &=c_1 \lambda_1 \vec v_1 + c_2 \lambda_2 \vec v_2 + ... + c_n \lambda_n \vec v_n, 
\end{align*}
but the more interesting calculation involves multiple applications of A with
\begin{align*}
    A^2\vec w &= A^2(c_1 \vec v_1 + c_2 \vec v_2 + ... + c_n \vec v_n) \cr
    &=c_1 A^2 \vec v_1 + c_2 A^2 \vec v_2 + ... + c_n A^2\vec v_n \cr
     &=c_1 \lambda_1^2 \vec v_1 + c_2 \lambda_2^2 \vec v_2 + ... + c_n \lambda_n^2 \vec v_n.
\end{align*}
The Power Method recognizes what happens when A is applied repeatedly to some vector:
\begin{align*}
    A^k\vec w &= A^k(c_1 \vec v_1 + c_2 \vec v_2 + ... + c_n \vec v_n) \cr
    &=c_1 A^k \vec v_1 + c_2 A^k \vec v_2 + ... + c_n A^k\vec v_n \cr
     &=c_1 \lambda_1^k \vec v_1 + c_2 \lambda_2^k \vec v_2 + ... + c_n \lambda_n^k \vec v_n,
\end{align*}
it follows that the largest eigenvalue will start dominating the expression as $k\rightarrow \infty$. The Power Method is structured to allow the largest eigenvalue to dominate the expression so that the method is able to single out the largest eigenvalue and the corresponding eigenvector. 

An outline of the Power Method starts with some initial guess of the eigenvector, call it $\ds \vec x_1$. Let $\ds \vec y_1 = A \vec x_1$ then define $\ds a_1$ as the largest component of $\ds y_1$ in absolute value. The next iteration begins with $\ds \vec x_2 = \frac 1{a_1} \vec y_1$ followed by $\ds \vec y_2 = A \vec x_2$. The process continues to repeat with selection of $\ds a_2$ as the largest component of $\ds y_2$ in absolute value. It follows that $\ds a_k$ should converge to the dominant eigenvalue for A as $\ds k\rightarrow \infty$. It should also be the case that $\ds \vec x_k$ converges to the corresponding eigenvector. The rate of convergence is dependent on how dominant the largest eigenvalue is for A. The rate of convergence increases when the dominant eigenvalue is significantly larger in absolute value over the rest of the eigenvalues for A. 

%------------------------------------------------------------------
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{16}{New Lecture}{Kyle Riley}{Riley}
\section{Propositional Logic}


\definition A {\bf proposition} is a declarative sentence that is true or false, but not both. \bigskip \par




{\bf Note:} {\color{teal} be sure to review vocabulary covered in the book (a few examples include: atomic propositions, propositional calculus, etc.)}
\\





%Here is a citation, just for fun~\cite{CW87}.

\section*{References}
\beginrefs

\bibentry{KA89}{\sc Kendall Atkinson},
{\it Numerical Methods for Engineers}, 1989. 

\bibentry{AM96}{\sc Bilal Ayyub} and {\sc Richard McCuen},
{\it Introduction to Numerical Analysis (2nd ed.)}, 1996. 


\bibentry{LB16}{\sc Leon Brin},
{\it Tea Time Numerical Analysis (Experiences in Mathematics)  (2nd ed.)}, 2016. Website: \href{http://lqbrin.github.io/tea-time-numerical/}{lqbrin.github.io/tea-time-numerical} .

\bibentry{BF11}{\sc Richard L. Burden} and {\sc J. Douglas Faires},
{\it Numerical Analysis (9 ed.)}, 2011. Website: \href{https://books.google.com/books/about/Numerical\_Analysis.html?id=zXnSxY9G2JgC}{book website} . 

\bibentry{CC10}{\sc Steven Chapra} and {\sc Raymond Canale}, {\it Numerical Methods for Engineers (6 ed.)}, 2010.

\bibentry{GG19}{\sc Geogebra}, \href{https://www.geogebra.org}{website of geogebra.org} , 2019.

\bibentry{GV96}{\sc Gene Golub} and {\sc Charles F. Van Loan} {\it Matrix Computations 3 ed.}, 1996.


\bibentry{NH02}{\sc Nicholas J. Higham}, {\it Accuracy and Stability of Numerical Algorithms (2 ed.)}, 2002. 

\bibentry{KK09} {\sc Autar Kaw} and {\sc E. Eric Kalu}, {\it Numerical Methods with Applications (2nd ed.)}, 2009. Website: \href{http://autarkaw.com/books/numericalmethods/index.html}{book website} .

\bibentry{KC02}{\sc David Kincaid} and {\sc Ward Cheney},
{\it Numerical Analysis: Mathematics of Scientific Computing (3rd ed.)}, 2002. Website: \href{https://bookstore.ams.org/amstext-2}{book website} .

\bibentry{MF04}{\sc John Mathews} and {\sc Kurtis Fink}, {\it Numerical Methods Using Matlab (4th ed.)}, 2004. 

\bibentry{JR13}{\sc James Riggs}, {\it Computational Methods for Engineers with Matlab applications}, 2013. 

\endrefs

\end{document} 